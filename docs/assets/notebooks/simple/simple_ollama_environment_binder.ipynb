{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Ollama Environment – Binder-compatible Notebook\n",
    "\n",
    "This notebook sets up a **local Ollama server** inside a Jupyter session (for example, on [**mybinder.org**](https://mybinder.org)) without requiring `sudo` or Docker.\n",
    "\n",
    "It will:\n",
    "\n",
    "1. Install the **Ollama Python client** (`ollama`) and `requests`.\n",
    "2. Download the **Linux AMD64 Ollama tarball** (`ollama-linux-amd64.tgz`).\n",
    "3. Extract it into your home directory (e.g. `~/ollama`).\n",
    "4. Start `ollama serve` locally on `http://127.0.0.1:11434`.\n",
    "5. Pull a small default model: `qwen2.5:0.5b-instruct`.\n",
    "6. Run a minimal **Python chat example** using `ollama.chat()`.\n",
    "\n",
    "The setup works both on Binder and in any standard Jupyter environment running on Linux.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. How to use this notebook with Binder\n",
    "\n",
    "**A. Add the notebook to your repo**\n",
    "\n",
    "1. Save this file as `simple-ollama-environment-binder.ipynb`.\n",
    "2. Commit and push it to your Git repository (e.g. on GitHub or GitLab).\n",
    "\n",
    "**B. (Optional but recommended) Add `requirements.txt`**\n",
    "\n",
    "In the root of your repo, create a `requirements.txt` with at least:\n",
    "\n",
    "```text\n",
    "ollama\n",
    "requests\n",
    "```\n",
    "\n",
    "Binder will then preinstall these dependencies when building the image. (If you skip this,\n",
    "the notebook will still `pip install` them at runtime, but that is a bit slower.)\n",
    "\n",
    "**C. Create a Binder link**\n",
    "\n",
    "You can use a URL like this (replace placeholders in ALL CAPS):\n",
    "\n",
    "```text\n",
    "https://mybinder.org/v2/gh/USER/REPO/BRANCH?labpath=simple-ollama-environment-binder.ipynb\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```text\n",
    "https://mybinder.org/v2/gh/myname/simple-ollama-environment/HEAD?labpath=simple-ollama-environment-binder.ipynb\n",
    "```\n",
    "\n",
    "**D. Once Binder launches**\n",
    "\n",
    "1. Wait for the Binder image to build and JupyterLab to start.\n",
    "2. The notebook should open automatically (or click it in the file browser).\n",
    "3. Run the cells from top to bottom:\n",
    "   - **Cell 1** – installs Python dependencies.\n",
    "   - **Cells 2–4** – download & start the local Ollama server.\n",
    "   - **Cell 5** – pulls the `qwen2.5:0.5b-instruct` model.\n",
    "   - **Cell 6** – sends a test chat to the model.\n",
    "4. You can then adapt the last cell(s) for your own prompts and use-cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Python dependencies\n",
    "\n",
    "We install:\n",
    "\n",
    "- `ollama` – the official Python client\n",
    "- `requests` – to download the server tarball and poll the API\n",
    "\n",
    "If you're on Binder **and** you added these to `requirements.txt`, this step will be quick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ollama requests\n",
    "\n",
    "import ollama, requests\n",
    "print(\"ollama Python client version:\", getattr(ollama, \"__version__\", \"unknown\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure paths and environment variables\n",
    "\n",
    "We will install the Ollama binary into a directory in your home, e.g. `~/ollama`,\n",
    "and store models under `~/ollama-models`.\n",
    "\n",
    "This follows the same pattern commonly used on HPC / no-sudo environments:\n",
    "\n",
    "```bash\n",
    "mkdir -p ollama\n",
    "tar -C ollama -xzvf ollama-linux-amd64.tgz\n",
    "./ollama/bin/ollama --version\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "HOME = Path.home()\n",
    "OLLAMA_DIR = HOME / \"ollama\"\n",
    "OLLAMA_MODELS_DIR = HOME / \"ollama-models\"\n",
    "OLLAMA_TARBALL = HOME / \"ollama-linux-amd64.tgz\"\n",
    "\n",
    "OLLAMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OLLAMA_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Environment variables used by the Ollama server\n",
    "os.environ[\"OLLAMA_MODELS\"] = str(OLLAMA_MODELS_DIR)\n",
    "os.environ[\"OLLAMA_HOST\"] = \"http://127.0.0.1:11434\"\n",
    "\n",
    "OLLAMA_BIN = OLLAMA_DIR / \"bin\" / \"ollama\"\n",
    "\n",
    "print(\"HOME              =\", HOME)\n",
    "print(\"OLLAMA_DIR        =\", OLLAMA_DIR)\n",
    "print(\"OLLAMA_MODELS_DIR =\", OLLAMA_MODELS_DIR)\n",
    "print(\"OLLAMA_BIN        =\", OLLAMA_BIN)\n",
    "print(\"OLLAMA_HOST       =\", os.environ[\"OLLAMA_HOST\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and unpack the Ollama Linux AMD64 tarball\n",
    "\n",
    "This cell downloads `ollama-linux-amd64.tgz` and extracts it into `OLLAMA_DIR`.\n",
    "If the binary already exists, it will skip the download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import tarfile\n",
    "import subprocess\n",
    "\n",
    "OLLAMA_URL = \"https://ollama.com/download/ollama-linux-amd64.tgz\"\n",
    "\n",
    "if not OLLAMA_BIN.exists():\n",
    "    print(\"Downloading:\", OLLAMA_URL)\n",
    "    r = requests.get(OLLAMA_URL, stream=True)\n",
    "    r.raise_for_status()\n",
    "    data = r.content\n",
    "    print(\"Download size (bytes):\", len(data))\n",
    "\n",
    "    file_like = io.BytesIO(data)\n",
    "    print(\"Extracting to\", OLLAMA_DIR)\n",
    "    with tarfile.open(fileobj=file_like, mode=\"r:gz\") as tar:\n",
    "        tar.extractall(path=OLLAMA_DIR)\n",
    "else:\n",
    "    print(\"Ollama binary already exists at:\", OLLAMA_BIN)\n",
    "\n",
    "print(\"Verifying Ollama binary and version...\")\n",
    "if not OLLAMA_BIN.exists():\n",
    "    raise FileNotFoundError(f\"Expected Ollama binary at {OLLAMA_BIN}, but it was not found.\")\n",
    "\n",
    "result = subprocess.run([str(OLLAMA_BIN), \"--version\"], capture_output=True, text=True)\n",
    "print(\"Return code:\", result.returncode)\n",
    "print(result.stdout or result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start the Ollama server inside this Jupyter session\n",
    "\n",
    "This starts `ollama serve` in the background and waits until the API at\n",
    "`http://127.0.0.1:11434/api/tags` responds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import signal\n",
    "\n",
    "base_url = os.environ.get(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\")\n",
    "tags_url = base_url.rstrip(\"/\") + \"/api/tags\"\n",
    "\n",
    "# If an old server process exists, check its status\n",
    "if 'OLLAMA_SERVER_PROCESS' in globals():\n",
    "    if OLLAMA_SERVER_PROCESS.poll() is None:\n",
    "        print(\"Ollama server is already running with PID\", OLLAMA_SERVER_PROCESS.pid)\n",
    "    else:\n",
    "        print(\"Previous Ollama server process has exited; starting a new one...\")\n",
    "        del OLLAMA_SERVER_PROCESS\n",
    "\n",
    "if 'OLLAMA_SERVER_PROCESS' not in globals() or OLLAMA_SERVER_PROCESS.poll() is not None:\n",
    "    print(\"Starting Ollama server...\")\n",
    "    OLLAMA_SERVER_PROCESS = subprocess.Popen(\n",
    "        [str(OLLAMA_BIN), \"serve\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.STDOUT,\n",
    "    )\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"Waiting for Ollama server to become ready at\", tags_url)\n",
    "for i in range(60):\n",
    "    try:\n",
    "        r = requests.get(tags_url, timeout=2)\n",
    "        if r.ok:\n",
    "            print(\"✅ Ollama server is up!\")\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "    time.sleep(1)\n",
    "else:\n",
    "    print(\"⚠️ Server did not become ready within 60 seconds.\")\n",
    "    print(\"You may want to rerun this cell or check that the binary is executable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pull a small default model: `qwen2.5:0.5b-instruct`\n",
    "\n",
    "This pulls a relatively small model from the Ollama library. You can change\n",
    "`MODEL_NAME` to any other Ollama model tag you prefer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"qwen2.5:0.5b-instruct\"  # change this tag if you want a different model\n",
    "\n",
    "print(f\"Pulling model: {MODEL_NAME} ...\")\n",
    "pull_proc = subprocess.run([str(OLLAMA_BIN), \"pull\", MODEL_NAME])\n",
    "if pull_proc.returncode == 0:\n",
    "    print(\"✅ Model pulled successfully.\")\n",
    "else:\n",
    "    print(\"⚠️ Model pull returned a non-zero exit code.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quickstart: chat with the model via Python\n",
    "\n",
    "Now that the server is running and the model is pulled, we can send a simple\n",
    "chat message using the **Ollama Python client**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Ensure the client points to the local server\n",
    "os.environ.setdefault(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\")\n",
    "\n",
    "MODEL_NAME = globals().get(\"MODEL_NAME\", \"qwen2.5:0.5b-instruct\")\n",
    "\n",
    "prompt = (\n",
    "    \"Di' solo 'Ciao!' in italiano e poi dammi 1 consiglio molto breve \"\n",
    "    \"per studiare meglio (in italiano).\"\n",
    ")\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "print(\"--- Model response ---\\n\")\n",
    "print(response[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Optional) Stop the Ollama server\n",
    "\n",
    "This will gracefully stop the background `ollama serve` process. If you are on Binder,\n",
    "the entire container will also be discarded when you close the session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'OLLAMA_SERVER_PROCESS' in globals() and OLLAMA_SERVER_PROCESS.poll() is None:\n",
    "    print(\"Stopping Ollama server (PID\", OLLAMA_SERVER_PROCESS.pid, \")...\")\n",
    "    OLLAMA_SERVER_PROCESS.terminate()\n",
    "    try:\n",
    "        OLLAMA_SERVER_PROCESS.wait(timeout=10)\n",
    "        print(\"Ollama server stopped.\")\n",
    "    except Exception:\n",
    "        print(\"Process did not exit in time, killing...\")\n",
    "        OLLAMA_SERVER_PROCESS.kill()\n",
    "else:\n",
    "    print(\"No running Ollama server process found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}