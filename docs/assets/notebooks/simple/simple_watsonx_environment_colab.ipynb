{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "name": "simple_watsonx_environment_colab.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple watsonx.ai Environment (Google Colab)\n",
        "\n",
        "This notebook is a Colab adaptation of your **simple watsonx environment**:\n",
        "\n",
        "1. Install required libraries\n",
        "2. Configure IBM Cloud / watsonx.ai credentials\n",
        "3. Run a quickstart example with the native `ibm_watsonx_ai` SDK\n",
        "4. (Optional) Use watsonx.ai via **LangChain**\n",
        "\n",
        "> \u26a0\ufe0f **Security note**: Never share a notebook with your API keys visible. Use the input prompts provided here so the keys are not printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install-deps"
      },
      "source": [
        "# 1\ufe0f\u20e3 Install dependencies\n",
        "# Run this cell once when you open the notebook (or if the runtime is reset).\n",
        "\n",
        "!pip install -q ibm-watsonx-ai langchain-ibm python-dotenv\n",
        "print(\"\u2705 Libraries installed (or already present).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "set-credentials"
      },
      "source": [
        "# 2\ufe0f\u20e3 Configure your IBM Cloud / watsonx.ai credentials\n",
        "# These values stay only in this Colab runtime and are not printed.\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "print(\"\u26a0\ufe0f Your keys are kept only in this runtime. Do NOT share this notebook after filling them.\")\n",
        "\n",
        "# Required\n",
        "IBM_CLOUD_API_KEY = getpass.getpass(\"IBM Cloud API key: \")\n",
        "IBM_CLOUD_PROJECT_ID = input(\"IBM Cloud project_id: \").strip()\n",
        "\n",
        "# Region URL \u2013 change if you use a different region (e.g. eu-de, ca-tor, etc.)\n",
        "default_url = \"https://us-south.ml.cloud.ibm.com\"\n",
        "entered_url = input(f\"IBM Cloud URL [{default_url}]: \").strip()\n",
        "IBM_CLOUD_URL = entered_url or default_url\n",
        "\n",
        "# Store them in environment variables (same pattern as in the simple watsonx environment project)\n",
        "os.environ[\"IBM_CLOUD_API_KEY\"] = IBM_CLOUD_API_KEY\n",
        "os.environ[\"IBM_CLOUD_PROJECT_ID\"] = IBM_CLOUD_PROJECT_ID\n",
        "os.environ[\"IBM_CLOUD_URL\"] = IBM_CLOUD_URL\n",
        "\n",
        "# Also set aliases in case other code expects them\n",
        "os.environ[\"WATSONX_APIKEY\"] = IBM_CLOUD_API_KEY\n",
        "os.environ[\"PROJECT_ID\"] = IBM_CLOUD_PROJECT_ID\n",
        "os.environ[\"WATSONX_URL\"] = IBM_CLOUD_URL\n",
        "\n",
        "print(\"\\n\u2705 Credentials stored in environment variables.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3\ufe0f\u20e3 watsonx.ai quickstart with `ibm_watsonx_ai`\n",
        "\n",
        "This mirrors the quickstart logic from your original project: it\n",
        "\n",
        "- reads credentials from environment variables,\n",
        "- creates a watsonx.ai `ModelInference` client, and\n",
        "- generates text from a chosen foundation model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "watsonx-quickstart"
      },
      "source": [
        "import os\n",
        "from ibm_watsonx_ai import Credentials, APIClient\n",
        "from ibm_watsonx_ai.foundation_models import ModelInference\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "\n",
        "# Read credentials from environment variables (set in the previous cell)\n",
        "api_key = os.environ.get(\"IBM_CLOUD_API_KEY\") or os.environ.get(\"WATSONX_APIKEY\")\n",
        "url = os.environ.get(\"IBM_CLOUD_URL\") or os.environ.get(\"WATSONX_URL\")\n",
        "project_id = os.environ.get(\"IBM_CLOUD_PROJECT_ID\") or os.environ.get(\"PROJECT_ID\")\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing API key (IBM_CLOUD_API_KEY or WATSONX_APIKEY).\")\n",
        "if not url:\n",
        "    raise ValueError(\"Missing URL (IBM_CLOUD_URL or WATSONX_URL).\")\n",
        "if not project_id:\n",
        "    raise ValueError(\"Missing project id (IBM_CLOUD_PROJECT_ID or PROJECT_ID).\")\n",
        "\n",
        "print(\"\u2705 Credentials loaded.\")\n",
        "print(f\"URL: {url}\")\n",
        "print(f\"Project ID: {project_id}\")\n",
        "\n",
        "# Create client & model\n",
        "credentials = Credentials(url=url, api_key=api_key)\n",
        "client = APIClient(credentials=credentials, project_id=project_id)  # created for completeness / reuse\n",
        "\n",
        "# \ud83d\udc49 Choose a model that is available in your watsonx project/space.\n",
        "# Replace with whatever you actually have access to, for example:\n",
        "#   - \"ibm/granite-13b-chat-v2\"\n",
        "#   - \"ibm/granite-8b-code-instruct\"\n",
        "#   - \"meta-llama/llama-3-3-70b-instruct\"\n",
        "model_id = \"meta-llama/llama-3-3-70b-instruct\"  # Change if needed\n",
        "\n",
        "prompt = \"Write a short story about a robot who wants to be a painter.\"\n",
        "\n",
        "params = {\n",
        "    GenParams.DECODING_METHOD: \"greedy\",\n",
        "    GenParams.MAX_NEW_TOKENS: 200,\n",
        "}\n",
        "\n",
        "model = ModelInference(model_id=model_id, credentials=credentials, project_id=project_id)\n",
        "\n",
        "print(\"\\n\ud83d\ude80 Sending request to watsonx.ai...\")\n",
        "response = model.generate_text(prompt=prompt, params=params)\n",
        "print(\"\\n--- watsonx.ai Response ---\\n\")\n",
        "print(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4\ufe0f\u20e3 Optional: Use watsonx.ai via LangChain\n",
        "\n",
        "This is similar to the LangChain integration in your original notebook,\n",
        "but simplified for Colab. You can build chains, tools, and agents on top\n",
        "of `WatsonxLLM` as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "langchain-watsonx"
      },
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_ibm import WatsonxLLM\n",
        "\n",
        "# If you prefer using a .env file, you can upload it via Colab UI\n",
        "# and this call will load it. If not present, it's harmless.\n",
        "load_dotenv()\n",
        "\n",
        "api_key = os.environ.get(\"IBM_CLOUD_API_KEY\") or os.environ.get(\"WATSONX_APIKEY\")\n",
        "url = os.environ.get(\"IBM_CLOUD_URL\") or os.environ.get(\"WATSONX_URL\")\n",
        "project_id = os.environ.get(\"IBM_CLOUD_PROJECT_ID\") or os.environ.get(\"PROJECT_ID\")\n",
        "\n",
        "if not api_key or not url or not project_id:\n",
        "    raise ValueError(\"Missing credentials for LangChain example.\")\n",
        "\n",
        "# Reuse the same model_id as above or adjust as needed\n",
        "model_id = \"meta-llama/llama-3-3-70b-instruct\"  # Change if needed\n",
        "\n",
        "params = {\n",
        "    \"decoding_method\": \"greedy\",\n",
        "    \"max_new_tokens\": 128,\n",
        "}\n",
        "\n",
        "llm = WatsonxLLM(\n",
        "    model_id=model_id,\n",
        "    url=url,\n",
        "    apikey=api_key,\n",
        "    project_id=project_id,\n",
        "    params=params,\n",
        ")\n",
        "\n",
        "print(\"\ud83d\ude80 Calling watsonx.ai through LangChain...\")\n",
        "response = llm.invoke(\"Give me 3 concise study tips for learning Python.\")\n",
        "print(\"\\n--- LangChain + watsonx.ai Response ---\\n\")\n",
        "print(response)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}