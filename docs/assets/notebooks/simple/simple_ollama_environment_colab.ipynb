{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cf02e9",
   "metadata": {},
   "source": [
    "# ðŸ§ª Simple Ollama Environment â€” Google Colab\n",
    "\n",
    "This notebook recreates the **Simple Ollama Environment** (Python 3 + Jupyter + Ollama) in a Google Colab runtime.\n",
    "\n",
    "It will:\n",
    "\n",
    "1. Install the Python client **`ollama`**.\n",
    "2. Download the **Ollama server** binary (Linux, no root required).\n",
    "3. Start the Ollama server in the background inside this notebook.\n",
    "4. Pull a small model (default: `qwen2.5:0.5b-instruct`).\n",
    "5. Run a minimal chat example via the Python client.\n",
    "\n",
    "> âš ï¸ **Note:** Colab runtimes are ephemeral. If the runtime restarts, you need to rerun the setup cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3704a3",
   "metadata": {},
   "source": [
    "## 0. (Optional) Enable GPU in Colab\n",
    "\n",
    "Before running the setup cells, you can enable a GPU:\n",
    "\n",
    "1. In the Colab menu, go to **Runtime â†’ Change runtime type**.\n",
    "2. Set **Hardware accelerator** to **GPU**.\n",
    "3. Click **Save**.\n",
    "\n",
    "Small models will also run on CPU, just more slowly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09727241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"ðŸ”Ž Checking for NVIDIA GPU...\")\n",
    "try:\n",
    "    gpu_info = subprocess.check_output([\"nvidia-smi\"], stderr=subprocess.STDOUT, text=True)\n",
    "    print(\"âœ… GPU detected! Summary:\\n\")\n",
    "    # Show only first few lines\n",
    "    print(\"\\n\".join(gpu_info.splitlines()[:8]))\n",
    "except Exception:\n",
    "    print(\"âš ï¸ No NVIDIA GPU detected or driver unavailable.\")\n",
    "    print(\"   Colab CPU-only runtimes can still run small models, just a bit slower.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f39fca",
   "metadata": {},
   "source": [
    "## 1. Install Python dependencies\n",
    "\n",
    "We install the official **Ollama Python library** plus `requests` (for simple health checks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"ollama>=0.6,<1.0\" requests\n",
    "\n",
    "import importlib\n",
    "spec = importlib.util.find_spec(\"ollama\")\n",
    "print(\"âœ… Installed 'ollama' Python client.\" if spec is not None else \"âŒ Failed to install 'ollama' client.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beaece8",
   "metadata": {},
   "source": [
    "## 2. Download the Ollama server binary (no root)\n",
    "\n",
    "We fetch the pre-built **Linux x86_64** Ollama binary directly into your home directory (no `sudo`, no systemd changes).\n",
    "\n",
    "This follows the **non-root** installation pattern suggested in the Ollama community docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a721c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "OLLAMA_HOME = os.path.join(HOME, \"opt\", \"ollama\")\n",
    "SRC_DIR = os.path.join(HOME, \"src\")\n",
    "\n",
    "os.makedirs(OLLAMA_HOME, exist_ok=True)\n",
    "os.makedirs(SRC_DIR, exist_ok=True)\n",
    "\n",
    "tar_path = os.path.join(SRC_DIR, \"ollama-linux-amd64.tgz\")\n",
    "ollama_bin_path = os.path.join(OLLAMA_HOME, \"ollama\")\n",
    "\n",
    "if not os.path.exists(ollama_bin_path):\n",
    "    print(\"â¬‡ï¸ Downloading Ollama Linux binary...\")\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"curl\",\n",
    "            \"-L\",\n",
    "            \"https://ollama.com/download/ollama-linux-amd64.tgz\",\n",
    "            \"-o\",\n",
    "            tar_path,\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "    print(\"ðŸ“¦ Extracting...\")\n",
    "    subprocess.run(\n",
    "        [\"tar\", \"-C\", OLLAMA_HOME, \"-xzf\", tar_path],\n",
    "        check=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"âœ… Ollama binary already present, skipping download.\")\n",
    "\n",
    "if not os.path.exists(ollama_bin_path):\n",
    "    raise RuntimeError(f\"Ollama binary not found at {ollama_bin_path}\")\n",
    "\n",
    "print(\"âœ… Ollama binary ready at:\", ollama_bin_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143dcd4d",
   "metadata": {},
   "source": [
    "## 3. Start the Ollama server in the background\n",
    "\n",
    "This cell:\n",
    "\n",
    "- Configures some basic environment variables.\n",
    "- Starts `ollama serve` as a **background process** inside the Python kernel.\n",
    "- Waits until the HTTP API is healthy at `http://127.0.0.1:11434`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6909d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import requests\n",
    "\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "OLLAMA_HOME = os.path.join(HOME, \"opt\", \"ollama\")\n",
    "OLLAMA_BIN = os.path.join(OLLAMA_HOME, \"ollama\")\n",
    "\n",
    "if not os.path.exists(OLLAMA_BIN):\n",
    "    raise RuntimeError(\"Ollama binary not found. Run the previous install cell first.\")\n",
    "\n",
    "# Stop an existing server (if any) from a previous run\n",
    "try:\n",
    "    OLLAMA_SERVER_PROCESS  # type: ignore[name-defined]\n",
    "    if OLLAMA_SERVER_PROCESS.poll() is None:\n",
    "        print(\"ðŸ›‘ Stopping existing Ollama server...\")\n",
    "        OLLAMA_SERVER_PROCESS.terminate()\n",
    "        try:\n",
    "            OLLAMA_SERVER_PROCESS.wait(timeout=10)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            OLLAMA_SERVER_PROCESS.kill()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Where to store models (in the Colab filesystem)\n",
    "os.environ.setdefault(\"OLLAMA_MODELS\", os.path.join(\"/content\", \"ollama-models\"))\n",
    "os.makedirs(os.environ[\"OLLAMA_MODELS\"], exist_ok=True)\n",
    "\n",
    "# Basic tuning flags\n",
    "os.environ.setdefault(\"OLLAMA_MAX_LOADED_MODELS\", \"1\")\n",
    "os.environ.setdefault(\"OLLAMA_NUM_PARALLEL\", \"1\")\n",
    "\n",
    "# Host where the API will be available\n",
    "os.environ[\"OLLAMA_HOST\"] = \"http://127.0.0.1:11434\"\n",
    "\n",
    "print(\"ðŸš€ Starting Ollama server...\")\n",
    "OLLAMA_SERVER_PROCESS = subprocess.Popen(\n",
    "    [OLLAMA_BIN, \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.STDOUT,\n",
    ")\n",
    "\n",
    "# Wait for health endpoint\n",
    "for i in range(60):\n",
    "    try:\n",
    "        r = requests.get(\"http://127.0.0.1:11434/api/tags\", timeout=2)\n",
    "        if r.ok:\n",
    "            print(\"âœ… Ollama server is up at http://127.0.0.1:11434\")\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "    time.sleep(1)\n",
    "else:\n",
    "    raise RuntimeError(\"Ollama server did not become healthy in time.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccdc922",
   "metadata": {},
   "source": [
    "## 4. Pull a small model\n",
    "\n",
    "We'll pull the same small instruction-tuned model used in the original **Simple Ollama Environment**:\n",
    "\n",
    "- Default: `qwen2.5:0.5b-instruct`\n",
    "\n",
    "You can change the model name before running the cell (e.g. to `llama3.2:1b` or `qwen3:0.6b`, if available on Ollama).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f776e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Change this if you want a different model\n",
    "MODEL_NAME = \"qwen2.5:0.5b-instruct\"\n",
    "\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "OLLAMA_HOME = os.path.join(HOME, \"opt\", \"ollama\")\n",
    "OLLAMA_BIN = os.path.join(OLLAMA_HOME, \"ollama\")\n",
    "\n",
    "print(f\"ðŸ“¥ Pulling model: {MODEL_NAME}\")\n",
    "subprocess.run([OLLAMA_BIN, \"pull\", MODEL_NAME], check=True)\n",
    "print(\"âœ… Model pulled and ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2f619",
   "metadata": {},
   "source": [
    "## 5. Run a quick chat from Python\n",
    "\n",
    "Now that:\n",
    "\n",
    "- the **Ollama server** is running, and  \n",
    "- a model is pulled,\n",
    "\n",
    "we can use the `ollama` Python client to send a simple chat request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import os\n",
    "\n",
    "# Ensure the client talks to our local server\n",
    "os.environ.setdefault(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\")\n",
    "\n",
    "MODEL_NAME = \"qwen2.5:0.5b-instruct\"  # keep in sync with the pull cell above\n",
    "\n",
    "print(\"âœ… Using Ollama Python client version:\", getattr(ollama, \"__version__\", \"unknown\"))\n",
    "print(\"ðŸ§  Talking to model:\", MODEL_NAME)\n",
    "\n",
    "prompt = \"Di' solo 'Ciao!' in italiano e poi dammi 1 consiglio per studiare meglio.\"\n",
    "\n",
    "resp = ollama.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ’¬ User:\")\n",
    "print(prompt)\n",
    "print(\"\\nðŸ¤– Model reply:\\n\")\n",
    "print(resp[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac07c1d",
   "metadata": {},
   "source": [
    "## 6. Next steps\n",
    "\n",
    "You now have:\n",
    "\n",
    "- An **Ollama server** running inside Colab.\n",
    "- A **Python client** ready to call `ollama.chat` / `ollama.generate`.\n",
    "- At least one **local model** pulled.\n",
    "\n",
    "From here you can:\n",
    "\n",
    "- Swap models (edit `MODEL_NAME` and re-run the pull + chat cells).\n",
    "- Use `stream=True` in `ollama.chat` for streaming responses.\n",
    "- Integrate Ollama with other libraries (LangChain, LlamaIndex, etc.) in additional cells.\n",
    "\n",
    "Happy hacking! ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
