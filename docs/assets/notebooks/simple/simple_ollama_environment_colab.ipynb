{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13cf02e9",
      "metadata": {
        "id": "13cf02e9"
      },
      "source": [
        "# ðŸ§ª Simple Ollama Environment â€” Google Colab (Fixed)\n",
        "\n",
        "This notebook recreates the **Simple Ollama Environment** (Python 3 + Jupyter + Ollama) in a Google Colab runtime.\n",
        "\n",
        "It will:\n",
        "\n",
        "1. Install the Python client **`ollama`**.\n",
        "2. Download the **Ollama server** binary (Linux, no root required).\n",
        "3. Start the Ollama server in the background inside this notebook.\n",
        "4. Pull a small model (default: `qwen2.5:0.5b-instruct`).\n",
        "5. Run a minimal chat example via the Python client.\n",
        "\n",
        "> âš ï¸ **Note:** Colab runtimes are ephemeral. If the runtime restarts, you need to rerun the setup cells.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c3704a3",
      "metadata": {
        "id": "9c3704a3"
      },
      "source": [
        "## 0. (Optional) Enable GPU in Colab\n",
        "\n",
        "Before running the setup cells, you can enable a GPU:\n",
        "\n",
        "1. In the Colab menu, go to **Runtime â†’ Change runtime type**.\n",
        "2. Set **Hardware accelerator** to **GPU**.\n",
        "3. Click **Save**.\n",
        "\n",
        "Small models will also run on CPU, just more slowly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09727241",
      "metadata": {
        "id": "09727241"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"ðŸ”Ž Checking for NVIDIA GPU...\")\n",
        "try:\n",
        "    gpu_info = subprocess.check_output([\"nvidia-smi\"], stderr=subprocess.STDOUT, text=True)\n",
        "    print(\"âœ… GPU detected! Summary:\\n\")\n",
        "    # Show only first few lines\n",
        "    print(\"\\n\".join(gpu_info.splitlines()[:12]))\n",
        "except Exception:\n",
        "    print(\"âš ï¸ No NVIDIA GPU detected or driver unavailable.\")\n",
        "    print(\"   Colab CPU-only runtimes can still run small models, just a bit slower.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f39fca",
      "metadata": {
        "id": "b7f39fca"
      },
      "source": [
        "## 1. Install Python dependencies\n",
        "\n",
        "We install the official **Ollama Python library** plus `requests` (for simple health checks).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc25e764",
      "metadata": {
        "id": "bc25e764"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"ollama>=0.6,<1.0\" requests\n",
        "\n",
        "import importlib\n",
        "spec = importlib.util.find_spec(\"ollama\")\n",
        "print(\"âœ… Installed 'ollama' Python client.\" if spec is not None else \"âŒ Failed to install 'ollama' client.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7beaece8",
      "metadata": {
        "id": "7beaece8"
      },
      "source": [
        "## 2. Download the Ollama server binary (no root)\n",
        "\n",
        "We fetch the pre-built **Linux x86_64** Ollama binary directly into your home directory (no `sudo`, no systemd changes).\n",
        "\n",
        "This follows the **non-root** installation pattern suggested in the Ollama community docs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a721c5a9",
      "metadata": {
        "id": "a721c5a9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import subprocess\n",
        "\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "OLLAMA_HOME = os.path.join(HOME, \"opt\", \"ollama\")\n",
        "SRC_DIR = os.path.join(HOME, \"src\")\n",
        "\n",
        "os.makedirs(OLLAMA_HOME, exist_ok=True)\n",
        "os.makedirs(SRC_DIR, exist_ok=True)\n",
        "\n",
        "tar_path = os.path.join(SRC_DIR, \"ollama-linux-amd64.tgz\")\n",
        "# Corrected path: the ollama binary is extracted into the 'bin' subdirectory\n",
        "ollama_bin_path = os.path.join(OLLAMA_HOME, \"bin\", \"ollama\")\n",
        "\n",
        "if not os.path.exists(ollama_bin_path):\n",
        "    print(\"â¬‡ï¸ Downloading Ollama Linux binary...\")\n",
        "    subprocess.run(\n",
        "        [\n",
        "            \"curl\",\n",
        "            \"-L\",\n",
        "            \"https://ollama.com/download/ollama-linux-amd64.tgz\",\n",
        "            \"-o\",\n",
        "            tar_path,\n",
        "        ],\n",
        "        check=True,\n",
        "    )\n",
        "\n",
        "    # Verify downloaded file size. Ollama binary is usually several MBs.\n",
        "    if not os.path.exists(tar_path) or os.path.getsize(tar_path) < 1024 * 1024: # Less than 1MB is suspicious\n",
        "        raise RuntimeError(f\"Downloaded Ollama tarball is missing or too small: {tar_path}\")\n",
        "\n",
        "    print(\"ðŸ“¦ Extracting...\")\n",
        "    # The tar command extracts contents directly into OLLAMA_HOME, but the binary is in a 'bin' subdir\n",
        "    subprocess.run(\n",
        "        [\"tar\", \"-C\", OLLAMA_HOME, \"-xzf\", tar_path],\n",
        "        check=True,\n",
        "    )\n",
        "\n",
        "    # Verify existence immediately after extraction with the corrected path\n",
        "    if not os.path.exists(ollama_bin_path):\n",
        "        print(f\"âš ï¸ Ollama binary not found after extraction. Listing contents of {OLLAMA_HOME} recursively for diagnosis:\")\n",
        "        ls_output = subprocess.check_output([\"ls\", \"-laR\", OLLAMA_HOME], text=True, stderr=subprocess.STDOUT)\n",
        "        print(ls_output) # Print the captured output\n",
        "        raise RuntimeError(f\"Ollama binary not found at {ollama_bin_path} after extraction. See directory listing above.\")\n",
        "else:\n",
        "    print(\"âœ… Ollama binary already present, skipping download.\")\n",
        "\n",
        "# Final check (redundant if previous checks are robust, but good for safety)\n",
        "if not os.path.exists(ollama_bin_path):\n",
        "    raise RuntimeError(f\"Ollama binary not found at {ollama_bin_path}\")\n",
        "\n",
        "print(\"âœ… Ollama binary ready at:\", ollama_bin_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "143dcd4d",
      "metadata": {
        "id": "143dcd4d"
      },
      "source": [
        "## 3. Start the Ollama server in the background\n",
        "\n",
        "This cell:\n",
        "\n",
        "- Configures some basic environment variables.\n",
        "- Starts `ollama serve` as a **background process** inside the Python kernel.\n",
        "- Waits until the HTTP API is healthy at `http://127.0.0.1:11434`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd6909d8",
      "metadata": {
        "id": "fd6909d8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import requests\n",
        "\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "OLLAMA_HOME = os.path.join(HOME, \"opt\", \"ollama\")\n",
        "# Corrected OLLAMA_BIN path to point to the 'bin' subdirectory\n",
        "OLLAMA_BIN = os.path.join(OLLAMA_HOME, \"bin\", \"ollama\")\n",
        "\n",
        "if not os.path.exists(OLLAMA_BIN):\n",
        "    raise RuntimeError(\"Ollama binary not found. Run the previous install cell first.\")\n",
        "\n",
        "# Stop an existing server (if any) from a previous run\n",
        "try:\n",
        "    OLLAMA_SERVER_PROCESS  # type: ignore[name-defined]\n",
        "    if OLLAMA_SERVER_PROCESS.poll() is None:\n",
        "        print(\"ðŸ›‘ Stopping existing Ollama server...\")\n",
        "        OLLAMA_SERVER_PROCESS.terminate()\n",
        "        try:\n",
        "            OLLAMA_SERVER_PROCESS.wait(timeout=10)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            OLLAMA_SERVER_PROCESS.kill()\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "# Where to store models (in the Colab filesystem)\n",
        "os.environ[\"OLLAMA_MODELS\"] = os.path.join(\"/content\", \"ollama-models\")\n",
        "os.makedirs(os.environ[\"OLLAMA_MODELS\"], exist_ok=True)\n",
        "\n",
        "# Basic tuning flags\n",
        "os.environ[\"OLLAMA_MAX_LOADED_MODELS\"] = \"1\"\n",
        "os.environ[\"OLLAMA_NUM_PARALLEL\"] = \"1\"\n",
        "\n",
        "# Host where the API will be available\n",
        "os.environ[\"OLLAMA_HOST\"] = \"http://127.0.0.1:11434\"\n",
        "\n",
        "print(\"ðŸš€ Starting Ollama server...\")\n",
        "print(f\"   Models directory: {os.environ['OLLAMA_MODELS']}\")\n",
        "print(f\"   Server host: {os.environ['OLLAMA_HOST']}\")\n",
        "\n",
        "OLLAMA_SERVER_PROCESS = subprocess.Popen(\n",
        "    [OLLAMA_BIN, \"serve\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    env=os.environ.copy()\n",
        ")\n",
        "\n",
        "# Wait for health endpoint\n",
        "print(\"â³ Waiting for server to start...\")\n",
        "for i in range(60):\n",
        "    try:\n",
        "        r = requests.get(\"http://127.0.0.1:11434/api/tags\", timeout=2)\n",
        "        if r.ok:\n",
        "            print(\"âœ… Ollama server is up at http://127.0.0.1:11434\")\n",
        "            break\n",
        "    except Exception:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "else:\n",
        "    raise RuntimeError(\"Ollama server did not become healthy in time.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ccdc922",
      "metadata": {
        "id": "3ccdc922"
      },
      "source": [
        "## 4. Pull a small model\n",
        "\n",
        "We'll pull the same small instruction-tuned model used in the original **Simple Ollama Environment**:\n",
        "\n",
        "- Default: `qwen2.5:0.5b-instruct`\n",
        "\n",
        "You can change the model name before running the cell (e.g. to `llama3.2:1b` or `qwen2:0.5b`, if available on Ollama).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f776e73",
      "metadata": {
        "id": "1f776e73"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Change this if you want a different model\n",
        "MODEL_NAME = \"qwen2.5:0.5b-instruct\"\n",
        "\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "OLLAMA_HOME = os.path.join(HOME, \"opt\", \"ollama\")\n",
        "# Corrected OLLAMA_BIN path to point to the 'bin' subdirectory\n",
        "OLLAMA_BIN = os.path.join(OLLAMA_HOME, \"bin\", \"ollama\")\n",
        "\n",
        "# Ensure environment variables are set\n",
        "if \"OLLAMA_HOST\" not in os.environ:\n",
        "    os.environ[\"OLLAMA_HOST\"] = \"http://127.0.0.1:11434\"\n",
        "\n",
        "print(f\"ðŸ“¥ Pulling model: {MODEL_NAME}\")\n",
        "print(f\"   Using OLLAMA_HOST: {os.environ['OLLAMA_HOST']}\")\n",
        "\n",
        "# Run with captured output for better error diagnosis\n",
        "result = subprocess.run(\n",
        "    [OLLAMA_BIN, \"pull\", MODEL_NAME],\n",
        "    env=os.environ.copy(),\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Show output\n",
        "if result.stdout:\n",
        "    print(result.stdout)\n",
        "\n",
        "if result.returncode != 0:\n",
        "    print(\"âŒ Error pulling model:\")\n",
        "    print(result.stderr)\n",
        "    raise RuntimeError(f\"Failed to pull model {MODEL_NAME}. See error above.\")\n",
        "\n",
        "print(\"âœ… Model pulled and ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verify_model",
      "metadata": {
        "id": "verify_model"
      },
      "source": [
        "## 4.5 Verify Model Installation\n",
        "\n",
        "Let's verify the model was successfully pulled and is available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verify_model_cell",
      "metadata": {
        "id": "verify_model_cell"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "OLLAMA_HOME = os.path.join(HOME, \"opt\", \"ollama\")\n",
        "OLLAMA_BIN = os.path.join(OLLAMA_HOME, \"bin\", \"ollama\")\n",
        "\n",
        "# Ensure environment variables are set\n",
        "if \"OLLAMA_HOST\" not in os.environ:\n",
        "    os.environ[\"OLLAMA_HOST\"] = \"http://127.0.0.1:11434\"\n",
        "\n",
        "print(\"ðŸ“‹ Listing available models:\")\n",
        "result = subprocess.run(\n",
        "    [OLLAMA_BIN, \"list\"],\n",
        "    env=os.environ.copy(),\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "print(result.stdout)\n",
        "\n",
        "if result.returncode != 0:\n",
        "    print(\"âŒ Error listing models:\")\n",
        "    print(result.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc2f619",
      "metadata": {
        "id": "afc2f619"
      },
      "source": [
        "## 5. Run a quick chat from Python\n",
        "\n",
        "Now that:\n",
        "\n",
        "- the **Ollama server** is running, and  \n",
        "- a model is pulled,\n",
        "\n",
        "we can use the `ollama` Python client to send a simple chat request.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e390d42f",
      "metadata": {
        "id": "e390d42f"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "import os\n",
        "\n",
        "# Ensure the client talks to our local server\n",
        "os.environ[\"OLLAMA_HOST\"] = \"http://127.0.0.1:11434\"\n",
        "\n",
        "MODEL_NAME = \"qwen2.5:0.5b-instruct\"  # keep in sync with the pull cell above\n",
        "\n",
        "print(\"âœ… Using Ollama Python client version:\", getattr(ollama, \"__version__\", \"unknown\"))\n",
        "print(\"ðŸ§  Talking to model:\", MODEL_NAME)\n",
        "print(f\"   Server: {os.environ['OLLAMA_HOST']}\\n\")\n",
        "\n",
        "prompt = \"Di' solo 'Ciao!' in italiano e poi dammi 1 consiglio per studiare meglio.\"\n",
        "\n",
        "print(\"ðŸ’¬ User:\")\n",
        "print(prompt)\n",
        "print(\"\\nðŸ¤– Generating response...\\n\")\n",
        "\n",
        "try:\n",
        "    resp = ollama.chat(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    \n",
        "    print(\"ðŸ¤– Model reply:\")\n",
        "    print(resp[\"message\"][\"content\"])\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error during chat: {e}\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"1. Make sure the server is running (check cell 3)\")\n",
        "    print(\"2. Verify the model was pulled successfully (check cell 4)\")\n",
        "    print(f\"3. Check that OLLAMA_HOST is set: {os.environ.get('OLLAMA_HOST', 'NOT SET')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "streaming_example",
      "metadata": {
        "id": "streaming_example"
      },
      "source": [
        "## 5.5 Streaming Chat Example\n",
        "\n",
        "Here's an example of streaming responses for real-time token generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "streaming_cell",
      "metadata": {
        "id": "streaming_cell"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "import os\n",
        "\n",
        "# Ensure the client talks to our local server\n",
        "os.environ[\"OLLAMA_HOST\"] = \"http://127.0.0.1:11434\"\n",
        "\n",
        "MODEL_NAME = \"qwen2.5:0.5b-instruct\"\n",
        "\n",
        "prompt = \"Write a short poem about AI in 4 lines.\"\n",
        "\n",
        "print(\"ðŸ’¬ User:\", prompt)\n",
        "print(\"\\nðŸ¤– Model reply (streaming):\\n\")\n",
        "\n",
        "try:\n",
        "    stream = ollama.chat(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        stream=True,\n",
        "    )\n",
        "    \n",
        "    for chunk in stream:\n",
        "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
        "    print()  # New line at the end\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error during streaming chat: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac07c1d",
      "metadata": {
        "id": "0ac07c1d"
      },
      "source": [
        "## 6. Next steps\n",
        "\n",
        "You now have:\n",
        "\n",
        "- An **Ollama server** running inside Colab.\n",
        "- A **Python client** ready to call `ollama.chat` / `ollama.generate`.\n",
        "- At least one **local model** pulled.\n",
        "\n",
        "From here you can:\n",
        "\n",
        "- Swap models (edit `MODEL_NAME` and re-run the pull + chat cells).\n",
        "- Use `stream=True` in `ollama.chat` for streaming responses.\n",
        "- Integrate Ollama with other libraries (LangChain, LlamaIndex, etc.) in additional cells.\n",
        "- Try other models like:\n",
        "  - `llama3.2:1b` or `llama3.2:3b`\n",
        "  - `phi4:3.8b`\n",
        "  - `gemma2:2b`\n",
        "  - `deepseek-r1:1.5b`\n",
        "\n",
        "Happy hacking! ðŸš€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "troubleshooting",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## 7. Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "### Server not starting\n",
        "- Check if port 11434 is already in use\n",
        "- Re-run cell 3 to restart the server\n",
        "\n",
        "### Model pull fails\n",
        "- Verify internet connectivity\n",
        "- Try a smaller model first (e.g., `tinyllama:1.1b`)\n",
        "- Check Ollama library is available at https://ollama.com/library\n",
        "\n",
        "### Out of memory\n",
        "- Use smaller models on CPU\n",
        "- With GPU, check memory usage with `!nvidia-smi`\n",
        "- Reduce `OLLAMA_MAX_LOADED_MODELS` or `OLLAMA_NUM_PARALLEL`\n",
        "\n",
        "### Runtime disconnected\n",
        "- You'll need to re-run all cells from the beginning\n",
        "- Models persist in `/content/ollama-models` but server needs restart\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
