{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3.1 \u2013 Agent with watsonx.ai + Accelerator RAG API\n",
        "\n",
        "This notebook implements the Lab 3.1 agent in the `simple-watsonx-environment`.\n",
        "\n",
        "High-level flow:\n",
        "\n",
        "1. The accelerator FastAPI service exposes a `/ask` endpoint for RAG.\n",
        "2. We wrap `/ask` as a **tool** (`rag_service_tool`).\n",
        "3. We add a small **calculator tool** for arithmetic.\n",
        "4. Granite on watsonx.ai acts as a **planner + final answer generator**.\n",
        "\n",
        "You can adapt this to your own models and endpoints as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install dependencies (run once per environment)\n",
        "!pip install -q \"ibm-watsonx-ai>=1.1.22\" requests pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import ast\n",
        "import operator as op\n",
        "from typing import Dict, Any\n",
        "\n",
        "import requests\n",
        "from pydantic import BaseModel\n",
        "from ibm_watsonx_ai import Credentials\n",
        "from ibm_watsonx_ai.foundation_models import Model\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
        "\n",
        "# --- Configuration ---\n",
        "ACCELERATOR_API_URL = os.getenv(\"ACCELERATOR_API_URL\", \"http://localhost:8000/ask\")\n",
        "WATSONX_URL = os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\")\n",
        "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\") or input(\"WATSONX_APIKEY: \")\n",
        "WATSONX_PROJECT_ID = os.getenv(\"WATSONX_PROJECT_ID\") or input(\"WATSONX_PROJECT_ID: \")\n",
        "LLM_MODEL_ID = os.getenv(\"LLM_MODEL_ID\", \"ibm/granite-3-3-8b-instruct\")\n",
        "\n",
        "creds = Credentials(url=WATSONX_URL, api_key=WATSONX_APIKEY)\n",
        "params = {\n",
        "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
        "    GenParams.MAX_NEW_TOKENS: 512,\n",
        "    GenParams.MIN_NEW_TOKENS: 1,\n",
        "    GenParams.TEMPERATURE: 0.2,\n",
        "}\n",
        "\n",
        "planner_model = Model(\n",
        "    model_id=LLM_MODEL_ID,\n",
        "    credentials=creds,\n",
        "    project_id=WATSONX_PROJECT_ID,\n",
        "    params=params,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Tools\n",
        "\n",
        "We define two tools:\n",
        "\n",
        "- `rag_service_tool(question)` \u2013 calls the accelerator `/ask` endpoint.\n",
        "- `calculator_tool(expression)` \u2013 safely evaluates arithmetic expressions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def rag_service_tool(question: str) -> Dict[str, Any]:\n",
        "    \"\"\"Call the accelerator `/ask` endpoint with the given question.\n",
        "\n",
        "    Returns a dict with keys like `answer`, `citations`, `model_id`, `latency_ms`.\n",
        "    The exact schema depends on your accelerator implementation.\n",
        "    \"\"\"\n",
        "    payload = {\"question\": question}\n",
        "    start = time.time()\n",
        "    resp = requests.post(ACCELERATOR_API_URL, json=payload, timeout=60)\n",
        "    latency_ms = int((time.time() - start) * 1000)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    data.setdefault(\"latency_ms\", latency_ms)\n",
        "    return data\n",
        "\n",
        "\n",
        "# --- Safe calculator implementation using AST ---\n",
        "_allowed_operators = {\n",
        "    ast.Add: op.add,\n",
        "    ast.Sub: op.sub,\n",
        "    ast.Mult: op.mul,\n",
        "    ast.Div: op.truediv,\n",
        "    ast.Pow: op.pow,\n",
        "    ast.Mod: op.mod,\n",
        "}\n",
        "\n",
        "\n",
        "def _eval_ast(node):\n",
        "    if isinstance(node, ast.Num):  # type: ignore[attr-defined]\n",
        "        return node.n\n",
        "    if isinstance(node, ast.BinOp) and type(node.op) in _allowed_operators:\n",
        "        return _allowed_operators[type(node.op)](_eval_ast(node.left), _eval_ast(node.right))\n",
        "    if isinstance(node, ast.UnaryOp) and isinstance(node.op, (ast.UAdd, ast.USub)):\n",
        "        value = _eval_ast(node.operand)\n",
        "        return +value if isinstance(node.op, ast.UAdd) else -value\n",
        "    raise ValueError(\"Unsupported expression\")\n",
        "\n",
        "\n",
        "def calculator_tool(expression: str) -> str:\n",
        "    \"\"\"Safely evaluate a simple arithmetic expression.\n",
        "\n",
        "    Supports +, -, *, /, %, and exponentiation. No function calls or variables.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parsed = ast.parse(expression, mode=\"eval\")\n",
        "        result = _eval_ast(parsed.body)\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error evaluating expression: {e}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Planner Schema & Prompt\n",
        "\n",
        "We ask Granite to output **JSON** describing which tool to call and with what arguments.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ToolPlan(BaseModel):\n",
        "    tool: str\n",
        "    arguments: Dict[str, Any]\n",
        "\n",
        "\n",
        "PLANNER_SYSTEM_PROMPT = (\n",
        "    \"You are a planner agent. You must choose exactly ONE tool per request.\\n\\n\"\n",
        "    \"Available tools:\\n\"\n",
        "    \"- rag_service: Use this to answer enterprise questions using the /ask RAG API.\\n\"\n",
        "    \"- calculator: Use this to evaluate arithmetic expressions like '2 * (3 + 4)'.\\n\\n\"\n",
        "    \"Return a JSON object with keys 'tool' and 'arguments'. Do not include any extra text.\\n\"\n",
        "    \"If the user is clearly asking a math question, prefer 'calculator'. Otherwise, prefer 'rag_service'.\"\n",
        ")\n",
        "\n",
        "\n",
        "def plan_tool_call(user_input: str) -> ToolPlan:\n",
        "    \"\"\"Ask the LLM which tool to use and with what arguments.\"\"\"\n",
        "    user_prompt = (\n",
        "        f\"User input: {user_input}\\n\\n\"\n",
        "        \"Respond ONLY with JSON, for example:\\n\"\n",
        "        '{\"tool\": \"calculator\", \"arguments\": {\"expression\": \"2 + 2\"}}'\n",
        "    )\n",
        "    prompt = f\"{PLANNER_SYSTEM_PROMPT}\\n\\n{user_prompt}\"\n",
        "    raw = planner_model.generate_text(prompt=prompt)\n",
        "    text = raw[\"results\"][0][\"generated_text\"].strip()\n",
        "    # Try to extract JSON\n",
        "    try:\n",
        "        plan_dict = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback: try to find JSON substring\n",
        "        start = text.find(\"{\")\n",
        "        end = text.rfind(\"}\") + 1\n",
        "        if start >= 0 and end > start:\n",
        "            plan_dict = json.loads(text[start:end])\n",
        "        else:\n",
        "            raise ValueError(f\"Could not parse JSON from planner output: {text!r}\")\n",
        "    return ToolPlan(**plan_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Answer Step\n",
        "\n",
        "After tool execution, we call the LLM again to generate a user-friendly answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "FINAL_ANSWER_SYSTEM = (\n",
        "    \"You are a helpful assistant. You will be given: the original user question, the tool you used, \"\n",
        "    \"and the tool output. Compose a clear and concise final answer. \"\n",
        "    \"If the tool output indicates an error, explain the error and suggest what the user can try.\"\n",
        ")\n",
        "\n",
        "\n",
        "def generate_final_answer(user_input: str, tool_name: str, tool_output: Any) -> str:\n",
        "    \"\"\"Call the LLM to turn tool output into a final answer.\"\"\"\n",
        "    context = (\n",
        "        f\"User question: {user_input}\\n\"\n",
        "        f\"Tool used: {tool_name}\\n\"\n",
        "        f\"Tool output: {tool_output}\\n\"\n",
        "        \"\\nPlease write the final answer for the user.\"\n",
        "    )\n",
        "    prompt = f\"{FINAL_ANSWER_SYSTEM}\\n\\n{context}\"\n",
        "    raw = planner_model.generate_text(prompt=prompt)\n",
        "    return raw[\"results\"][0][\"generated_text\"].strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Orchestrate One Turn of the Agent\n",
        "\n",
        "`run_agent_once` ties everything together:\n",
        "\n",
        "1. Planner picks a tool.\n",
        "2. Python executes the tool.\n",
        "3. LLM generates a final answer.\n",
        "4. We return a structured record that you can log or analyze later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def run_agent_once(user_input: str) -> Dict[str, Any]:\n",
        "    plan = plan_tool_call(user_input)\n",
        "    tool_name = plan.tool\n",
        "    args = plan.arguments or {}\n",
        "\n",
        "    if tool_name == \"rag_service\":\n",
        "        question = args.get(\"question\") or user_input\n",
        "        tool_output = rag_service_tool(question)\n",
        "    elif tool_name == \"calculator\":\n",
        "        expr = args.get(\"expression\") or user_input\n",
        "        tool_output = calculator_tool(expr)\n",
        "    else:\n",
        "        tool_output = f\"Unknown tool: {tool_name}\"\n",
        "\n",
        "    final_answer = generate_final_answer(user_input, tool_name, tool_output)\n",
        "\n",
        "    return {\n",
        "        \"question\": user_input,\n",
        "        \"tool\": tool_name,\n",
        "        \"tool_args\": args,\n",
        "        \"tool_output\": tool_output,\n",
        "        \"final_answer\": final_answer,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Smoke Tests\n",
        "\n",
        "Run a couple of tests:\n",
        "\n",
        "- A RAG-style question (should pick `rag_service`).\n",
        "- A pure math question (should pick `calculator`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "test_questions = [\n",
        "    \"What is RAG and why do we use it?\",\n",
        "    \"What is 2 * (3 + 4)?\",\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Q:\", q)\n",
        "    result = run_agent_once(q)\n",
        "    print(\"Tool:\", result[\"tool\"], \"Args:\", result[\"tool_args\"])\n",
        "    print(\"Final answer:\\n\", result[\"final_answer\"])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}