{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3.3 - Agentic AI with LangGraph + watsonx.ai\n",
        "\n",
        "This notebook demonstrates how to build a stateful agent workflow using LangGraph with watsonx.ai.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- How to create stateful agent workflows with LangGraph\n",
        "- Integrating watsonx.ai models with LangGraph\n",
        "- Building multi-node agent graphs with conditional routing\n",
        "- Implementing RAG (Retrieval-Augmented Generation) pipelines\n",
        "- Managing agent state across multiple steps\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "User Question\n",
        "     |\n",
        "     v\n",
        "  Router Node (decides which path to take)\n",
        "     |\n",
        "     +----------+----------+\n",
        "     |          |          |\n",
        "     v          v          v\n",
        "  RAG Node  Calculator  Direct Answer\n",
        "     |          |          |\n",
        "     +----------+----------+\n",
        "     |\n",
        "     v\n",
        " Generation Node (watsonx.ai)\n",
        "     |\n",
        "     v\n",
        "Final Answer\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "### Google Colab Compatibility\n",
        "\n",
        "This notebook works both locally and in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"‚úì Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚úì Running in local environment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q langgraph langchain-core langchain-ibm \"ibm-watsonx-ai>=1.1.22\" requests typing-extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure watsonx.ai Credentials\n",
        "\n",
        "Set up your IBM watsonx.ai credentials.\n",
        "\n",
        "### Required Credentials\n",
        "\n",
        "1. **API Key** - Get from [IBM Cloud IAM](https://cloud.ibm.com/iam/apikeys)\n",
        "2. **Project ID** - From your watsonx.ai project settings\n",
        "3. **URL** - Regional endpoint (default: us-south)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Configuration for watsonx.ai\n",
        "WATSONX_URL = os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\")\n",
        "\n",
        "if not os.getenv(\"WATSONX_APIKEY\"):\n",
        "    WATSONX_APIKEY = getpass(\"Enter your watsonx.ai API Key: \")\n",
        "else:\n",
        "    WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
        "\n",
        "if not os.getenv(\"WATSONX_PROJECT_ID\"):\n",
        "    WATSONX_PROJECT_ID = getpass(\"Enter your watsonx.ai Project ID: \")\n",
        "else:\n",
        "    WATSONX_PROJECT_ID = os.getenv(\"WATSONX_PROJECT_ID\")\n",
        "\n",
        "# Model configuration\n",
        "LLM_MODEL_ID = os.getenv(\"LLM_MODEL_ID\", \"ibm/granite-3-8b-instruct\")\n",
        "\n",
        "# Accelerator API URL\n",
        "ACCELERATOR_API_URL = os.getenv(\"ACCELERATOR_API_URL\", \"http://localhost:8000/ask\")\n",
        "\n",
        "print(\"‚úì Configuration loaded\")\n",
        "print(f\"  Model: {LLM_MODEL_ID}\")\n",
        "print(f\"  URL: {WATSONX_URL}\")\n",
        "print(f\"  RAG API: {ACCELERATOR_API_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize watsonx.ai LLM\n",
        "\n",
        "We'll use the LangChain integration for watsonx.ai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ibm import WatsonxLLM\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Configure model parameters for better responses\n",
        "model_params = {\n",
        "    \"decoding_method\": \"greedy\",\n",
        "    \"max_new_tokens\": 1500,\n",
        "    \"min_new_tokens\": 1,\n",
        "    \"temperature\": 0.3,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 1\n",
        "}\n",
        "\n",
        "# Initialize watsonx.ai LLM\n",
        "llm = WatsonxLLM(\n",
        "    model_id=LLM_MODEL_ID,\n",
        "    url=WATSONX_URL,\n",
        "    apikey=WATSONX_APIKEY,\n",
        "    project_id=WATSONX_PROJECT_ID,\n",
        "    params=model_params\n",
        ")\n",
        "\n",
        "print(\"‚úì watsonx.ai LLM initialized successfully\")\n",
        "\n",
        "# Test the LLM\n",
        "test_response = llm.invoke(\"What is AI?\")\n",
        "print(f\"\\nTest response: {test_response[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Graph State\n",
        "\n",
        "LangGraph uses a typed state that flows through the graph nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    State for our LangGraph agent.\n",
        "    \n",
        "    Attributes:\n",
        "        input_text: Original user question\n",
        "        question_type: Type of question (rag, calculator, direct)\n",
        "        rag_answer: Response from RAG service (if applicable)\n",
        "        calculation_result: Result from calculator (if applicable)\n",
        "        intermediate_context: Any intermediate processing context\n",
        "        final_answer: Polished final answer from LLM\n",
        "        metadata: Additional metadata about the processing\n",
        "    \"\"\"\n",
        "    input_text: str\n",
        "    question_type: Optional[str]\n",
        "    rag_answer: Optional[Dict[str, Any]]\n",
        "    calculation_result: Optional[str]\n",
        "    intermediate_context: Optional[str]\n",
        "    final_answer: Optional[str]\n",
        "    metadata: Optional[Dict[str, Any]]\n",
        "\n",
        "\n",
        "print(\"‚úì Graph state defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Node Functions\n",
        "\n",
        "Each node in the graph performs a specific function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Router Node\n",
        "\n",
        "The router determines which path the question should take."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def router_node(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Route the question to the appropriate handler.\n",
        "    \n",
        "    Logic:\n",
        "    - If question contains math expressions or numbers with operators -> calculator\n",
        "    - If question asks about knowledge/facts -> RAG\n",
        "    - Otherwise -> direct answer\n",
        "    \"\"\"\n",
        "    question = state[\"input_text\"].lower()\n",
        "    \n",
        "    # Check for mathematical expressions\n",
        "    math_patterns = [\n",
        "        r'\\d+\\s*[+\\-*/]\\s*\\d+',  # Basic arithmetic\n",
        "        r'calculate|compute|what is \\d+',  # Math keywords\n",
        "        r'\\d+\\s*\\*\\*\\s*\\d+',  # Power\n",
        "    ]\n",
        "    \n",
        "    for pattern in math_patterns:\n",
        "        if re.search(pattern, question):\n",
        "            return {\"question_type\": \"calculator\"}\n",
        "    \n",
        "    # Check for knowledge/RAG questions\n",
        "    rag_keywords = [\n",
        "        'what is', 'explain', 'describe', 'tell me about',\n",
        "        'how does', 'why', 'rag', 'retrieval', 'watsonx',\n",
        "        'granite', 'ai', 'machine learning', 'llm'\n",
        "    ]\n",
        "    \n",
        "    if any(keyword in question for keyword in rag_keywords):\n",
        "        return {\"question_type\": \"rag\"}\n",
        "    \n",
        "    # Default to direct answer\n",
        "    return {\"question_type\": \"direct\"}\n",
        "\n",
        "\n",
        "print(\"‚úì Router node defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RAG Node\n",
        "\n",
        "Calls the RAG service to retrieve relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def rag_node(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Query the RAG service with the user's question.\n",
        "    \"\"\"\n",
        "    question = state[\"input_text\"]\n",
        "    \n",
        "    try:\n",
        "        payload = {\"question\": question}\n",
        "        resp = requests.post(ACCELERATOR_API_URL, json=payload, timeout=60)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        \n",
        "        return {\n",
        "            \"rag_answer\": data,\n",
        "            \"intermediate_context\": f\"Retrieved answer from RAG service\",\n",
        "            \"metadata\": {\"source\": \"rag_service\", \"success\": True}\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"rag_answer\": {\"error\": str(e)},\n",
        "            \"intermediate_context\": f\"Error calling RAG service: {str(e)}\",\n",
        "            \"metadata\": {\"source\": \"rag_service\", \"success\": False, \"error\": str(e)}\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"‚úì RAG node defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculator Node\n",
        "\n",
        "Performs safe arithmetic calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "import operator as op\n",
        "\n",
        "# Safe calculator implementation\n",
        "_allowed_operators = {\n",
        "    ast.Add: op.add,\n",
        "    ast.Sub: op.sub,\n",
        "    ast.Mult: op.mul,\n",
        "    ast.Div: op.truediv,\n",
        "    ast.Pow: op.pow,\n",
        "}\n",
        "\n",
        "def _eval_ast(node):\n",
        "    if isinstance(node, ast.Num):  # Python 3.7\n",
        "        return node.n\n",
        "    if isinstance(node, ast.Constant):  # Python 3.8+\n",
        "        return node.value\n",
        "    if isinstance(node, ast.BinOp) and type(node.op) in _allowed_operators:\n",
        "        return _allowed_operators[type(node.op)](_eval_ast(node.left), _eval_ast(node.right))\n",
        "    if isinstance(node, ast.UnaryOp) and isinstance(node.op, (ast.UAdd, ast.USub)):\n",
        "        value = _eval_ast(node.operand)\n",
        "        return +value if isinstance(node.op, ast.UAdd) else -value\n",
        "    raise ValueError(\"Unsupported expression\")\n",
        "\n",
        "def calculator_node(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract and evaluate mathematical expressions from the question.\n",
        "    \"\"\"\n",
        "    question = state[\"input_text\"]\n",
        "    \n",
        "    # Try to extract mathematical expression\n",
        "    # Look for patterns like \"calculate X\" or \"what is X\"\n",
        "    patterns = [\n",
        "        r'calculate[:\\s]+(.+)',\n",
        "        r'compute[:\\s]+(.+)',\n",
        "        r'what is[:\\s]+(.+)',\n",
        "        r'solve[:\\s]+(.+)',\n",
        "    ]\n",
        "    \n",
        "    expression = question\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, question.lower())\n",
        "        if match:\n",
        "            expression = match.group(1).strip()\n",
        "            break\n",
        "    \n",
        "    # Clean up the expression\n",
        "    expression = expression.strip('?!.')\n",
        "    \n",
        "    try:\n",
        "        parsed = ast.parse(expression, mode=\"eval\")\n",
        "        result = _eval_ast(parsed.body)\n",
        "        \n",
        "        return {\n",
        "            \"calculation_result\": str(result),\n",
        "            \"intermediate_context\": f\"Calculated: {expression} = {result}\",\n",
        "            \"metadata\": {\"source\": \"calculator\", \"success\": True, \"expression\": expression}\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"calculation_result\": f\"Error: {str(e)}\",\n",
        "            \"intermediate_context\": f\"Failed to calculate: {expression}\",\n",
        "            \"metadata\": {\"source\": \"calculator\", \"success\": False, \"error\": str(e)}\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"‚úì Calculator node defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Direct Answer Node\n",
        "\n",
        "For simple questions that don't need RAG or calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def direct_node(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Handle direct questions without tools.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"intermediate_context\": \"Question will be answered directly by the LLM\",\n",
        "        \"metadata\": {\"source\": \"direct\", \"success\": True}\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"‚úì Direct answer node defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generation Node\n",
        "\n",
        "The final node that uses watsonx.ai to generate the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generation_node(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Use watsonx.ai to generate the final answer based on the state.\n",
        "    \"\"\"\n",
        "    question = state[\"input_text\"]\n",
        "    question_type = state.get(\"question_type\", \"unknown\")\n",
        "    \n",
        "    # Build context based on question type\n",
        "    if question_type == \"rag\" and state.get(\"rag_answer\"):\n",
        "        rag_data = state[\"rag_answer\"]\n",
        "        answer = rag_data.get(\"answer\") or rag_data.get(\"result\") or \"No answer available\"\n",
        "        citations = rag_data.get(\"citations\") or rag_data.get(\"chunks\") or []\n",
        "        \n",
        "        prompt_template = ChatPromptTemplate.from_template(\n",
        "            \"You are a helpful AI assistant powered by watsonx.ai.\\n\\n\"\n",
        "            \"Question: {question}\\n\\n\"\n",
        "            \"Context from knowledge base:\\n{context}\\n\\n\"\n",
        "            \"Citations: {citations}\\n\\n\"\n",
        "            \"Please provide a clear, concise answer based on the context above. \"\n",
        "            \"If the context doesn't fully answer the question, say so.\"\n",
        "        )\n",
        "        \n",
        "        formatted = prompt_template.format_messages(\n",
        "            question=question,\n",
        "            context=answer,\n",
        "            citations=json.dumps(citations, indent=2) if citations else \"None\"\n",
        "        )\n",
        "        \n",
        "    elif question_type == \"calculator\" and state.get(\"calculation_result\"):\n",
        "        result = state[\"calculation_result\"]\n",
        "        \n",
        "        prompt_template = ChatPromptTemplate.from_template(\n",
        "            \"You are a helpful AI assistant powered by watsonx.ai.\\n\\n\"\n",
        "            \"Question: {question}\\n\\n\"\n",
        "            \"Calculation result: {result}\\n\\n\"\n",
        "            \"Please explain the calculation and present the answer clearly.\"\n",
        "        )\n",
        "        \n",
        "        formatted = prompt_template.format_messages(\n",
        "            question=question,\n",
        "            result=result\n",
        "        )\n",
        "        \n",
        "    else:\n",
        "        # Direct answer\n",
        "        prompt_template = ChatPromptTemplate.from_template(\n",
        "            \"You are a helpful AI assistant powered by watsonx.ai.\\n\\n\"\n",
        "            \"Question: {question}\\n\\n\"\n",
        "            \"Please provide a helpful, accurate answer.\"\n",
        "        )\n",
        "        \n",
        "        formatted = prompt_template.format_messages(question=question)\n",
        "    \n",
        "    # Generate response\n",
        "    prompt_text = formatted[0].content\n",
        "    final_answer = llm.invoke(prompt_text)\n",
        "    \n",
        "    return {\"final_answer\": final_answer}\n",
        "\n",
        "\n",
        "print(\"‚úì Generation node defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Build the LangGraph\n",
        "\n",
        "Now we'll connect all the nodes into a graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import START, END, StateGraph\n",
        "\n",
        "# Create the graph\n",
        "graph = StateGraph(GraphState)\n",
        "\n",
        "# Add nodes\n",
        "graph.add_node(\"router\", router_node)\n",
        "graph.add_node(\"rag\", rag_node)\n",
        "graph.add_node(\"calculator\", calculator_node)\n",
        "graph.add_node(\"direct\", direct_node)\n",
        "graph.add_node(\"generation\", generation_node)\n",
        "\n",
        "# Add edges from START to router\n",
        "graph.add_edge(START, \"router\")\n",
        "\n",
        "# Add conditional edges from router to appropriate handler\n",
        "def route_question(state: GraphState) -> str:\n",
        "    \"\"\"Determine which node to route to based on question type.\"\"\"\n",
        "    q_type = state.get(\"question_type\", \"direct\")\n",
        "    return q_type\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    \"router\",\n",
        "    route_question,\n",
        "    {\n",
        "        \"rag\": \"rag\",\n",
        "        \"calculator\": \"calculator\",\n",
        "        \"direct\": \"direct\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# All paths lead to generation\n",
        "graph.add_edge(\"rag\", \"generation\")\n",
        "graph.add_edge(\"calculator\", \"generation\")\n",
        "graph.add_edge(\"direct\", \"generation\")\n",
        "\n",
        "# Generation leads to END\n",
        "graph.add_edge(\"generation\", END)\n",
        "\n",
        "# Compile the graph\n",
        "app = graph.compile()\n",
        "\n",
        "print(\"‚úì LangGraph compiled successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize the Graph (Optional)\n",
        "\n",
        "LangGraph can generate a visual representation of the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to display the graph structure\n",
        "try:\n",
        "    from IPython.display import Image, display\n",
        "    display(Image(app.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "    print(f\"Could not display graph visualization: {e}\")\n",
        "    print(\"\\nGraph structure:\")\n",
        "    print(\"START -> router -> [rag|calculator|direct] -> generation -> END\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test the Agent\n",
        "\n",
        "Let's test our LangGraph agent with different types of questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 1: Knowledge Question (RAG Path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_1 = \"What is Retrieval-Augmented Generation and why is it important?\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"QUESTION: {question_1}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "state_1 = {\"input_text\": question_1}\n",
        "result_1 = app.invoke(state_1)\n",
        "\n",
        "print(f\"\\nRoute taken: {result_1.get('question_type', 'unknown')}\")\n",
        "print(f\"Intermediate: {result_1.get('intermediate_context', 'N/A')}\")\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(\"=\" * 80)\n",
        "print(result_1.get('final_answer', 'No answer generated'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 2: Math Question (Calculator Path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_2 = \"Calculate: (25 + 15) * 2 - 10\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"QUESTION: {question_2}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "state_2 = {\"input_text\": question_2}\n",
        "result_2 = app.invoke(state_2)\n",
        "\n",
        "print(f\"\\nRoute taken: {result_2.get('question_type', 'unknown')}\")\n",
        "print(f\"Intermediate: {result_2.get('intermediate_context', 'N/A')}\")\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(\"=\" * 80)\n",
        "print(result_2.get('final_answer', 'No answer generated'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 3: General Question (Direct Path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_3 = \"Hello! How can you help me today?\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"QUESTION: {question_3}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "state_3 = {\"input_text\": question_3}\n",
        "result_3 = app.invoke(state_3)\n",
        "\n",
        "print(f\"\\nRoute taken: {result_3.get('question_type', 'unknown')}\")\n",
        "print(f\"Intermediate: {result_3.get('intermediate_context', 'N/A')}\")\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(\"=\" * 80)\n",
        "print(result_3.get('final_answer', 'No answer generated'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Interactive Agent Function\n",
        "\n",
        "Create a helper function for easy interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_agent(question: str, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Ask a question to the LangGraph agent.\n",
        "    \n",
        "    Args:\n",
        "        question: Your question\n",
        "        verbose: Whether to print detailed information\n",
        "    \n",
        "    Returns:\n",
        "        The final answer\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"QUESTION: {question}\")\n",
        "        print(\"=\" * 80)\n",
        "    \n",
        "    state = {\"input_text\": question}\n",
        "    result = app.invoke(state)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nüìç Route: {result.get('question_type', 'unknown').upper()}\")\n",
        "        print(f\"üîÑ Process: {result.get('intermediate_context', 'N/A')}\")\n",
        "        \n",
        "        metadata = result.get('metadata', {})\n",
        "        if metadata:\n",
        "            print(f\"üìä Metadata: {json.dumps(metadata, indent=2)}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"üí° ANSWER:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(result.get('final_answer', 'No answer generated'))\n",
        "        print(\"=\"* 80)\n",
        "    \n",
        "    return result.get('final_answer', 'No answer generated')\n",
        "\n",
        "\n",
        "# Try it out!\n",
        "# ask_agent(\"Your question here\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Advanced: Stream the Agent Execution\n",
        "\n",
        "LangGraph supports streaming to see each step as it happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_agent_stream(question: str):\n",
        "    \"\"\"\n",
        "    Ask a question and stream the execution step by step.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"QUESTION: {question}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nüîÑ Execution Flow:\\n\")\n",
        "    \n",
        "    state = {\"input_text\": question}\n",
        "    \n",
        "    for step_num, step in enumerate(app.stream(state), 1):\n",
        "        node_name = list(step.keys())[0]\n",
        "        node_output = step[node_name]\n",
        "        \n",
        "        print(f\"Step {step_num}: {node_name.upper()}\")\n",
        "        \n",
        "        # Show relevant updates\n",
        "        if \"question_type\" in node_output:\n",
        "            print(f\"  ‚îî‚îÄ Routed to: {node_output['question_type']}\")\n",
        "        if \"intermediate_context\" in node_output:\n",
        "            print(f\"  ‚îî‚îÄ {node_output['intermediate_context']}\")\n",
        "        if \"final_answer\" in node_output:\n",
        "            print(f\"  ‚îî‚îÄ Generated final answer\")\n",
        "        \n",
        "        print()\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"üí° FINAL ANSWER:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Get final result\n",
        "    final_result = app.invoke(state)\n",
        "    print(final_result.get('final_answer', 'No answer generated'))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# Example\n",
        "# ask_agent_stream(\"Explain what watsonx.ai is\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **LangGraph Fundamentals**: Built a stateful agent workflow with multiple nodes\n",
        "2. **Conditional Routing**: Implemented intelligent routing based on question type\n",
        "3. **watsonx.ai Integration**: Used Granite models for response generation\n",
        "4. **State Management**: Tracked state across multiple processing steps\n",
        "5. **RAG Pipeline**: Integrated external knowledge retrieval\n",
        "\n",
        "### Key Components\n",
        "\n",
        "- **StateGraph**: The main graph structure that manages flow\n",
        "- **Nodes**: Individual processing units (router, rag, calculator, generation)\n",
        "- **Edges**: Connections between nodes (conditional and direct)\n",
        "- **State**: Typed dictionary that flows through the graph\n",
        "\n",
        "### Advantages of LangGraph\n",
        "\n",
        "1. **Explicit Control Flow**: Clear, visual representation of agent logic\n",
        "2. **State Management**: Built-in state tracking across steps\n",
        "3. **Debugging**: Easy to inspect state at each node\n",
        "4. **Streaming**: Can stream execution for real-time feedback\n",
        "5. **Flexibility**: Easy to add new nodes or modify routing logic\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Clear State Schema**: Define a comprehensive state type\n",
        "2. **Single Responsibility**: Each node should do one thing well\n",
        "3. **Error Handling**: Include try-catch blocks in nodes\n",
        "4. **Metadata Tracking**: Add metadata for debugging and analysis\n",
        "5. **Modular Design**: Keep nodes independent and reusable\n",
        "\n",
        "### Comparison with Other Frameworks\n",
        "\n",
        "| Feature | LangGraph | CrewAI | Plain LangChain |\n",
        "|---------|-----------|--------|------------------|\n",
        "| State Management | ‚úÖ Built-in | ‚ö†Ô∏è Manual | ‚ùå Manual |\n",
        "| Visual Flow | ‚úÖ Yes | ‚ùå No | ‚ùå No |\n",
        "| Conditional Routing | ‚úÖ Native | ‚ö†Ô∏è Limited | ‚úÖ Manual |\n",
        "| Streaming | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |\n",
        "| Learning Curve | Medium | Easy | Easy |\n",
        "\n",
        "### When to Use LangGraph\n",
        "\n",
        "- Complex multi-step workflows\n",
        "- Need for explicit state management\n",
        "- Conditional routing between different paths\n",
        "- Debugging and visualization requirements\n",
        "- Production-grade agent systems\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore the CrewAI notebook for team-based agents\n",
        "- Try Langflow for visual agent building\n",
        "- Add more nodes (web search, database query, etc.)\n",
        "- Implement memory/conversation history\n",
        "- Build more complex conditional logic\n",
        "\n",
        "---\n",
        "\n",
        "**Course**: Multi-Agent Systems with watsonx.ai  \n",
        "**Lab**: 3.3 - LangGraph Integration  \n",
        "**Platform**: Compatible with Google Colab and local environments"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
