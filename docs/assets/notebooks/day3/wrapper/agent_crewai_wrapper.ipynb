{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Hw3Po5d1eOgo",
      "metadata": {
        "id": "Hw3Po5d1eOgo"
      },
      "source": [
        "# IBM watsonx.ai + CrewAI Workshop\n",
        "\n",
        "This notebook shows how to:\n",
        "\n",
        "- Connect to **IBM watsonx.ai** from Python\n",
        "- Wrap watsonx.ai models so they can be used as a **custom LLM inside CrewAI**\n",
        "- Build a small **multi‚Äëagent workflow** (a research & writing crew) powered by watsonx.ai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vkMXKTkEeOgp",
      "metadata": {
        "id": "vkMXKTkEeOgp"
      },
      "source": [
        "## 0. Prerequisites\n",
        "\n",
        "To run this notebook you need:\n",
        "\n",
        "- Python 3.10+\n",
        "- An IBM Cloud account with access to **watsonx.ai**\n",
        "- A watsonx.ai **service instance**, **project**, and **API key**\n",
        "- Basic familiarity with Python and virtual environments\n",
        "\n",
        "> üí° Run the cells in order from top to bottom the first time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ppyrcJAPeOgp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppyrcJAPeOgp",
        "outputId": "5be161d1-9a0b-4de2-e0af-c9666fc2eb3d"
      },
      "outputs": [],
      "source": [
        "# 1) Install required packages\n",
        "# Run this once in each new environment.\n",
        "# Remove the leading '%' if your environment does not support IPython magics.\n",
        "\n",
        "%pip install -U \"crewai[tools]\" langchain-ibm ibm-watsonx-ai python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RtZPID29eOgp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtZPID29eOgp",
        "outputId": "63815486-707a-4b20-b80c-68afaa449a9e"
      },
      "outputs": [],
      "source": [
        "# 2) Quick version check (optional)\n",
        "\n",
        "import sys, platform\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Platform:\", platform.platform())\n",
        "\n",
        "try:\n",
        "    import crewai\n",
        "    print(\"crewai:\", crewai.__version__)\n",
        "except Exception as e:\n",
        "    print(\"crewai not imported yet:\", e)\n",
        "\n",
        "try:\n",
        "    import langchain_ibm\n",
        "    print(\"langchain-ibm:\", getattr(langchain_ibm, \"__version__\", \"unknown\"))\n",
        "except Exception as e:\n",
        "    print(\"langchain-ibm not imported yet:\", e)\n",
        "\n",
        "try:\n",
        "    import ibm_watsonx_ai\n",
        "    print(\"ibm-watsonx-ai:\", getattr(ibm_watsonx_ai, \"__version__\", \"unknown\"))\n",
        "except Exception as e:\n",
        "    print(\"ibm-watsonx-ai not imported yet:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RVcrC1TFeOgp",
      "metadata": {
        "id": "RVcrC1TFeOgp"
      },
      "source": [
        "## 1. Configure IBM watsonx.ai credentials\n",
        "\n",
        "You‚Äôll need three pieces of information:\n",
        "\n",
        "- **IBM Cloud API key** for watsonx.ai\n",
        "- **Service URL** (depends on your region), for example:\n",
        "  - `https://us-south.ml.cloud.ibm.com` (Dallas)\n",
        "  - `https://eu-de.ml.cloud.ibm.com` (Frankfurt)\n",
        "  - `https://eu-gb.ml.cloud.ibm.com` (London)\n",
        "  - `https://jp-tok.ml.cloud.ibm.com` (Tokyo)\n",
        "  - `https://au-syd.ml.cloud.ibm.com` (Sydney)\n",
        "- **Project ID** for your watsonx.ai project\n",
        "\n",
        "The next cell will ask for these values and store them in environment variables:\n",
        "\n",
        "- `WATSONX_APIKEY` and `WATSONX_API_KEY` (both set for compatibility)\n",
        "- `WATSONX_URL`\n",
        "- `WATSONX_PROJECT_ID`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ywZuodoyeOgq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywZuodoyeOgq",
        "outputId": "1282147e-e184-4d19-b4f1-f6949a517539"
      },
      "outputs": [],
      "source": [
        "# 3) Enter IBM watsonx.ai credentials\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"üëâ Enter your IBM watsonx.ai credentials (they stay only in this notebook session).\")\n",
        "\n",
        "WATSONX_API_KEY = getpass(\"IBM Cloud API key for watsonx.ai: \")\n",
        "WATSONX_URL = input(\"watsonx.ai URL (e.g. https://us-south.ml.cloud.ibm.com): \").strip()\n",
        "WATSONX_PROJECT_ID = input(\"watsonx.ai Project ID: \").strip()\n",
        "\n",
        "# Store in environment variables (some libraries expect one name, some the other)\n",
        "os.environ[\"WATSONX_APIKEY\"] = WATSONX_API_KEY\n",
        "os.environ[\"WATSONX_API_KEY\"] = WATSONX_API_KEY\n",
        "os.environ[\"WATSONX_URL\"] = WATSONX_URL\n",
        "os.environ[\"WATSONX_PROJECT_ID\"] = WATSONX_PROJECT_ID\n",
        "\n",
        "print(\"\\n‚úÖ Environment variables set: WATSONX_APIKEY, WATSONX_API_KEY, WATSONX_URL, WATSONX_PROJECT_ID\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xq5b_Mt0fHf5",
      "metadata": {
        "id": "xq5b_Mt0fHf5"
      },
      "outputs": [],
      "source": [
        "WATSONX_URL = \"https://us-south.ml.cloud.ibm.com\"\n",
        "os.environ[\"WATSONX_URL\"] = WATSONX_URL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hdO__hmleOgq",
      "metadata": {
        "id": "hdO__hmleOgq"
      },
      "source": [
        "## 2. Quick sanity check: Chat with a watsonx.ai model\n",
        "\n",
        "We‚Äôll use the `ChatWatsonx` integration from **langchain-ibm** to make a simple\n",
        "chat call and verify that credentials and networking work correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T_giNWM8eOgq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_giNWM8eOgq",
        "outputId": "585f50e8-7696-4185-8a84-a0e0997d2c0a"
      },
      "outputs": [],
      "source": [
        "# 4) Simple chat call to watsonx.ai\n",
        "\n",
        "from langchain_ibm import ChatWatsonx\n",
        "\n",
        "# Pick any chat-capable model that is enabled in your region.\n",
        "# You can adjust this later.\n",
        "WATSONX_MODEL_ID = \"ibm/granite-3-8b-instruct\"\n",
        "\n",
        "parameters = {\n",
        "    \"temperature\": 0.3,\n",
        "    \"max_tokens\": 256,\n",
        "}\n",
        "\n",
        "chat = ChatWatsonx(\n",
        "    model_id=WATSONX_MODEL_ID,\n",
        "    url=os.environ[\"WATSONX_URL\"],\n",
        "    project_id=os.environ[\"WATSONX_PROJECT_ID\"],\n",
        "    params=parameters,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    (\"system\", \"You are a concise technical assistant.\"),\n",
        "    (\"human\", \"In 3 bullet points, explain what CrewAI is.\"),\n",
        "]\n",
        "\n",
        "print(\"‚è≥ Calling watsonx.ai...\")\n",
        "response = chat.invoke(messages)\n",
        "print(\"\\n=== watsonx.ai response ===\\n\")\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UDlN9xJyeOgq",
      "metadata": {
        "id": "UDlN9xJyeOgq"
      },
      "source": [
        "## 3. Creating a CrewAI‚Äëcompatible watsonx LLM\n",
        "\n",
        "CrewAI expects language models to follow a simple interface. For providers that\n",
        "don‚Äôt have first‚Äëclass support in LiteLLM yet, CrewAI exposes a `BaseLLM` class\n",
        "you can subclass.\n",
        "\n",
        "We‚Äôll now build a small wrapper around `ChatWatsonx` that implements this\n",
        "interface so we can attach watsonx models directly to CrewAI agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9oMoXmwieOgq",
      "metadata": {
        "id": "9oMoXmwieOgq"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "from crewai import BaseLLM\n",
        "from langchain_ibm import ChatWatsonx\n",
        "\n",
        "\n",
        "class WatsonxCrewAILLM(BaseLLM):\n",
        "    \"\"\"CrewAI-compatible wrapper around IBM watsonx.ai via ChatWatsonx.\n",
        "\n",
        "    This implementation focuses on simple text-only chat.\n",
        "    Tool calling could be added later by extending `call()`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str,\n",
        "        url: str,\n",
        "        project_id: str,\n",
        "        api_key: Optional[str] = None,\n",
        "        temperature: float = 0.3,\n",
        "        max_tokens: int = 512,\n",
        "        top_p: float = 1.0,\n",
        "    ) -> None:\n",
        "        # REQUIRED: call parent constructor with a model name & temperature\n",
        "        super().__init__(model=model_id, temperature=temperature)\n",
        "\n",
        "        import os\n",
        "\n",
        "        if api_key is None:\n",
        "            api_key = os.getenv(\"WATSONX_APIKEY\") or os.getenv(\"WATSONX_API_KEY\")\n",
        "\n",
        "        if not api_key:\n",
        "            raise ValueError(\n",
        "                \"No IBM watsonx API key provided. \"\n",
        "                \"Set WATSONX_APIKEY/WATSONX_API_KEY or pass api_key explicitly.\"\n",
        "            )\n",
        "\n",
        "        # Make sure underlying SDK sees an API key\n",
        "        os.environ.setdefault(\"WATSONX_APIKEY\", api_key)\n",
        "        os.environ.setdefault(\"WATSONX_API_KEY\", api_key)\n",
        "\n",
        "        self.url = url\n",
        "        self.project_id = project_id\n",
        "        self.model_id = model_id\n",
        "\n",
        "        # Configure the underlying chat model\n",
        "        self._chat = ChatWatsonx(\n",
        "            model_id=model_id,\n",
        "            url=url,\n",
        "            project_id=project_id,\n",
        "            params={\n",
        "                \"temperature\": temperature,\n",
        "                \"max_tokens\": max_tokens,\n",
        "                \"top_p\": top_p,\n",
        "            },\n",
        "        )\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        messages: Union[str, List[Dict[str, str]]],\n",
        "        tools: Optional[List[dict]] = None,\n",
        "        callbacks: Optional[List[Any]] = None,\n",
        "        available_functions: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs, # Add kwargs to accept unexpected arguments\n",
        "    ) -> str:\n",
        "        \"\"\"Core entry point used by CrewAI.\n",
        "\n",
        "        For this workshop we ignore tools/function-calling and just forward\n",
        "        the content to watsonx.ai and return the model's text.\n",
        "        \"\"\"\n",
        "        # Normalize input into something ChatWatsonx understands\n",
        "        if isinstance(messages, str):\n",
        "            # Just a plain user prompt\n",
        "            chat_input = messages\n",
        "        else:\n",
        "            # Expect list of {\"role\": ..., \"content\": ...}\n",
        "            processed: List[tuple] = []\n",
        "            for m in messages:\n",
        "                role = m.get(\"role\", \"user\")\n",
        "                content = m.get(\"content\", \"\")\n",
        "                if not content:\n",
        "                    continue\n",
        "                processed.append((role, content))\n",
        "\n",
        "            # If we didn't manage to build a structured chat, fall back to flat text\n",
        "            if processed:\n",
        "                chat_input = processed\n",
        "            else:\n",
        "                chat_input = \"\"\n",
        "\n",
        "        # Call watsonx.ai\n",
        "        result = self._chat.invoke(chat_input)\n",
        "        # langchain-ibm returns an AIMessage; we want the text content\n",
        "        return getattr(result, \"content\", str(result))\n",
        "\n",
        "    # Optional but recommended extra metadata methods\n",
        "\n",
        "    def supports_function_calling(self) -> bool:\n",
        "        \"\"\"Tell CrewAI whether this LLM supports function/tool calling.\n",
        "\n",
        "        We keep this `False` for the workshop to avoid wiring tool calling.\n",
        "        \"\"\"\n",
        "        return False\n",
        "\n",
        "    def get_context_window_size(self) -> int:\n",
        "        \"\"\"Approximate context window size in tokens.\n",
        "\n",
        "        You can adjust this based on the specific model you choose.\n",
        "        \"\"\"\n",
        "        return 8192\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4EyG3noeOgq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4EyG3noeOgq",
        "outputId": "1c46f93b-f8f8-4fe6-b78c-668e8e98fe55"
      },
      "outputs": [],
      "source": [
        "# 6) Smoke test: use the custom LLM directly\n",
        "\n",
        "watsonx_llm = WatsonxCrewAILLM(\n",
        "    model_id=WATSONX_MODEL_ID,\n",
        "    url=os.environ[\"WATSONX_URL\"],\n",
        "    project_id=os.environ[\"WATSONX_PROJECT_ID\"],\n",
        "    # api_key will be read from env if omitted\n",
        "    temperature=0.3,\n",
        "    max_tokens=256,\n",
        ")\n",
        "\n",
        "print(\"‚è≥ Calling watsonx via WatsonxCrewAILLM...\")\n",
        "reply = watsonx_llm.call(\"Say a one-sentence hello from watsonx.ai through CrewAI's BaseLLM.\")\n",
        "print(\"\\n=== Custom LLM reply ===\\n\")\n",
        "print(reply)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dn8HhEEzeOgq",
      "metadata": {
        "id": "dn8HhEEzeOgq"
      },
      "source": [
        "## 4. Building a small CrewAI workflow powered by watsonx.ai\n",
        "\n",
        "We‚Äôll build a simple three‚Äëagent crew:\n",
        "\n",
        "1. **Researcher** ‚Äì gathers structured notes about a topic  \n",
        "2. **Writer** ‚Äì turns the notes into a tutorial‚Äëstyle article  \n",
        "3. **Editor** ‚Äì polishes the article for a workshop audience  \n",
        "\n",
        "All three agents will share the same `WatsonxCrewAILLM` instance so they all use\n",
        "your watsonx.ai model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rQek3s2peOgq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQek3s2peOgq",
        "outputId": "730a50b5-9e85-430b-a1f4-401ed7d16926"
      },
      "outputs": [],
      "source": [
        "# 7) Choose a topic for the crew\n",
        "\n",
        "topic = \"Building multi-agent workflows with IBM watsonx.ai and CrewAI\"\n",
        "\n",
        "print(\"Current topic:\", topic)\n",
        "# Change the string above and re-run this cell to explore a different topic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DdiR9__ReOgr",
      "metadata": {
        "id": "DdiR9__ReOgr"
      },
      "outputs": [],
      "source": [
        "# 8) Define CrewAI agents that use the watsonx-backed LLM\n",
        "\n",
        "from crewai import Agent\n",
        "\n",
        "researcher = Agent(\n",
        "    role=\"AI Researcher\",\n",
        "    goal=\"Deeply research the given topic and produce clear, structured notes.\",\n",
        "    backstory=(\n",
        "        \"You are an expert AI research assistant. You excel at organizing complex \"\n",
        "        \"information into concise bullet points that are easy to reuse.\"\n",
        "    ),\n",
        "    llm=watsonx_llm,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "writer = Agent(\n",
        "    role=\"Technical Writer\",\n",
        "    goal=\"Turn research notes into an engaging, practical tutorial article.\",\n",
        "    backstory=(\n",
        "        \"You are a patient technical writer who explains advanced AI concepts in \"\n",
        "        \"a way that intermediate Python developers can understand.\"\n",
        "    ),\n",
        "    llm=watsonx_llm,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "editor = Agent(\n",
        "    role=\"Editor\",\n",
        "    goal=(\n",
        "        \"Improve clarity, structure, and tone of drafts so they are ready to be \"\n",
        "        \"shared in a live workshop.\"\n",
        "    ),\n",
        "    backstory=(\n",
        "        \"You are a meticulous editor who focuses on correctness, structure, and \"\n",
        "        \"beginner-friendly language.\"\n",
        "    ),\n",
        "    llm=watsonx_llm,\n",
        "    verbose=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BVEe_mHxeOgr",
      "metadata": {
        "id": "BVEe_mHxeOgr"
      },
      "outputs": [],
      "source": [
        "# 9) Define tasks and how they depend on each other\n",
        "\n",
        "from crewai import Task\n",
        "\n",
        "research_task = Task(\n",
        "    description=(\n",
        "        f\"Research the topic: '{topic}'.\\n\"\n",
        "        \"- Explain what IBM watsonx.ai is and its main capabilities.\\n\"\n",
        "        \"- Explain what CrewAI is and when you might use it.\\n\"\n",
        "        \"- List 8‚Äì12 concrete ideas for using watsonx.ai models inside multi-agent workflows.\\n\"\n",
        "        \"- Highlight any important security or cost considerations.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "        \"A markdown document with three sections: 'watsonx.ai overview', \"\n",
        "        \"'CrewAI overview', and 'Use-case ideas', each with bullet points.\"\n",
        "    ),\n",
        "    agent=researcher,\n",
        ")\n",
        "\n",
        "writing_task = Task(\n",
        "    description=(\n",
        "        \"Using the research notes, write a tutorial-style article about the topic.\\n\"\n",
        "        \"The article should cover:\\n\"\n",
        "        \"1. High-level overview.\\n\"\n",
        "        \"2. Architecture: where watsonx.ai fits vs CrewAI.\\n\"\n",
        "        \"3. Step-by-step guide to building a simple crew that calls a watsonx model.\\n\"\n",
        "        \"4. Practical tips and common pitfalls.\\n\"\n",
        "        \"Target length: 800‚Äì1200 words.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "        \"A markdown article with clear headings, short paragraphs, and at least \"\n",
        "        \"one numbered list of steps.\"\n",
        "    ),\n",
        "    agent=writer,\n",
        "    # Use the output of the research_task as context\n",
        "    context=[research_task],\n",
        ")\n",
        "\n",
        "editing_task = Task(\n",
        "    description=(\n",
        "        \"Take the draft article from the writer and polish it for a workshop\\n\"\n",
        "        \"audience. Focus on clarity, structure, and making it easy to follow in a\\n\"\n",
        "        \"live coding session.\\n\"\n",
        "        \"- Fix any obvious mistakes.\\n\"\n",
        "        \"- Make security best practices around API keys very explicit.\\n\"\n",
        "        \"- Ensure all code snippets are self-contained and well explained.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "        \"A polished markdown article ready to be shown in a training workshop.\"\n",
        "    ),\n",
        "    agent=editor,\n",
        "    context=[writing_task],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BZvTcDRLeOgr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BZvTcDRLeOgr",
        "outputId": "ad582c9c-dfc7-43b7-86b3-519e577b4540"
      },
      "outputs": [],
      "source": [
        "# 10) Create the crew and run the workflow\n",
        "\n",
        "from crewai import Crew, Process\n",
        "\n",
        "crew = Crew(\n",
        "    agents=[researcher, writer, editor],\n",
        "    tasks=[research_task, writing_task, editing_task],\n",
        "    process=Process.sequential,  # run tasks one after another\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"‚è≥ Kicking off crew... this may take a little while.\")\n",
        "result = crew.kickoff()\n",
        "\n",
        "# CrewAI typically returns an object with a `.raw` attribute, but we fall back\n",
        "# gracefully in case the API changes.\n",
        "final_text = getattr(result, \"raw\", str(result))\n",
        "\n",
        "print(\"\\n=== Final Workshop Article ===\\n\")\n",
        "print(final_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VO3Am_TZeOgr",
      "metadata": {
        "id": "VO3Am_TZeOgr"
      },
      "source": [
        "## 5. Experiment: change the topic\n",
        "\n",
        "To adapt this workflow to a different workshop or demo:\n",
        "\n",
        "1. Go back to the **\"Choose a topic\"** cell and change the `topic` string.  \n",
        "2. Re-run that cell.  \n",
        "3. Re-run the **agent**, **task**, and **crew** cells (8‚Äì10).  \n",
        "\n",
        "The same watsonx‚Äëpowered crew will now produce content for your new topic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r36S7AXKeOgr",
      "metadata": {
        "id": "r36S7AXKeOgr"
      },
      "source": [
        "## 6. Next steps and extensions\n",
        "\n",
        "Ideas for extending this notebook:\n",
        "\n",
        "- **Add tools**: connect CrewAI tools (web search, file I/O, internal APIs) and\n",
        "  wire them to watsonx by implementing function/tool calling in `WatsonxCrewAILLM`.\n",
        "- **RAG workflows**: combine watsonx.ai with a vector store so agents can search\n",
        "  over your own documentation or code.\n",
        "- **Hierarchical crews**: introduce a manager agent that plans and delegates work\n",
        "  instead of the simple sequential process used here.\n",
        "- **Logging and observability**: connect CrewAI's tracing to your preferred\n",
        "  monitoring stack to observe token usage and agent behaviour in production.\n",
        "\n",
        "You can use this notebook as a starting point for your own internal workshops\n",
        "or hands‚Äëon labs around IBM watsonx.ai + CrewAI.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
