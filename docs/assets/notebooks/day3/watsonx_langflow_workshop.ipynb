{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# IBM watsonx.ai + Langflow Workshop\n\nThis notebook walks you through:\n\n- Installing and configuring **Langflow** for use with **IBM watsonx.ai**\n- Setting up watsonx.ai credentials for use in Langflow\n- Building a **basic chat flow** in Langflow that uses the IBM watsonx.ai component\n- Calling your Langflow + watsonx flow from Python via the **Langflow API**\n\n> \u26a0\ufe0f This notebook does **not** replace the Langflow UI. You still build the flow\n> visually in Langflow, but this notebook helps with environment setup and API usage.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. Prerequisites\n\nTo run this workshop you need:\n\n- Python 3.10+\n- An IBM Cloud account with access to **watsonx.ai**\n- A watsonx.ai **service instance**, **project**, and **API key**\n- A running **Langflow** instance (Desktop, Docker, or `langflow run`)\n- Basic familiarity with Python, terminals, and web UIs\n\nUseful links (open in your browser):\n\n- Langflow docs: `https://docs.langflow.org/`\n- IBM bundle docs: `https://docs.langflow.org/bundles-ibm`\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Install Python dependencies\n\nYou only need to run this cell **once per environment**.\n\nIf your Jupyter environment supports IPython magics, you can keep `%pip`.\nOtherwise, remove the leading `%` and run as plain `pip`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Install dependencies (run once)\n# Uncomment the next line if your environment supports IPython magics:\n# %pip install -U langflow \"langflow[all]\" langchain-ibm ibm-watsonx-ai python-dotenv requests\n\nprint(\"If needed, install with: pip install -U langflow 'langflow[all]' langchain-ibm ibm-watsonx-ai python-dotenv requests\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Configure IBM watsonx.ai credentials\n\nYou\u2019ll need three pieces of information from IBM Cloud:\n\n- **IBM Cloud API key** for watsonx.ai\n- **Service URL** (depends on your region), for example:\n  - `https://us-south.ml.cloud.ibm.com` (Dallas)\n  - `https://eu-de.ml.cloud.ibm.com` (Frankfurt)\n  - `https://eu-gb.ml.cloud.ibm.com` (London)\n  - `https://jp-tok.ml.cloud.ibm.com` (Tokyo)\n  - `https://au-syd.ml.cloud.ibm.com` (Sydney)\n- **Project ID** for your watsonx.ai project\n\nThis cell will:\n\n- Prompt you for your API key, URL, and project ID\n- Store them in environment variables:\n  - `WATSONX_APIKEY` and `WATSONX_API_KEY` (both set for compatibility)\n  - `WATSONX_URL`\n  - `WATSONX_PROJECT_ID`\n- Enable the Langflow feature that lets global variables fall back to env vars:\n  - `LANGFLOW_FALLBACK_TO_ENV_VAR=True`\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\nfrom getpass import getpass\n\nprint(\"\ud83d\udc49 Enter your IBM watsonx.ai credentials (they stay only in this notebook session).\\n\")\n\n\nWATSONX_API_KEY = getpass(\"IBM Cloud API key for watsonx.ai: \")\nWATSONX_URL = input(\"watsonx.ai URL (e.g. https://us-south.ml.cloud.ibm.com): \").strip()\nWATSONX_PROJECT_ID = input(\"watsonx.ai Project ID: \").strip()\n\n# Store in environment variables (some libraries expect one name, some the other)\nos.environ[\"WATSONX_APIKEY\"] = WATSONX_API_KEY\nos.environ[\"WATSONX_API_KEY\"] = WATSONX_API_KEY\nos.environ[\"WATSONX_URL\"] = WATSONX_URL\nos.environ[\"WATSONX_PROJECT_ID\"] = WATSONX_PROJECT_ID\n\n# Optional but useful for Langflow global variables\nos.environ[\"LANGFLOW_FALLBACK_TO_ENV_VAR\"] = \"True\"\n\nprint(\"\\n\u2705 Environment variables set: WATSONX_APIKEY, WATSONX_API_KEY, WATSONX_URL, WATSONX_PROJECT_ID, LANGFLOW_FALLBACK_TO_ENV_VAR\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Quick sanity check: Chat with a watsonx.ai model\n\nBefore involving Langflow, make sure your watsonx credentials and network work\nby calling a watsonx.ai chat model directly using `langchain-ibm`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from langchain_ibm import ChatWatsonx\n\n# Pick any chat-capable model that is enabled in your region.\n# You can adjust this later.\nWATSONX_MODEL_ID = \"ibm/granite-3-8b-instruct\"\n\nparameters = {\n    \"temperature\": 0.2,\n    \"max_tokens\": 256,\n}\n\nchat = ChatWatsonx(\n    model_id=WATSONX_MODEL_ID,\n    url=os.environ[\"WATSONX_URL\"],\n    project_id=os.environ[\"WATSONX_PROJECT_ID\"],\n    params=parameters,\n)\n\nmessages = [\n    (\"system\", \"You are a concise assistant for IBM watsonx.ai + Langflow workshops.\"),\n    (\"human\", \"In 3 bullet points, explain what Langflow is.\"),\n]\n\nprint(\"\u23f3 Calling watsonx.ai...\\n\")\nresponse = chat.invoke(messages)\nprint(\"=== watsonx.ai response ===\\n\")\nprint(response.content)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Start Langflow\n\nYou can run Langflow in several ways:\n\n- **Langflow Desktop** (recommended for beginners)\n- **Docker**: `docker run -p 7860:7860 langflowai/langflow:latest`\n- **CLI** in a virtual environment: `langflow run`\n\nFor this workshop, we assume Langflow is running locally on:\n\n- `http://localhost:7860` (default for `langflow run`)\n- Or `http://localhost:7861` if you're using watsonx Orchestrate Developer Edition with `--with-langflow`\n\n> \ud83e\udde0 Tip: Start Langflow **after** setting your watsonx environment variables so\n> components and global variables can see them.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Example: start Langflow from a terminal (not directly from this notebook)\n# (Run these commands in a separate shell, not inside Python)\n\nexample_commands = '''\n# In your terminal (NOT in Python):\n\n# 1) Activate your virtual environment (if you created one)\n#    e.g. on macOS/Linux:\n#    source .venv/bin/activate\n#\n# 2) Make sure environment variables are set (API key, URL, project ID)\n#    export WATSONX_APIKEY=...\n#    export WATSONX_URL=...\n#    export WATSONX_PROJECT_ID=...\n#\n# 3) Start Langflow:\n#    langflow run\n#\n# 4) Open the UI in your browser:\n#    http://localhost:7860\n'''\n\nprint(example_commands)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Build a basic watsonx.ai chat flow in Langflow\n\nNow switch to the **Langflow UI** in your browser and build a simple flow that\nuses the IBM bundle.\n\n### 5.1 Create a new flow\n\n1. In the Langflow UI, click **New Flow**.\n2. Name it something like: `watsonx_basic_chat`.\n3. Optionally add a description: `Simple chat using IBM watsonx.ai`.\n\n### 5.2 Add components\n\nDrag the following components onto the canvas:\n\n1. **Chat Input** (category: *Input / Output*)\n2. **Prompt Template** (category: *Processing*)\n3. **IBM watsonx.ai** (category: *Bundles \u2192 IBM*)\n4. **Chat Output** (category: *Input / Output*)\n\nConnect them like this:\n\n`Chat Input \u2192 Prompt Template \u2192 IBM watsonx.ai \u2192 Chat Output`\n\n### 5.3 Configure the Prompt Template\n\nClick the **Prompt Template** component and set:\n\n- **Template** (example):\n\n```text\nYou are an AI assistant for an IBM watsonx.ai + Langflow workshop.\nAnswer clearly and keep responses under 8 sentences.\n\nUser question:\n{input}\n```\n\n- Make sure the **input variable** name (e.g. `input`) matches the incoming\n  field from the Chat Input component. The default is usually fine.\n\n### 5.4 Configure the IBM watsonx.ai component\n\nClick the **IBM watsonx.ai** component and set:\n\n- **url**: `{{WATSONX_URL}}` or your region URL directly\n- **project_id**: `{{WATSONX_PROJECT_ID}}` or your project ID directly\n- **api_key**: `{{WATSONX_APIKEY}}` (or another global variable name)\n\nIf you use `{{...}}` syntax, make sure you have matching **Global Variables**\ndefined in **Settings \u2192 Global Variables** in Langflow, or that\n`LANGFLOW_FALLBACK_TO_ENV_VAR=True` and the environment variables are set.\n\nFor **model_name**:\n\n- Once `url`, `project_id`, and `api_key` are valid, the component can\n  auto-populate available models from your watsonx.ai deployment.\n- Choose a chat-capable model, for example:\n  - `ibm/granite-3-8b-instruct`\n\nYou can adjust other parameters as needed, for example:\n\n- `temperature = 0.2`\n- `max_tokens = 512`\n- `top_p = 0.9`\n\n### 5.5 Test the flow in the Langflow UI\n\n1. Click the **Chat Input** component.\n2. Enter a message, such as: `Explain what this flow does.`\n3. Click **Run** or use the chat panel.\n4. You should see a response appear in **Chat Output**, generated by your\n   watsonx.ai model through Langflow.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Export your flow as JSON (optional)\n\nTo make this flow portable or use it with other tools (like watsonx Orchestrate):\n\n1. In the Langflow UI, open your `watsonx_basic_chat` flow.\n2. Click **Share \u2192 Export**.\n3. Choose **Export as JSON** (optionally check \u201cSave with my API keys\u201d if you\n   need the credentials embedded for local testing).\n4. Save the JSON file (for example, `watsonx_basic_chat.json`).\n\nYou can re-import this JSON into any Langflow instance to recreate the flow.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Call your Langflow + watsonx flow from Python\n\nEvery Langflow flow can be executed from code using the **Langflow API**.\n\nWe\u2019ll prepare a small helper in Python to call your flow via HTTP.\n\nYou need:\n\n- Your **Langflow server URL**, e.g. `http://localhost:7860`\n- A **Langflow API key** (see `Settings \u2192 Langflow API Keys` in the UI, or the\n  docs on API keys and authentication)\n- Your flow\u2019s **ID** (from `Share \u2192 API access` or from the flow URL)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import uuid\nimport requests\n\nprint(\"\ud83d\udc49 Configure Langflow API access.\\n\")\n\n\nLANGFLOW_SERVER_URL = input(\"Langflow server URL (e.g. http://localhost:7860): \").strip()\nFLOW_ID = input(\"Flow ID (from Langflow 'Share \u2192 API access'): \").strip()\nLANGFLOW_API_KEY = getpass(\"Langflow API key (Settings \u2192 Langflow API Keys): \")\n\n# Optional: store in environment variables for reuse\nos.environ[\"LANGFLOW_SERVER_URL\"] = LANGFLOW_SERVER_URL\nos.environ[\"LANGFLOW_FLOW_ID\"] = FLOW_ID\nos.environ[\"LANGFLOW_API_KEY\"] = LANGFLOW_API_KEY\n\nprint(\"\\n\u2705 Langflow API configuration saved in this session.\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def run_langflow_chat(message: str, session_id: str | None = None) -> dict:\n    \"\"\"Call the Langflow flow via /api/v1/run/{flow_id} and return the JSON response.\"\"\"\n    if session_id is None:\n        session_id = f\"chat-{uuid.uuid4()}\"\n\n    url = f\"{os.environ['LANGFLOW_SERVER_URL'].rstrip('/')}/api/v1/run/{os.environ['LANGFLOW_FLOW_ID']}\"\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"x-api-key\": os.environ[\"LANGFLOW_API_KEY\"],\n    }\n\n    payload = {\n        # basic chat-style request; Langflow examples use this pattern\n        \"input_value\": message,\n        \"session_id\": session_id,\n        \"input_type\": \"chat\",\n        \"output_type\": \"chat\",\n        \"output_component\": \"\",  # default first chat output\n        \"tweaks\": None,           # no runtime overrides yet\n    }\n\n    response = requests.post(url, headers=headers, json=payload, timeout=60)\n    response.raise_for_status()\n    return response.json()\n\n\nprint(\"Helper function `run_langflow_chat` is ready.\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Test: run your watsonx + Langflow flow once from Python\n\ntest_message = \"Give me 3 workshop ideas for using IBM watsonx.ai with Langflow.\"\nprint(\"\u23f3 Calling Langflow flow...\\n\")\nresult = run_langflow_chat(test_message)\n\nprint(\"=== Raw API response keys ===\")\nprint(result.keys())\n\n# Try to extract the first chat output text, if present\ntry:\n    outputs = result.get(\"outputs\", [])\n    if outputs:\n        # outputs[0][\"outputs\"][0][\"results\"][\"message\"][\"text\"]\n        first_output = outputs[0][\"outputs\"][0][\"results\"][\"message\"][\"text\"]\n        print(\"\\n=== First chat output ===\\n\")\n        print(first_output)\n    else:\n        print(\"No 'outputs' field found in response. Inspect `result` manually.\")\nexcept Exception as e:\n    print(\"Could not parse chat output from response. Inspect `result` directly.\")\n    print(\"Error:\", e)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Advanced: tweak IBM watsonx.ai parameters at runtime\n\nLangflow supports **tweaks**: one-time overrides of component parameters when you\ncall a flow via the API.\n\nTo use tweaks:\n\n1. In Langflow, find the **IBM watsonx.ai** component ID, something like:\n   - `IBMwatsonxModel-XXXXXX`\n   You can see this in the API access modal or in the flow JSON.\n2. Use that ID as a key in the `tweaks` dictionary.\n\nExample (Python): override the model and temperature for a single request.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def run_langflow_chat_with_tweaks(message: str, tweaks: dict, session_id: str | None = None) -> dict:\n    if session_id is None:\n        session_id = f\"chat-{uuid.uuid4()}\"\n\n    url = f\"{os.environ['LANGFLOW_SERVER_URL'].rstrip('/')}/api/v1/run/{os.environ['LANGFLOW_FLOW_ID']}\"\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"x-api-key\": os.environ[\"LANGFLOW_API_KEY\"],\n    }\n\n    payload = {\n        \"input_value\": message,\n        \"session_id\": session_id,\n        \"input_type\": \"chat\",\n        \"output_type\": \"chat\",\n        \"output_component\": \"\",\n        \"tweaks\": tweaks,\n    }\n\n    response = requests.post(url, headers=headers, json=payload, timeout=60)\n    response.raise_for_status()\n    return response.json()\n\n\n# EXAMPLE: replace \"IBMwatsonxModel-XXXX\" with your component ID from Langflow\nexample_tweaks = {\n    \"IBMwatsonxModel-XXXX\": {\n        \"model_name\": \"ibm/granite-3-2b-instruct\",  # example smaller model\n        \"temperature\": 0.1,\n    }\n}\n\nprint(\"Example tweaks dict defined as `example_tweaks`.\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Next steps and workshop ideas\n\nNow you have:\n\n- A running Langflow instance configured to talk to IBM watsonx.ai\n- A basic chat flow using the **IBM watsonx.ai** bundle component\n- A Python helper to call that flow through the **Langflow API**\n- An example of how to use **tweaks** to override model parameters at runtime\n\nIdeas to extend this workshop:\n\n- Build a **RAG flow** by adding IBM watsonx.ai Embeddings, a vector store, and\n  document loaders.\n- Turn your flow into a **tool** for watsonx Orchestrate (export JSON, then\n  import as a tool).\n- Add **agents** in Langflow (Agent components) and point them to your IBM\n  watsonx.ai Language Model.\n- Secure your Langflow server with authentication and a reverse proxy, following\n  the Langflow best practices.\n\nYou can reuse this notebook as a starting point for internal trainings or\nhackathons that focus on **IBM watsonx.ai + Langflow**.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}