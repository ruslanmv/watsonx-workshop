{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3.4 - Visual Agent Building with Langflow + watsonx.ai\n",
        "\n",
        "This notebook demonstrates how to build AI agents visually using Langflow with watsonx.ai integration.\n",
        "\n",
        "## What is Langflow?\n",
        "\n",
        "Langflow is a **visual UI for LangChain** that allows you to build complex agent workflows through a drag-and-drop interface. It's perfect for:\n",
        "\n",
        "- Rapid prototyping of agent workflows\n",
        "- Visual debugging of LLM pipelines  \n",
        "- Collaboration between technical and non-technical teams\n",
        "- Testing different agent architectures quickly\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- Installing and running Langflow\n",
        "- Integrating watsonx.ai with Langflow\n",
        "- Building visual agent workflows\n",
        "- Creating RAG pipelines visually\n",
        "- Exporting and deploying Langflow flows\n",
        "- Using Langflow programmatically\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Langflow UI (Browser)\n",
        "     |\n",
        "     v\n",
        "Visual Flow Builder\n",
        "  - Drag & Drop Components\n",
        "  - Connect Nodes\n",
        "  - Configure watsonx.ai\n",
        "     |\n",
        "     v\n",
        "LangChain Backend\n",
        "  - watsonx.ai LLM\n",
        "  - Custom Tools\n",
        "  - Agent Logic\n",
        "     |\n",
        "     v\n",
        "Execution & Results\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "### Google Colab Compatibility\n",
        "\n",
        "This notebook works in both Google Colab and local environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"‚úì Running in Google Colab\")\n",
        "    print(\"\\nNote: Langflow UI will be accessible through a public URL in Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚úì Running in local environment\")\n",
        "    print(\"\\nNote: Langflow UI will be accessible at http://localhost:7860\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Langflow and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Langflow and required packages\n",
        "!pip install -q langflow \"ibm-watsonx-ai>=1.1.22\" langchain-ibm requests\n",
        "\n",
        "print(\"‚úì Langflow and dependencies installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure watsonx.ai Credentials\n",
        "\n",
        "Set up your IBM watsonx.ai credentials for use in Langflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Configuration for watsonx.ai\n",
        "WATSONX_URL = os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\")\n",
        "\n",
        "if not os.getenv(\"WATSONX_APIKEY\"):\n",
        "    WATSONX_APIKEY = getpass(\"Enter your watsonx.ai API Key: \")\n",
        "    os.environ[\"WATSONX_APIKEY\"] = WATSONX_APIKEY\n",
        "else:\n",
        "    WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
        "\n",
        "if not os.getenv(\"WATSONX_PROJECT_ID\"):\n",
        "    WATSONX_PROJECT_ID = getpass(\"Enter your watsonx.ai Project ID: \")\n",
        "    os.environ[\"WATSONX_PROJECT_ID\"] = WATSONX_PROJECT_ID\n",
        "else:\n",
        "    WATSONX_PROJECT_ID = os.getenv(\"WATSONX_PROJECT_ID\")\n",
        "\n",
        "# Set environment variables for Langflow to use\n",
        "os.environ[\"WATSONX_URL\"] = WATSONX_URL\n",
        "\n",
        "# Model configuration\n",
        "LLM_MODEL_ID = os.getenv(\"LLM_MODEL_ID\", \"ibm/granite-3-8b-instruct\")\n",
        "\n",
        "print(\"‚úì Configuration loaded and environment variables set\")\n",
        "print(f\"  Model: {LLM_MODEL_ID}\")\n",
        "print(f\"  URL: {WATSONX_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Start Langflow Server\n",
        "\n",
        "We'll start the Langflow server in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import threading\n",
        "\n",
        "def start_langflow_background():\n",
        "    \"\"\"\n",
        "    Start Langflow server in the background.\n",
        "    \"\"\"\n",
        "    if IN_COLAB:\n",
        "        # In Colab, use pyngrok to create a public URL\n",
        "        !pip install -q pyngrok\n",
        "        from pyngrok import ngrok\n",
        "        \n",
        "        # Start Langflow\n",
        "        proc = subprocess.Popen(\n",
        "            [\"langflow\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE\n",
        "        )\n",
        "        \n",
        "        # Wait for server to start\n",
        "        time.sleep(10)\n",
        "        \n",
        "        # Create public URL\n",
        "        public_url = ngrok.connect(7860)\n",
        "        print(f\"\\nüåê Langflow is running at: {public_url}\")\n",
        "        print(\"\\nClick the link above to open Langflow in a new tab\")\n",
        "        \n",
        "        return proc, public_url\n",
        "    else:\n",
        "        # Local environment\n",
        "        proc = subprocess.Popen(\n",
        "            [\"langflow\", \"run\", \"--host\", \"127.0.0.1\", \"--port\", \"7860\"],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE\n",
        "        )\n",
        "        \n",
        "        time.sleep(5)\n",
        "        \n",
        "        print(\"\\nüåê Langflow is running at: http://localhost:7860\")\n",
        "        print(\"\\nOpen this URL in your browser to access Langflow\")\n",
        "        \n",
        "        return proc, \"http://localhost:7860\"\n",
        "\n",
        "# Start Langflow\n",
        "print(\"Starting Langflow server...\")\n",
        "print(\"This may take a moment...\\n\")\n",
        "\n",
        "langflow_proc, langflow_url = start_langflow_background()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"LANGFLOW SERVER STARTED\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Using Langflow with watsonx.ai\n",
        "\n",
        "### Step-by-Step Guide\n",
        "\n",
        "#### 1. Open Langflow UI\n",
        "\n",
        "- Click on the Langflow URL from the cell above\n",
        "- The Langflow interface will open in a new tab\n",
        "\n",
        "#### 2. Create a New Flow\n",
        "\n",
        "- Click \"New Project\" or \"+ New Flow\"\n",
        "- You'll see a blank canvas with a sidebar of components\n",
        "\n",
        "#### 3. Add watsonx.ai LLM Component\n",
        "\n",
        "Since watsonx.ai might not have a pre-built component, we'll use a custom approach:\n",
        "\n",
        "1. **Option A: Use LangChain LLM Component**\n",
        "   - Drag \"LLM\" from the sidebar\n",
        "   - Configure with custom code (see below)\n",
        "\n",
        "2. **Option B: Use Custom Component**\n",
        "   - Create a custom Python component\n",
        "   - Import watsonx.ai integration\n",
        "\n",
        "#### 4. Build Your Flow\n",
        "\n",
        "Connect components visually:\n",
        "- Input ‚Üí LLM ‚Üí Output (simple flow)\n",
        "- Input ‚Üí RAG Tool ‚Üí LLM ‚Üí Output (RAG flow)\n",
        "- Input ‚Üí Agent ‚Üí Tools ‚Üí LLM ‚Üí Output (agent flow)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Sample Flow Configurations\n",
        "\n",
        "Here are some pre-built flow configurations you can import into Langflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Flow 1: Simple Question Answering with watsonx.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Simple QA Flow Configuration\n",
        "simple_qa_flow = {\n",
        "    \"name\": \"Simple QA with watsonx.ai\",\n",
        "    \"description\": \"A basic question answering flow using watsonx.ai\",\n",
        "    \"nodes\": [\n",
        "        {\n",
        "            \"id\": \"input\",\n",
        "            \"type\": \"ChatInput\",\n",
        "            \"position\": {\"x\": 100, \"y\": 200}\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"llm\",\n",
        "            \"type\": \"CustomLLM\",\n",
        "            \"config\": {\n",
        "                \"model_id\": LLM_MODEL_ID,\n",
        "                \"api_key\": \"${WATSONX_APIKEY}\",\n",
        "                \"project_id\": \"${WATSONX_PROJECT_ID}\",\n",
        "                \"url\": WATSONX_URL\n",
        "            },\n",
        "            \"position\": {\"x\": 400, \"y\": 200}\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"output\",\n",
        "            \"type\": \"ChatOutput\",\n",
        "            \"position\": {\"x\": 700, \"y\": 200}\n",
        "        }\n",
        "    ],\n",
        "    \"edges\": [\n",
        "        {\"source\": \"input\", \"target\": \"llm\"},\n",
        "        {\"source\": \"llm\", \"target\": \"output\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save flow configuration\n",
        "with open(\"langflow_simple_qa.json\", \"w\") as f:\n",
        "    json.dump(simple_qa_flow, f, indent=2)\n",
        "\n",
        "print(\"‚úì Simple QA flow configuration saved to: langflow_simple_qa.json\")\n",
        "print(\"\\nYou can import this file in Langflow using: Import ‚Üí Upload JSON\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Flow 2: RAG Pipeline with watsonx.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG Flow Configuration\n",
        "rag_flow = {\n",
        "    \"name\": \"RAG Pipeline with watsonx.ai\",\n",
        "    \"description\": \"Retrieval-Augmented Generation using watsonx.ai\",\n",
        "    \"nodes\": [\n",
        "        {\n",
        "            \"id\": \"input\",\n",
        "            \"type\": \"ChatInput\",\n",
        "            \"position\": {\"x\": 100, \"y\": 200}\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"retriever\",\n",
        "            \"type\": \"APIRequest\",\n",
        "            \"config\": {\n",
        "                \"url\": \"${ACCELERATOR_API_URL}\",\n",
        "                \"method\": \"POST\"\n",
        "            },\n",
        "            \"position\": {\"x\": 300, \"y\": 200}\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"prompt\",\n",
        "            \"type\": \"PromptTemplate\",\n",
        "            \"config\": {\n",
        "                \"template\": \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "            },\n",
        "            \"position\": {\"x\": 500, \"y\": 200}\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"llm\",\n",
        "            \"type\": \"CustomLLM\",\n",
        "            \"config\": {\n",
        "                \"model_id\": LLM_MODEL_ID,\n",
        "                \"api_key\": \"${WATSONX_APIKEY}\",\n",
        "                \"project_id\": \"${WATSONX_PROJECT_ID}\"\n",
        "            },\n",
        "            \"position\": {\"x\": 700, \"y\": 200}\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"output\",\n",
        "            \"type\": \"ChatOutput\",\n",
        "            \"position\": {\"x\": 900, \"y\": 200}\n",
        "        }\n",
        "    ],\n",
        "    \"edges\": [\n",
        "        {\"source\": \"input\", \"target\": \"retriever\"},\n",
        "        {\"source\": \"retriever\", \"target\": \"prompt\", \"data\": \"context\"},\n",
        "        {\"source\": \"input\", \"target\": \"prompt\", \"data\": \"question\"},\n",
        "        {\"source\": \"prompt\", \"target\": \"llm\"},\n",
        "        {\"source\": \"llm\", \"target\": \"output\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save flow configuration\n",
        "with open(\"langflow_rag_pipeline.json\", \"w\") as f:\n",
        "    json.dump(rag_flow, f, indent=2)\n",
        "\n",
        "print(\"‚úì RAG pipeline flow configuration saved to: langflow_rag_pipeline.json\")\n",
        "print(\"\\nYou can import this file in Langflow using: Import ‚Üí Upload JSON\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Custom watsonx.ai Component for Langflow\n",
        "\n",
        "Create a custom component that can be used in Langflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom watsonx.ai component code\n",
        "watsonx_component_code = '''\n",
        "from langflow import CustomComponent\n",
        "from langchain_ibm import WatsonxLLM\n",
        "from langflow.field_typing import Text\n",
        "\n",
        "class WatsonxAIComponent(CustomComponent):\n",
        "    \"\"\"\n",
        "    Custom Langflow component for IBM watsonx.ai.\n",
        "    \"\"\"\n",
        "    \n",
        "    display_name = \"watsonx.ai LLM\"\n",
        "    description = \"IBM watsonx.ai foundation models\"\n",
        "    \n",
        "    def build_config(self):\n",
        "        return {\n",
        "            \"model_id\": {\n",
        "                \"display_name\": \"Model ID\",\n",
        "                \"info\": \"watsonx.ai model identifier\",\n",
        "                \"value\": \"ibm/granite-3-8b-instruct\"\n",
        "            },\n",
        "            \"api_key\": {\n",
        "                \"display_name\": \"API Key\",\n",
        "                \"info\": \"IBM Cloud API Key\",\n",
        "                \"password\": True\n",
        "            },\n",
        "            \"project_id\": {\n",
        "                \"display_name\": \"Project ID\",\n",
        "                \"info\": \"watsonx.ai Project ID\"\n",
        "            },\n",
        "            \"url\": {\n",
        "                \"display_name\": \"URL\",\n",
        "                \"info\": \"watsonx.ai endpoint URL\",\n",
        "                \"value\": \"https://us-south.ml.cloud.ibm.com\"\n",
        "            },\n",
        "            \"temperature\": {\n",
        "                \"display_name\": \"Temperature\",\n",
        "                \"info\": \"Sampling temperature (0-1)\",\n",
        "                \"value\": 0.3\n",
        "            },\n",
        "            \"max_new_tokens\": {\n",
        "                \"display_name\": \"Max New Tokens\",\n",
        "                \"info\": \"Maximum tokens to generate\",\n",
        "                \"value\": 1000\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def build(self, \n",
        "              model_id: str,\n",
        "              api_key: str,\n",
        "              project_id: str,\n",
        "              url: str,\n",
        "              temperature: float = 0.3,\n",
        "              max_new_tokens: int = 1000) -> WatsonxLLM:\n",
        "        \"\"\"\n",
        "        Build and return a watsonx.ai LLM instance.\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            \"decoding_method\": \"greedy\",\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"temperature\": temperature\n",
        "        }\n",
        "        \n",
        "        llm = WatsonxLLM(\n",
        "            model_id=model_id,\n",
        "            url=url,\n",
        "            apikey=api_key,\n",
        "            project_id=project_id,\n",
        "            params=params\n",
        "        )\n",
        "        \n",
        "        return llm\n",
        "'''\n",
        "\n",
        "# Save custom component\n",
        "with open(\"watsonx_component.py\", \"w\") as f:\n",
        "    f.write(watsonx_component_code)\n",
        "\n",
        "print(\"‚úì Custom watsonx.ai component saved to: watsonx_component.py\")\n",
        "print(\"\\nTo use this component in Langflow:\")\n",
        "print(\"1. Copy the file to Langflow's custom_components directory\")\n",
        "print(\"2. Restart Langflow\")\n",
        "print(\"3. The component will appear in the sidebar under 'Custom'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Programmatic Access to Langflow\n",
        "\n",
        "You can also interact with Langflow flows programmatically using the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def run_langflow_flow(flow_id: str, input_text: str, langflow_url: str = \"http://localhost:7860\"):\n",
        "    \"\"\"\n",
        "    Run a Langflow flow programmatically.\n",
        "    \n",
        "    Args:\n",
        "        flow_id: The ID of the flow to run\n",
        "        input_text: The input text for the flow\n",
        "        langflow_url: Base URL of the Langflow server\n",
        "    \n",
        "    Returns:\n",
        "        Response from the flow\n",
        "    \"\"\"\n",
        "    api_url = f\"{langflow_url}/api/v1/run/{flow_id}\"\n",
        "    \n",
        "    payload = {\n",
        "        \"inputs\": {\n",
        "            \"input\": input_text\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(api_url, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "\n",
        "# Example usage (replace with your actual flow ID)\n",
        "# result = run_langflow_flow(\n",
        "#     flow_id=\"your-flow-id-here\",\n",
        "#     input_text=\"What is watsonx.ai?\"\n",
        "# )\n",
        "# print(json.dumps(result, indent=2))\n",
        "\n",
        "print(\"‚úì Langflow API helper function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Example Flows to Build\n",
        "\n",
        "Here are some example flows you can build in Langflow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Flow 1: Simple Chatbot\n",
        "\n",
        "**Components:**\n",
        "1. Chat Input\n",
        "2. watsonx.ai LLM\n",
        "3. Chat Output\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "Chat Input ‚Üí watsonx.ai LLM ‚Üí Chat Output\n",
        "```\n",
        "\n",
        "**Use Case:** Basic conversational AI\n",
        "\n",
        "---\n",
        "\n",
        "### Flow 2: RAG System\n",
        "\n",
        "**Components:**\n",
        "1. Chat Input\n",
        "2. Document Loader / API Call (RAG service)\n",
        "3. Prompt Template\n",
        "4. watsonx.ai LLM\n",
        "5. Chat Output\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "Chat Input ‚Üí RAG Service ‚Üí Prompt Template ‚Üí watsonx.ai LLM ‚Üí Chat Output\n",
        "```\n",
        "\n",
        "**Use Case:** Question answering over documents\n",
        "\n",
        "---\n",
        "\n",
        "### Flow 3: Multi-Tool Agent\n",
        "\n",
        "**Components:**\n",
        "1. Chat Input\n",
        "2. Tool Router (custom)\n",
        "3. RAG Tool\n",
        "4. Calculator Tool\n",
        "5. watsonx.ai LLM\n",
        "6. Chat Output\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "Chat Input ‚Üí Router ‚Üí [RAG Tool / Calculator Tool] ‚Üí watsonx.ai LLM ‚Üí Chat Output\n",
        "```\n",
        "\n",
        "**Use Case:** Agent with multiple capabilities\n",
        "\n",
        "---\n",
        "\n",
        "### Flow 4: Conversational Memory\n",
        "\n",
        "**Components:**\n",
        "1. Chat Input\n",
        "2. Conversation Buffer Memory\n",
        "3. Prompt Template (with history)\n",
        "4. watsonx.ai LLM\n",
        "5. Chat Output\n",
        "\n",
        "**Flow:**\n",
        "```\n",
        "Chat Input ‚Üí Memory ‚Üí Prompt Template ‚Üí watsonx.ai LLM ‚Üí Chat Output\n",
        "                ‚Üë                                              |\n",
        "                +----------------------------------------------+\n",
        "```\n",
        "\n",
        "**Use Case:** Chatbot with conversation history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Best Practices for Langflow\n",
        "\n",
        "### Design Principles\n",
        "\n",
        "1. **Start Simple**: Begin with basic flows and add complexity gradually\n",
        "2. **Modular Components**: Create reusable components for common operations\n",
        "3. **Test Incrementally**: Test each node individually before connecting them\n",
        "4. **Use Logging**: Add logging nodes to debug flow execution\n",
        "5. **Version Control**: Export and save your flows regularly\n",
        "\n",
        "### Performance Tips\n",
        "\n",
        "1. **Caching**: Use caching for expensive operations (embeddings, API calls)\n",
        "2. **Batch Processing**: Process multiple inputs together when possible\n",
        "3. **Async Execution**: Use async components for I/O-bound operations\n",
        "4. **Resource Limits**: Set appropriate limits for token generation\n",
        "5. **Error Handling**: Add error handling nodes for robustness\n",
        "\n",
        "### Security Considerations\n",
        "\n",
        "1. **API Keys**: Store credentials as environment variables\n",
        "2. **Input Validation**: Validate user inputs before processing\n",
        "3. **Rate Limiting**: Implement rate limits for public endpoints\n",
        "4. **Access Control**: Restrict access to sensitive flows\n",
        "5. **Audit Logs**: Track flow executions for security monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Exporting and Deploying Flows\n",
        "\n",
        "### Export Options\n",
        "\n",
        "1. **JSON Export**\n",
        "   - Export flow as JSON for version control\n",
        "   - Share flows with team members\n",
        "   - Import into other Langflow instances\n",
        "\n",
        "2. **Python Code Export**\n",
        "   - Generate standalone Python code\n",
        "   - Integrate into existing applications\n",
        "   - Customize and extend functionality\n",
        "\n",
        "3. **API Endpoint**\n",
        "   - Deploy flow as REST API\n",
        "   - Access from any application\n",
        "   - Scale independently\n",
        "\n",
        "### Deployment Strategies\n",
        "\n",
        "1. **Local Development**: Run Langflow locally for testing\n",
        "2. **Cloud Deployment**: Deploy to cloud platforms (AWS, GCP, Azure)\n",
        "3. **Container Deployment**: Package as Docker container\n",
        "4. **Serverless**: Deploy as serverless function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Integrating with Other Tools\n",
        "\n",
        "Langflow can integrate with various tools and services:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example integrations\n",
        "\n",
        "integration_examples = {\n",
        "    \"Vector Databases\": [\n",
        "        \"Weaviate\",\n",
        "        \"Pinecone\",\n",
        "        \"Chroma\",\n",
        "        \"Milvus\"\n",
        "    ],\n",
        "    \"Document Loaders\": [\n",
        "        \"PDF Loader\",\n",
        "        \"Web Scraper\",\n",
        "        \"CSV Loader\",\n",
        "        \"API Connector\"\n",
        "    ],\n",
        "    \"LLM Providers\": [\n",
        "        \"watsonx.ai (IBM)\",\n",
        "        \"OpenAI\",\n",
        "        \"Hugging Face\",\n",
        "        \"Anthropic\"\n",
        "    ],\n",
        "    \"Tools\": [\n",
        "        \"Google Search\",\n",
        "        \"Wikipedia\",\n",
        "        \"Python REPL\",\n",
        "        \"Custom APIs\"\n",
        "    ],\n",
        "    \"Memory\": [\n",
        "        \"Conversation Buffer\",\n",
        "        \"Entity Memory\",\n",
        "        \"Vector Store Memory\",\n",
        "        \"Redis Cache\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"Available Integrations in Langflow:\\n\")\n",
        "for category, items in integration_examples.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    for item in items:\n",
        "        print(f\"  - {item}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Troubleshooting Common Issues\n",
        "\n",
        "### Issue 1: Langflow won't start\n",
        "\n",
        "**Solution:**\n",
        "```python\n",
        "# Check if port 7860 is already in use\n",
        "!lsof -ti:7860 | xargs kill -9  # Kill any process on port 7860\n",
        "# Then restart Langflow\n",
        "```\n",
        "\n",
        "### Issue 2: watsonx.ai authentication fails\n",
        "\n",
        "**Solution:**\n",
        "- Verify API key and Project ID are correct\n",
        "- Check that environment variables are set\n",
        "- Ensure API key has proper permissions\n",
        "\n",
        "### Issue 3: Flow execution is slow\n",
        "\n",
        "**Solutions:**\n",
        "- Reduce max_new_tokens parameter\n",
        "- Use caching for repeated operations\n",
        "- Optimize prompt templates\n",
        "- Use faster models for non-critical components\n",
        "\n",
        "### Issue 4: Custom components not appearing\n",
        "\n",
        "**Solution:**\n",
        "- Restart Langflow after adding custom components\n",
        "- Check component code for syntax errors\n",
        "- Verify component is in correct directory\n",
        "- Check Langflow logs for error messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Summary and Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **Visual Development**: Build AI agents using drag-and-drop interface\n",
        "2. **watsonx.ai Integration**: Connect IBM Granite models to Langflow\n",
        "3. **Custom Components**: Create reusable components for specific needs\n",
        "4. **Flow Export**: Export flows as JSON or Python code\n",
        "5. **API Access**: Interact with flows programmatically\n",
        "\n",
        "### Key Benefits of Langflow\n",
        "\n",
        "| Feature | Benefit |\n",
        "|---------|----------|\n",
        "| Visual Interface | Faster prototyping and development |\n",
        "| No-Code/Low-Code | Accessible to non-programmers |\n",
        "| Real-time Testing | Immediate feedback on changes |\n",
        "| Component Reuse | Build faster with pre-made components |\n",
        "| Easy Collaboration | Share flows with team visually |\n",
        "| Export Options | Deploy to production easily |\n",
        "\n",
        "### When to Use Langflow\n",
        "\n",
        "‚úÖ **Good for:**\n",
        "- Rapid prototyping\n",
        "- Testing different agent architectures\n",
        "- Teaching and learning agent concepts\n",
        "- Building simple to moderate complexity flows\n",
        "- Collaborating with non-technical team members\n",
        "\n",
        "‚ùå **Not ideal for:**\n",
        "- Very complex custom logic\n",
        "- Performance-critical applications\n",
        "- Highly specialized workflows\n",
        "- Fine-grained control over every detail\n",
        "\n",
        "### Comparison with Other Approaches\n",
        "\n",
        "| Feature | Langflow | Code-First (LangChain) | CrewAI | LangGraph |\n",
        "|---------|----------|------------------------|--------|----------|\n",
        "| Visual Interface | ‚úÖ Yes | ‚ùå No | ‚ùå No | ‚ö†Ô∏è Limited |\n",
        "| Learning Curve | Low | High | Medium | Medium |\n",
        "| Flexibility | Medium | Very High | Medium | High |\n",
        "| Prototyping Speed | Very Fast | Slow | Fast | Medium |\n",
        "| Production Ready | Yes | Yes | Yes | Yes |\n",
        "| Debugging | Easy | Hard | Medium | Medium |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Experiment**: Build the example flows in Langflow\n",
        "2. **Customize**: Create your own custom components\n",
        "3. **Integrate**: Connect with your RAG service or APIs\n",
        "4. **Deploy**: Export and deploy a production flow\n",
        "5. **Learn More**: Explore Langflow documentation and community\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "- [Langflow Documentation](https://docs.langflow.org/)\n",
        "- [Langflow GitHub](https://github.com/logspace-ai/langflow)\n",
        "- [watsonx.ai Documentation](https://www.ibm.com/docs/en/watsonx-as-a-service)\n",
        "- [LangChain Documentation](https://python.langchain.com/)\n",
        "\n",
        "---\n",
        "\n",
        "**Course**: Multi-Agent Systems with watsonx.ai  \n",
        "**Lab**: 3.4 - Langflow Visual Agent Builder  \n",
        "**Platform**: Compatible with Google Colab and local environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Cleanup\n",
        "\n",
        "When you're done, stop the Langflow server:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Langflow server\n",
        "try:\n",
        "    langflow_proc.terminate()\n",
        "    langflow_proc.wait(timeout=5)\n",
        "    print(\"‚úì Langflow server stopped successfully\")\n",
        "except:\n",
        "    try:\n",
        "        langflow_proc.kill()\n",
        "        print(\"‚úì Langflow server force stopped\")\n",
        "    except:\n",
        "        print(\"‚ö† Could not stop Langflow server automatically\")\n",
        "        print(\"You may need to stop it manually or restart your kernel\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
