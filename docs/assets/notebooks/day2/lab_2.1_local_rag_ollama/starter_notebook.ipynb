{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1: Local RAG with Ollama - Starter Notebook\n",
    "\n",
    "**Duration**: 60 minutes | **Difficulty**: Intermediate\n",
    "\n",
    "## Objectives\n",
    "- Build a complete RAG pipeline using local Ollama models\n",
    "- Implement document loading, chunking, and embedding\n",
    "- Create a vector store with Chroma\n",
    "- Build a query-response pipeline with source attribution\n",
    "\n",
    "## Prerequisites\n",
    "- Ollama installed and running\n",
    "- Python packages: langchain, chromadb, sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "\n",
    "# TODO: Configure your RAG system parameters\n",
    "CONFIG = {\n",
    "    \"corpus_path\": \"data/corpus\",  # Path to your documents\n",
    "    \"chunk_size\": 1000,            # Size of text chunks\n",
    "    \"chunk_overlap\": 200,          # Overlap between chunks\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",  # HuggingFace model\n",
    "    \"llm_model\": \"llama2\",         # Ollama model\n",
    "    \"vector_db_path\": \"./chroma_db\",  # Path to store vector DB\n",
    "    \"top_k\": 5                     # Number of chunks to retrieve\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"Corpus path: {CONFIG['corpus_path']}\")\n",
    "print(f\"Chunk size: {CONFIG['chunk_size']}\")\n",
    "print(f\"Top-k: {CONFIG['top_k']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Document Loading\n",
    "\n",
    "**Task**: Load documents from your corpus directory.\n",
    "\n",
    "**Hints**: \n",
    "- Use `DirectoryLoader` to load all text files\n",
    "- You can use glob patterns like `**/*.txt` to match files recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement document loading\n",
    "# loader = DirectoryLoader(...)\n",
    "# documents = loader.load()\n",
    "\n",
    "# SOLUTION TEMPLATE:\n",
    "# loader = DirectoryLoader(\n",
    "#     CONFIG[\"corpus_path\"],\n",
    "#     glob=\"**/*.txt\"\n",
    "# )\n",
    "# documents = loader.load()\n",
    "\n",
    "# For now, create sample documents if corpus doesn't exist\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Sample documents about RAG\n",
    "documents = [\n",
    "    Document(page_content=\"\"\"Retrieval Augmented Generation (RAG) is a technique that combines \n",
    "    information retrieval with text generation. It allows language models to access external \n",
    "    knowledge bases, reducing hallucinations and improving factual accuracy.\"\"\", \n",
    "    metadata={\"source\": \"rag_intro.txt\"}),\n",
    "    \n",
    "    Document(page_content=\"\"\"Vector databases store embeddings and enable semantic search. \n",
    "    Popular options include Chroma, FAISS, and Elasticsearch. They use similarity metrics \n",
    "    like cosine similarity to find relevant documents.\"\"\", \n",
    "    metadata={\"source\": \"vector_db.txt\"}),\n",
    "    \n",
    "    Document(page_content=\"\"\"Chunking strategies affect RAG performance. Common approaches include \n",
    "    fixed-size chunking, semantic chunking, and recursive splitting. Overlap between chunks \n",
    "    helps maintain context.\"\"\", \n",
    "    metadata={\"source\": \"chunking.txt\"})\n",
    "]\n",
    "\n",
    "print(f\"✅ Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text Chunking\n",
    "\n",
    "**Task**: Split documents into smaller chunks for better retrieval.\n",
    "\n",
    "**Hints**:\n",
    "- Use `RecursiveCharacterTextSplitter`\n",
    "- Try different separators: `[\"\\n\\n\", \"\\n\", \". \", \" \"]`\n",
    "- Experiment with chunk_size and chunk_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement text chunking\n",
    "# splitter = RecursiveCharacterTextSplitter(...)\n",
    "# chunks = splitter.split_documents(documents)\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CONFIG[\"chunk_size\"],\n",
    "    chunk_overlap=CONFIG[\"chunk_overlap\"],\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"✅ Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"\\nFirst chunk preview:\")\n",
    "print(chunks[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Embeddings\n",
    "\n",
    "**Task**: Initialize the embedding model.\n",
    "\n",
    "**Hints**:\n",
    "- Use `HuggingFaceEmbeddings`\n",
    "- Set `model_kwargs={'device': 'cpu'}` for CPU\n",
    "- Enable `normalize_embeddings=True` for better similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize embedding model\n",
    "# embeddings = HuggingFaceEmbeddings(...)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=CONFIG[\"embedding_model\"],\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"✅ Embedding model initialized\")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embeddings.embed_query(\"What is RAG?\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Vector Store\n",
    "\n",
    "**Task**: Create a Chroma vector database from your chunks.\n",
    "\n",
    "**Hints**:\n",
    "- Use `Chroma.from_documents()`\n",
    "- Provide documents, embeddings, and persist_directory\n",
    "- Call `.persist()` to save the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create vector store\n",
    "# vectorstore = Chroma.from_documents(...)\n",
    "# vectorstore.persist()\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=CONFIG[\"vector_db_path\"]\n",
    ")\n",
    "\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\"✅ Vector store created and persisted\")\n",
    "print(f\"Location: {CONFIG['vector_db_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Retrieval\n",
    "\n",
    "**Task**: Test that retrieval works correctly.\n",
    "\n",
    "**Hints**:\n",
    "- Use `vectorstore.similarity_search(query, k=top_k)`\n",
    "- Inspect the returned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test retrieval\n",
    "# test_query = \"What is RAG?\"\n",
    "# results = vectorstore.similarity_search(...)\n",
    "\n",
    "test_query = \"What is RAG?\"\n",
    "results = vectorstore.similarity_search(test_query, k=CONFIG[\"top_k\"])\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nFound {len(results)} relevant chunks:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Source: {doc.metadata.get('source', 'unknown')}\")\n",
    "    print(doc.page_content[:150] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Initialize LLM\n",
    "\n",
    "**Task**: Initialize the Ollama LLM for generation.\n",
    "\n",
    "**Hints**:\n",
    "- Use `Ollama(model=...)`\n",
    "- Make sure Ollama is running (`ollama serve`)\n",
    "- Make sure the model is pulled (`ollama pull llama2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize LLM\n",
    "# llm = Ollama(...)\n",
    "\n",
    "llm = Ollama(model=CONFIG[\"llm_model\"])\n",
    "\n",
    "print(\"✅ LLM initialized\")\n",
    "\n",
    "# Test LLM\n",
    "test_response = llm.invoke(\"Say hello!\")\n",
    "print(f\"\\nTest response: {test_response[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build RAG Chain\n",
    "\n",
    "**Task**: Connect the retriever and LLM into a complete RAG pipeline.\n",
    "\n",
    "**Hints**:\n",
    "- Use `RetrievalQA.from_chain_type()`\n",
    "- Set `chain_type=\"stuff\"` to stuff all retrieved docs into the prompt\n",
    "- Set `return_source_documents=True` for citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build RAG chain\n",
    "# qa_chain = RetrievalQA.from_chain_type(...)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": CONFIG[\"top_k\"]}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"✅ RAG chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Query the RAG System\n",
    "\n",
    "**Task**: Ask questions and get answers with citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    \"\"\"Ask a question and display answer with sources.\"\"\"\n",
    "    result = qa_chain({\"query\": question})\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"\\nAnswer: {result['result']}\")\n",
    "    print(f\"\\nSources:\")\n",
    "    for doc in result['source_documents']:\n",
    "        print(f\"  - {doc.metadata.get('source', 'unknown')}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test queries\n",
    "ask_question(\"What is RAG?\")\n",
    "ask_question(\"What are vector databases?\")\n",
    "ask_question(\"What is chunking?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Try modifying parameters and see how results change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different parameters\n",
    "# - Try different chunk sizes\n",
    "# - Try different top_k values\n",
    "# - Try different embedding models\n",
    "# - Add your own documents to the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- ✅ Built a complete local RAG pipeline\n",
    "- ✅ Implemented document loading and chunking\n",
    "- ✅ Created embeddings and vector store\n",
    "- ✅ Connected retrieval and generation\n",
    "- ✅ Added source attribution\n",
    "\n",
    "**Next**: Lab 2.2 - Build the same system with watsonx.ai for enterprise deployment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
