{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4: RAG Evaluation Harness - Automated Testing ðŸ§ª\n\n",
    "**Duration**: 60 minutes | **Difficulty**: Intermediate-Advanced\n\n",
    "## ðŸŽ¯ What You'll Build\n\n",
    "A production-grade evaluation system to:\n",
    "- âœ… Test RAG systems automatically\n",
    "- âœ… Measure retrieval accuracy\n",
    "- âœ… Measure answer quality\n",
    "- âœ… Track performance over time\n\n",
    "**Industry value**: This is how companies ensure RAG quality in production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "print('âœ… Evaluation harness ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Ground Truth Dataset\n\n",
    "Define test cases with expected answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        'question': 'What is RAG?',\n",
    "        'expected_answer': 'RAG combines retrieval with generation to reduce hallucinations',\n",
    "        'relevant_docs': ['rag.txt'],\n",
    "        'category': 'definition'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Explain vector databases',\n",
    "        'expected_answer': 'Vector databases store embeddings for semantic search',\n",
    "        'relevant_docs': ['vectors.txt'],\n",
    "        'category': 'technical'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f'âœ… Created {len(test_cases)} test cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_retrieval_precision(retrieved: List[str], relevant: List[str]) -> float:\n",
    "    '''Calculate precision: What % of retrieved docs are relevant?'''\n",
    "    if not retrieved:\n",
    "        return 0.0\n",
    "    correct = len(set(retrieved) & set(relevant))\n",
    "    return correct / len(retrieved)\n",
    "\n",
    "def calculate_retrieval_recall(retrieved: List[str], relevant: List[str]) -> float:\n",
    "    '''Calculate recall: What % of relevant docs were retrieved?'''\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    correct = len(set(retrieved) & set(relevant))\n",
    "    return correct / len(relevant)\n",
    "\n",
    "def calculate_answer_quality(generated: str, expected: str) -> Dict[str, float]:\n",
    "    '''Calculate ROUGE scores for answer quality.'''\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(expected, generated)\n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "print('âœ… Metrics defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate your RAG system\n",
    "def evaluate_rag_system(test_cases: List[Dict]) -> pd.DataFrame:\n",
    "    '''Run all test cases and collect metrics.'''\n",
    "    results = []\n",
    "    \n",
    "    for test in test_cases:\n",
    "        # TODO: Run your RAG system\n",
    "        # generated_answer = your_rag_system(test['question'])\n",
    "        # retrieved_docs = your_retriever(test['question'])\n",
    "        \n",
    "        # For now, mock results\n",
    "        precision = calculate_retrieval_precision(['rag.txt'], test['relevant_docs'])\n",
    "        recall = calculate_retrieval_recall(['rag.txt'], test['relevant_docs'])\n",
    "        \n",
    "        results.append({\n",
    "            'question': test['question'],\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'category': test['category']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_rag_system(test_cases)\n",
    "print('âœ… Evaluation complete!')\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics = eval_results[['precision', 'recall']].mean()\n",
    "metrics.plot(kind='bar', ax=ax)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('RAG System Performance')\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_results.png')\n",
    "print('âœ… Results visualized!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Production Best Practices\n\n",
    "**What you've learned:**\n",
    "- âœ… Systematic RAG evaluation\n",
    "- âœ… Standard metrics (Precision, Recall, ROUGE)\n",
    "- âœ… Automated testing framework\n\n",
    "**Use this in production to:**\n",
    "- Monitor RAG quality over time\n",
    "- Compare different configurations\n",
    "- Catch regressions before deployment\n",
    "\n",
    "**Next**: Apply these techniques to your own RAG systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
