{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Elasticsearch and watsonx.ai\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to build a production-grade RAG system using:\n",
    "- **Elasticsearch** as the vector database\n",
    "- **watsonx.ai** for embeddings and LLM generation\n",
    "- **IBM Granite** models for enterprise AI\n",
    "\n",
    "## Prerequisites\n",
    "- Elasticsearch instance running\n",
    "- watsonx.ai API credentials\n",
    "- Python packages: elasticsearch, ibm-watsonx-ai, langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.embeddings import Embeddings\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"elasticsearch_url\": os.getenv(\"ELASTICSEARCH_URL\", \"http://localhost:9200\"),\n",
    "    \"elasticsearch_user\": os.getenv(\"ELASTICSEARCH_USER\"),\n",
    "    \"elasticsearch_password\": os.getenv(\"ELASTICSEARCH_PASSWORD\"),\n",
    "    \"watsonx_url\": os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\"),\n",
    "    \"watsonx_apikey\": os.getenv(\"WATSONX_APIKEY\"),\n",
    "    \"watsonx_project_id\": os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    \"index_name\": \"rag-demo\",\n",
    "    \"embedding_model\": \"ibm/slate-30m-english-rtrvr\",\n",
    "    \"llm_model\": \"ibm/granite-13b-chat-v2\",\n",
    "    \"chunk_size\": 1000,\n",
    "    \"chunk_overlap\": 200,\n",
    "    \"top_k\": 5\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Elasticsearch\n",
    "if CONFIG[\"elasticsearch_user\"] and CONFIG[\"elasticsearch_password\"]:\n",
    "    es = Elasticsearch(\n",
    "        [CONFIG[\"elasticsearch_url\"]],\n",
    "        basic_auth=(CONFIG[\"elasticsearch_user\"], CONFIG[\"elasticsearch_password\"])\n",
    "    )\n",
    "else:\n",
    "    es = Elasticsearch([CONFIG[\"elasticsearch_url\"]])\n",
    "\n",
    "# Test connection\n",
    "if es.ping():\n",
    "    print(\"✅ Connected to Elasticsearch\")\n",
    "    print(f\"Cluster info: {es.info()['version']['number']}\")\n",
    "else:\n",
    "    print(\"❌ Failed to connect to Elasticsearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize watsonx.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup credentials\n",
    "credentials = Credentials(\n",
    "    url=CONFIG[\"watsonx_url\"],\n",
    "    api_key=CONFIG[\"watsonx_apikey\"]\n",
    ")\n",
    "\n",
    "project_id = CONFIG[\"watsonx_project_id\"]\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = Embeddings(\n",
    "    model_id=CONFIG[\"embedding_model\"],\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "llm_params = {\n",
    "    GenParams.MAX_NEW_TOKENS: 300,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 50\n",
    "}\n",
    "\n",
    "llm = ModelInference(\n",
    "    model_id=CONFIG[\"llm_model\"],\n",
    "    credentials=credentials,\n",
    "    project_id=project_id,\n",
    "    params=llm_params\n",
    ")\n",
    "\n",
    "print(\"✅ watsonx.ai initialized\")\n",
    "print(f\"Embedding model: {CONFIG['embedding_model']}\")\n",
    "print(f\"LLM: {CONFIG['llm_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"IBM watsonx.ai is an enterprise AI platform that provides access to \n",
    "        foundation models including IBM Granite. It offers tools for prompt engineering, \n",
    "        model tuning, and deployment.\"\"\",\n",
    "        metadata={\"source\": \"watsonx.txt\", \"category\": \"platform\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Elasticsearch is a distributed search and analytics engine built on \n",
    "        Apache Lucene. It supports vector search capabilities for semantic retrieval.\"\"\",\n",
    "        metadata={\"source\": \"elasticsearch.txt\", \"category\": \"database\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"RAG (Retrieval Augmented Generation) enhances LLM outputs by retrieving \n",
    "        relevant context from external knowledge bases before generation.\"\"\",\n",
    "        metadata={\"source\": \"rag.txt\", \"category\": \"technique\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Chunk documents\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CONFIG[\"chunk_size\"],\n",
    "    chunk_overlap=CONFIG[\"chunk_overlap\"]\n",
    ")\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"✅ Prepared {len(chunks)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Elasticsearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define index mapping with vector field\n",
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,  # Dimension for slate-30m model\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"metadata\": {\"type\": \"object\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create or recreate index\n",
    "if es.indices.exists(index=CONFIG[\"index_name\"]):\n",
    "    es.indices.delete(index=CONFIG[\"index_name\"])\n",
    "    print(f\"Deleted existing index: {CONFIG['index_name']}\")\n",
    "\n",
    "es.indices.create(index=CONFIG[\"index_name\"], body=index_mapping)\n",
    "print(f\"✅ Created index: {CONFIG['index_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Index Documents with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings and index documents\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Generate embedding using watsonx\n",
    "    embedding = embedding_model.embed_query(chunk.page_content)\n",
    "    \n",
    "    # Index document\n",
    "    doc = {\n",
    "        \"text\": chunk.page_content,\n",
    "        \"embedding\": embedding,\n",
    "        \"metadata\": chunk.metadata\n",
    "    }\n",
    "    \n",
    "    es.index(index=CONFIG[\"index_name\"], id=i, document=doc)\n",
    "    print(f\"Indexed chunk {i+1}/{len(chunks)}\")\n",
    "\n",
    "# Refresh index\n",
    "es.indices.refresh(index=CONFIG[\"index_name\"])\n",
    "print(f\"\\n✅ Indexed {len(chunks)} documents with embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Implement Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query: str, top_k: int = 5):\n",
    "    \"\"\"Retrieve relevant documents using vector search.\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    \n",
    "    # Vector search\n",
    "    search_query = {\n",
    "        \"knn\": {\n",
    "            \"field\": \"embedding\",\n",
    "            \"query_vector\": query_embedding,\n",
    "            \"k\": top_k,\n",
    "            \"num_candidates\": 100\n",
    "        },\n",
    "        \"_source\": [\"text\", \"metadata\"]\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=CONFIG[\"index_name\"], body=search_query)\n",
    "    \n",
    "    # Extract results\n",
    "    documents = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        documents.append({\n",
    "            \"text\": hit['_source']['text'],\n",
    "            \"metadata\": hit['_source']['metadata'],\n",
    "            \"score\": hit['_score']\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is watsonx.ai?\"\n",
    "results = retrieve_documents(test_query, top_k=3)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"[{i+1}] Score: {doc['score']:.4f}\")\n",
    "    print(f\"Text: {doc['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Implement RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, top_k: int = 5):\n",
    "    \"\"\"Answer a question using RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    documents = retrieve_documents(question, top_k)\n",
    "    \n",
    "    # Construct context\n",
    "    context = \"\\n\\n\".join([doc['text'] for doc in documents])\n",
    "    \n",
    "    # Create prompt using Granite format\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful AI assistant. Use the provided context to answer questions accurately.\n",
    "If the context doesn't contain the answer, say so.\n",
    "<|endofsystem|>\n",
    "\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "<|endofuser|>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = llm.generate_text(prompt=prompt)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer.strip(),\n",
    "        \"sources\": [doc['metadata'] for doc in documents],\n",
    "        \"num_sources\": len(documents)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "questions = [\n",
    "    \"What is watsonx.ai?\",\n",
    "    \"What is Elasticsearch used for?\",\n",
    "    \"How does RAG work?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = answer_question(question, top_k=CONFIG[\"top_k\"])\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"\\nAnswer: {result['answer']}\")\n",
    "    print(f\"\\nSources ({result['num_sources']}):\")\n",
    "    for source in result['sources']:\n",
    "        print(f\"  - {source.get('source', 'unknown')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- ✅ Enterprise-grade RAG with Elasticsearch and watsonx.ai\n",
    "- ✅ Vector search with cosine similarity\n",
    "- ✅ IBM Granite models for generation\n",
    "- ✅ Scalable architecture for production deployment\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Add more documents to the index\n",
    "2. Implement hybrid search (keyword + semantic)\n",
    "3. Add re-ranking for better results\n",
    "4. Integrate with production monitoring\n",
    "5. Add evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
