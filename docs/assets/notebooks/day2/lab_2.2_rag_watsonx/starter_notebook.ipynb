{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2: Enterprise RAG with watsonx.ai - Starter Notebook üöÄ\n",
    "\n",
    "**Duration**: 60 minutes | **Difficulty**: Intermediate | **Type**: Hands-on Lab\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "Welcome to Lab 2.2! In this hands-on lab, you'll build a **production-grade RAG (Retrieval-Augmented Generation) system** using IBM's enterprise AI platform, watsonx.ai.\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- ‚úÖ Understand how to authenticate with watsonx.ai securely\n",
    "- ‚úÖ Use IBM Granite models for enterprise text generation\n",
    "- ‚úÖ Create embeddings using watsonx's slate models\n",
    "- ‚úÖ Build a complete RAG pipeline for production use\n",
    "- ‚úÖ Compare enterprise RAG vs local RAG (from Lab 2.1)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Background: Why watsonx.ai?\n",
    "\n",
    "### Local RAG (Lab 2.1) vs Enterprise RAG (Lab 2.2)\n",
    "\n",
    "In Lab 2.1, you built a RAG system with:\n",
    "- **Ollama**: Local, open-source LLM\n",
    "- **Chroma**: Lightweight vector database\n",
    "- **HuggingFace**: Free embedding models\n",
    "\n",
    "**Great for**: Development, prototyping, learning, offline use\n",
    "\n",
    "Now in Lab 2.2, you'll use:\n",
    "- **watsonx.ai**: Enterprise-grade LLM platform\n",
    "- **IBM Granite**: Production-optimized foundation models\n",
    "- **watsonx Embeddings**: High-quality, retrieval-focused embeddings\n",
    "\n",
    "**Great for**: Production deployments, enterprise security, governance, scale\n",
    "\n",
    "### Real-World Use Case\n",
    "\n",
    "Imagine you're building a customer support chatbot for a large company:\n",
    "- **Needs**: 99.9% uptime, security compliance, audit logs, scalability\n",
    "- **Solution**: watsonx.ai provides all these enterprise features\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Prerequisites\n",
    "\n",
    "Before starting, make sure you have:\n",
    "\n",
    "### 1. IBM Cloud Account\n",
    "- Sign up at: https://cloud.ibm.com\n",
    "- Free tier available for learning\n",
    "\n",
    "### 2. watsonx.ai Access\n",
    "- Create a watsonx.ai instance in IBM Cloud\n",
    "- Get your API key from IAM (Identity and Access Management)\n",
    "- Create or select a project and note the Project ID\n",
    "\n",
    "### 3. Environment Setup\n",
    "```bash\n",
    "pip install ibm-watsonx-ai langchain chromadb python-dotenv\n",
    "```\n",
    "\n",
    "### 4. Credentials File\n",
    "Create a `.env` file with:\n",
    "```\n",
    "WATSONX_URL=https://us-south.ml.cloud.ibm.com\n",
    "WATSONX_APIKEY=your_api_key_here\n",
    "WATSONX_PROJECT_ID=your_project_id_here\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîê Security Best Practice #1: Never Hardcode Credentials!\n",
    "\n",
    "**‚ùå NEVER DO THIS:**\n",
    "```python\n",
    "api_key = \"pak-abc123...\"  # BAD! This can leak in version control\n",
    "```\n",
    "\n",
    "**‚úÖ ALWAYS DO THIS:**\n",
    "```python\n",
    "api_key = os.getenv(\"WATSONX_APIKEY\")  # GOOD! Credentials from environment\n",
    "```\n",
    "\n",
    "**Why?** If you commit hardcoded credentials to git, they can be stolen and used to access your resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Setup and Imports üì¶\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# IBM watsonx.ai SDK\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.embeddings import Embeddings\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "# LangChain for RAG orchestration\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings as LangChainEmbeddings\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"\\nüìñ Quick tip: If you see import errors, run: pip install ibm-watsonx-ai langchain chromadb python-dotenv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Understanding the Imports\n",
    "\n",
    "Let's break down what each import does:\n",
    "\n",
    "1. **`ibm_watsonx_ai`**: IBM's official Python SDK for watsonx.ai\n",
    "   - `Credentials`: Handles authentication securely\n",
    "   - `ModelInference`: Interface to call foundation models (like Granite)\n",
    "   - `Embeddings`: Creates vector embeddings from text\n",
    "   - `GenParams`: Constants for model parameters (temperature, tokens, etc.)\n",
    "\n",
    "2. **`langchain`**: Framework for building LLM applications\n",
    "   - `RecursiveCharacterTextSplitter`: Intelligently splits documents into chunks\n",
    "   - `Document`: Standard format for text with metadata\n",
    "   - `Chroma`: Vector database for similarity search\n",
    "\n",
    "3. **`dotenv`**: Loads environment variables from `.env` file (for security)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load and Verify Credentials üîë\n",
    "\n",
    "**This is the most important step!** Let's make sure your credentials are set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get credentials from environment\n",
    "WATSONX_URL = os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\")\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "WATSONX_PROJECT_ID = os.getenv(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "# Verification check\n",
    "print(\"üîç Credential Verification:\")\n",
    "print(f\"URL: {WATSONX_URL}\")\n",
    "print(f\"API Key: {'‚úÖ Found (length: ' + str(len(WATSONX_APIKEY)) + ')' if WATSONX_APIKEY else '‚ùå MISSING!'}\")\n",
    "print(f\"Project ID: {'‚úÖ Found (UUID format)' if WATSONX_PROJECT_ID else '‚ùå MISSING!'}\")\n",
    "\n",
    "# Stop if credentials are missing\n",
    "if not WATSONX_APIKEY or not WATSONX_PROJECT_ID:\n",
    "    raise ValueError(\n",
    "        \"\\n\\n‚ùå ERROR: Missing credentials!\\n\"\n",
    "        \"Please create a .env file with:\\n\"\n",
    "        \"WATSONX_APIKEY=your_api_key\\n\"\n",
    "        \"WATSONX_PROJECT_ID=your_project_id\\n\"\n",
    "    )\n",
    "\n",
    "print(\"\\n‚úÖ All credentials verified! Ready to connect to watsonx.ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Troubleshooting Credentials\n",
    "\n",
    "**If you see ‚ùå MISSING:**\n",
    "\n",
    "1. **Check your `.env` file exists** in the same directory as this notebook\n",
    "2. **Verify the format** (no quotes needed):\n",
    "   ```\n",
    "   WATSONX_APIKEY=pak-abc123...\n",
    "   WATSONX_PROJECT_ID=12345678-1234-1234-1234-123456789abc\n",
    "   ```\n",
    "3. **Get your API key**: IBM Cloud ‚Üí Manage ‚Üí Access (IAM) ‚Üí API keys\n",
    "4. **Get your Project ID**: watsonx.ai ‚Üí Projects ‚Üí [Your Project] ‚Üí Settings\n",
    "\n",
    "**Still having issues?** The `.env` file should be in the same folder as this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Initialize watsonx.ai Credentials üåê\n",
    "\n",
    "Now let's create a connection to watsonx.ai using our credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create credentials object\n",
    "# This securely stores your authentication info\n",
    "credentials = Credentials(\n",
    "    url=WATSONX_URL,\n",
    "    api_key=WATSONX_APIKEY\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Credentials object created!\")\n",
    "print(\"\\nüîê Your credentials are now securely stored in memory (never printed!)\")"\n",
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñ What Just Happened?\n",
    "\n",
    "The `Credentials` object:\n",
    "- Stores your API key and URL in a secure format\n",
    "- Will be used to authenticate all requests to watsonx.ai\n",
    "- Never exposes your actual API key (good security practice!)\n",
    "\n",
    "Think of it like a digital passport that proves you have access to watsonx.ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Initialize the Embedding Model üé®\n",
    "\n",
    "**What are embeddings?**\n",
    "- Embeddings convert text into numbers (vectors) that computers can understand\n",
    "- Similar sentences have similar vectors\n",
    "- This enables semantic search (finding meaning, not just keywords)\n",
    "\n",
    "**Example:**\n",
    "- \"I love pizza\" and \"Pizza is my favorite\" ‚Üí Similar vectors\n",
    "- \"I love pizza\" and \"Quantum physics is complex\" ‚Üí Different vectors\n",
    "\n",
    "We'll use IBM's **slate-30m-english-rtrvr** model, optimized for retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize the watsonx embedding model\n",
    "# Hint: Use Embeddings() with model_id, credentials, and project_id\n",
    "# Model to use: \"ibm/slate-30m-english-rtrvr\"\n",
    "\n",
    "# embedding_model = Embeddings(\n",
    "#     model_id=\"ibm/slate-30m-english-rtrvr\",\n",
    "#     credentials=credentials,\n",
    "#     project_id=WATSONX_PROJECT_ID\n",
    "# )\n",
    "\n",
    "# For now, we'll create it for you:\n",
    "embedding_model = Embeddings(\n",
    "    model_id=\"ibm/slate-30m-english-rtrvr\",\n",
    "    credentials=credentials,\n",
    "    project_id=WATSONX_PROJECT_ID\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embedding model initialized!\")\n",
    "print(\"Model: ibm/slate-30m-english-rtrvr\")\n",
    "print(\"Optimized for: Retrieval tasks in RAG systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test the Embedding Model\n",
    "\n",
    "Let's test it by creating embeddings for a sample sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding\n",
    "test_text = \"What is artificial intelligence?\"\n",
    "test_embedding = embedding_model.embed_query(test_text)\n",
    "\n",
    "print(f\"Original text: '{test_text}'\")\n",
    "print(f\"\\nEmbedding (vector):\")\n",
    "print(f\"  - Length: {len(test_embedding)} dimensions\")\n",
    "print(f\"  - First 5 values: {test_embedding[:5]}\")\n",
    "print(f\"\\nüìä Each word/sentence gets converted into a {len(test_embedding)}-dimensional vector!\")\n",
    "print(\"These vectors capture the semantic meaning of the text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Industry Best Practice: Embedding Dimensions\n",
    "\n",
    "**Why 384 dimensions?**\n",
    "- **Smaller models** (128-384 dims): Faster, less storage, good for most use cases\n",
    "- **Larger models** (768-1536 dims): More accurate, slower, more storage\n",
    "\n",
    "**slate-30m-english-rtrvr** uses 384 dims ‚Üí Great balance of speed and accuracy!\n",
    "\n",
    "**Real-world tip**: Start with smaller embeddings, only upgrade if accuracy isn't good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Initialize the LLM (Granite Model) üèîÔ∏è\n",
    "\n",
    "**What is Granite?**\n",
    "- IBM's family of foundation models\n",
    "- Trained on enterprise data and optimized for business use\n",
    "- Available in different sizes: 3B, 8B, 13B, 20B parameters\n",
    "- We'll use **granite-13b-chat-v2** (good balance of quality and speed)\n",
    "\n",
    "**Why Granite for enterprise?**\n",
    "- ‚úÖ Trained with enterprise compliance in mind\n",
    "- ‚úÖ Supports multiple languages\n",
    "- ‚úÖ Optimized for RAG and instruction-following\n",
    "- ‚úÖ Built-in safety guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model parameters\n",
    "# These control how the model generates text\n",
    "model_params = {\n",
    "    GenParams.DECODING_METHOD: \"greedy\",  # Deterministic (same input = same output)\n",
    "    GenParams.MAX_NEW_TOKENS: 300,        # Maximum length of response\n",
    "    GenParams.TEMPERATURE: 0.7,           # Creativity level (0=focused, 1=creative)\n",
    "    GenParams.TOP_P: 1,                   # Nucleus sampling (1=consider all tokens)\n",
    "    GenParams.TOP_K: 50                   # Consider top 50 most likely tokens\n",
    "}\n",
    "\n",
    "print(\"üéõÔ∏è Model Parameters Configured:\")\n",
    "print(f\"  Decoding: greedy (deterministic)\")\n",
    "print(f\"  Max tokens: 300\")\n",
    "print(f\"  Temperature: 0.7 (balanced)\")\n",
    "print(f\"\\nüí° Tip: For factual Q&A (like RAG), use lower temperature (0.3-0.7)\")\n",
    "print(\"For creative writing, use higher temperature (0.8-1.2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize the Granite model\n",
    "# Hint: Use ModelInference() with model_id, credentials, project_id, and params\n",
    "# Model to use: \"ibm/granite-13b-chat-v2\"\n",
    "\n",
    "# granite_model = ModelInference(\n",
    "#     model_id=\"ibm/granite-13b-chat-v2\",\n",
    "#     credentials=credentials,\n",
    "#     project_id=WATSONX_PROJECT_ID,\n",
    "#     params=model_params\n",
    "# )\n",
    "\n",
    "# For now, we'll create it for you:\n",
    "granite_model = ModelInference(\n",
    "    model_id=\"ibm/granite-13b-chat-v2\",\n",
    "    credentials=credentials,\n",
    "    project_id=WATSONX_PROJECT_ID,\n",
    "    params=model_params\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Granite model initialized!\")\n",
    "print(\"Model: ibm/granite-13b-chat-v2\")\n",
    "print(\"Ready to generate text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test the Granite Model\n",
    "\n",
    "Let's make sure the model works by asking it a simple question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_prompt = \"What is RAG in the context of AI? Explain in 2 sentences.\"\n",
    "\n",
    "print(f\"ü§î Question: {test_prompt}\\n\")\n",
    "print(\"‚è≥ Generating answer...\")\n",
    "\n",
    "start_time = time.time()\n",
    "test_response = granite_model.generate_text(prompt=test_prompt)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚ú® Granite's Answer:\")\n",
    "print(test_response)\n",
    "print(f\"\\n‚ö° Response time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding the Response\n",
    "\n",
    "**What just happened:**\n",
    "1. Your prompt was sent to watsonx.ai cloud\n",
    "2. The Granite model processed it\n",
    "3. Generated a response based on its training\n",
    "4. Sent the answer back to your notebook\n",
    "\n",
    "**Notice:**\n",
    "- The response is coherent and factual\n",
    "- Typically takes 1-3 seconds (depends on network and model load)\n",
    "- Cloud-based, so no GPU needed on your machine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Prepare Sample Documents üìÑ\n",
    "\n",
    "For our RAG system, we need documents to search through. In a real application, these would be:\n",
    "- Company knowledge base articles\n",
    "- Product documentation\n",
    "- Customer support FAQs\n",
    "- Legal documents\n",
    "- Research papers\n",
    "\n",
    "For this lab, we'll use sample documents about AI and watsonx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about AI and watsonx\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        IBM watsonx.ai is an enterprise-ready AI and data platform designed to multiply \n",
    "        the impact of AI across your business. It provides access to IBM's Granite foundation \n",
    "        models, as well as open-source models from Hugging Face. The platform includes tools \n",
    "        for prompt engineering, model tuning, and deployment. watsonx.ai is built on Red Hat \n",
    "        OpenShift and can be deployed on any cloud or on-premises.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"watsonx_overview.txt\", \"category\": \"platform\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Retrieval-Augmented Generation (RAG) is a technique that enhances large language \n",
    "        models by retrieving relevant information from external knowledge bases before \n",
    "        generating responses. This approach reduces hallucinations and provides more \n",
    "        accurate, up-to-date answers. RAG works by: 1) Converting user queries into \n",
    "        embeddings, 2) Searching a vector database for similar documents, 3) Providing \n",
    "        the retrieved context to the LLM along with the original question.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"rag_explanation.txt\", \"category\": \"technique\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        IBM Granite models are a family of foundation models trained on enterprise-grade \n",
    "        data. They are available in multiple sizes: Granite-3B, Granite-8B, Granite-13B, \n",
    "        and Granite-20B. The models are optimized for business applications including code \n",
    "        generation, instruction following, and RAG. Granite models feature built-in safety \n",
    "        guardrails and support for multiple languages including English, German, Spanish, \n",
    "        French, Japanese, and Portuguese.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"granite_models.txt\", \"category\": \"models\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Vector databases are specialized databases designed to store and search vector \n",
    "        embeddings efficiently. Popular options include Chroma, Pinecone, Weaviate, Milvus, \n",
    "        and Elasticsearch. These databases use algorithms like HNSW (Hierarchical Navigable \n",
    "        Small World) or IVF (Inverted File Index) to perform fast similarity searches. \n",
    "        Vector databases are essential for RAG systems as they enable semantic search \n",
    "        rather than just keyword matching.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"vector_databases.txt\", \"category\": \"technology\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Prompt engineering is the art and science of crafting effective prompts for \n",
    "        large language models. Good prompts are clear, specific, and provide context. \n",
    "        Common techniques include few-shot learning (providing examples), chain-of-thought \n",
    "        prompting (asking the model to explain its reasoning), and role-playing (asking \n",
    "        the model to act as an expert). For RAG systems, prompts should include both \n",
    "        the retrieved context and clear instructions on how to use it.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"prompt_engineering.txt\", \"category\": \"technique\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} documents\")\n",
    "print(\"\\nüìÑ Document sources:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"  {i}. {doc.metadata['source']} ({doc.metadata['category']})\")\n",
    "    print(f\"     Length: {len(doc.page_content.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Document Structure\n",
    "\n",
    "Each `Document` object has:\n",
    "- **`page_content`**: The actual text\n",
    "- **`metadata`**: Additional information (source file, category, date, etc.)\n",
    "\n",
    "**Why metadata matters:**\n",
    "- Helps with filtering (e.g., \"only search documents from 2024\")\n",
    "- Provides source attribution (\"This answer came from document X\")\n",
    "- Enables better user experience (show where info came from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Chunk the Documents üìë\n",
    "\n",
    "**Why chunk documents?**\n",
    "\n",
    "Imagine you have a 100-page manual. When someone asks a question:\n",
    "- ‚ùå Don't send all 100 pages to the LLM (too expensive, too slow)\n",
    "- ‚úÖ Send only the relevant paragraphs (cheaper, faster, more focused)\n",
    "\n",
    "**Chunking = Breaking documents into smaller, manageable pieces**\n",
    "\n",
    "**Key parameters:**\n",
    "- `chunk_size`: How many characters per chunk (e.g., 500)\n",
    "- `chunk_overlap`: How much overlap between chunks (e.g., 50)\n",
    "\n",
    "**Why overlap?** Prevents cutting sentences in half and losing context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a text splitter\n",
    "# Hint: Use RecursiveCharacterTextSplitter with chunk_size=500 and chunk_overlap=100\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=500,\n",
    "#     chunk_overlap=100,\n",
    "#     separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "# )\n",
    "\n",
    "# For now, we'll create it for you:\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,       # Each chunk is ~500 characters\n",
    "    chunk_overlap=100,    # 100 characters overlap between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Try to split on paragraphs first, then sentences\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "print(f\"\\nüìä Chunking stats:\")\n",
    "print(f\"  Average chunk size: {sum(len(c.page_content) for c in chunks) / len(chunks):.0f} characters\")\n",
    "print(f\"\\nüîç Example chunk:\")\n",
    "print(f\"  Source: {chunks[0].metadata['source']}\")\n",
    "print(f\"  Content: {chunks[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Chunking Best Practices (Industry Tips)\n",
    "\n",
    "**Optimal chunk size depends on your use case:**\n",
    "\n",
    "| Use Case | Chunk Size | Why |\n",
    "|----------|------------|-----|\n",
    "| FAQ/Short answers | 200-500 | Questions are short, answers are focused |\n",
    "| Documentation | 500-1000 | Need full paragraphs for context |\n",
    "| Long-form articles | 1000-1500 | Need complete sections |\n",
    "| Legal/Technical docs | 800-1200 | Preserve complete clauses/procedures |\n",
    "\n",
    "**Rule of thumb:** Start with 500, adjust based on your results!\n",
    "\n",
    "**Overlap:** Usually 10-20% of chunk size (prevents context loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Create Embeddings and Vector Store üóÑÔ∏è\n",
    "\n",
    "Now comes the magic! We'll:\n",
    "1. Convert each chunk into a vector (using watsonx embeddings)\n",
    "2. Store vectors in Chroma database\n",
    "3. Enable fast semantic search\n",
    "\n",
    "**This might take 30-60 seconds depending on your internet speed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper class to make watsonx embeddings work with LangChain\n",
    "class WatsonxEmbeddingsWrapper(LangChainEmbeddings):\n",
    "    \"\"\"Wrapper to make watsonx embeddings compatible with LangChain.\"\"\"\n",
    "    \n",
    "    def __init__(self, watsonx_embeddings):\n",
    "        self.watsonx_embeddings = watsonx_embeddings\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        return [self.watsonx_embeddings.embed_query(text) for text in texts]\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a single query.\"\"\"\n",
    "        return self.watsonx_embeddings.embed_query(text)\n",
    "\n",
    "# Wrap our watsonx embedding model\n",
    "embeddings_wrapper = WatsonxEmbeddingsWrapper(embedding_model)\n",
    "\n",
    "print(\"‚úÖ Embedding wrapper created!\")\n",
    "print(\"This allows watsonx embeddings to work seamlessly with LangChain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "print(\"‚è≥ Creating vector store (this may take 30-60 seconds)...\")\n",
    "print(\"Converting all chunks to embeddings and storing in Chroma...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_wrapper,\n",
    "    persist_directory=\"./watsonx_rag_db\"  # Save to disk so we don't have to recreate\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Vector store created in {elapsed:.1f} seconds!\")\n",
    "print(f\"\\nüìä Database stats:\")\n",
    "print(f\"  Chunks stored: {len(chunks)}\")\n",
    "print(f\"  Embedding dimension: 384\")\n",
    "print(f\"  Storage location: ./watsonx_rag_db/\")\n",
    "print(f\"\\nüí° Next time you run this, the database will load instantly from disk!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "**Behind the scenes:**\n",
    "1. Each chunk was sent to watsonx.ai\n",
    "2. The slate embedding model converted text ‚Üí 384-dimensional vector\n",
    "3. Vector + metadata stored in Chroma database\n",
    "4. An index was built for fast similarity search\n",
    "\n",
    "**Now you can search by meaning, not just keywords!**\n",
    "\n",
    "Example:\n",
    "- Query: \"How do I reduce AI hallucinations?\"\n",
    "- Match: Document about RAG (even though it doesn't say \"hallucinations\")\n",
    "- Why: Similar semantic meaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Test Retrieval üîé\n",
    "\n",
    "Before building the full RAG pipeline, let's test that retrieval works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"What is watsonx.ai?\"\n",
    "\n",
    "print(f\"üîç Query: '{test_query}'\\n\")\n",
    "print(\"Searching for relevant chunks...\\n\")\n",
    "\n",
    "# Retrieve top 3 most relevant chunks\n",
    "relevant_chunks = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "print(f\"‚úÖ Found {len(relevant_chunks)} relevant chunks:\\n\")\n",
    "\n",
    "for i, chunk in enumerate(relevant_chunks, 1):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(f\"Source: {chunk.metadata['source']}\")\n",
    "    print(f\"Category: {chunk.metadata['category']}\")\n",
    "    print(f\"Content preview: {chunk.page_content[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Retrieval Analysis\n",
    "\n",
    "**What to notice:**\n",
    "- The most relevant chunk should be from `watsonx_overview.txt`\n",
    "- Results are ranked by similarity (best match first)\n",
    "- Even if the query uses different words, it finds the right content!\n",
    "\n",
    "**Try different queries:**\n",
    "- \"How to reduce hallucinations?\"\n",
    "- \"What models does IBM offer?\"\n",
    "- \"Explain vector databases\"\n",
    "\n",
    "Change `test_query` above and re-run to see different results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Build the RAG Pipeline üöÄ\n",
    "\n",
    "**Now for the grand finale!** Let's put it all together:\n",
    "\n",
    "```\n",
    "User Question ‚Üí Retrieve Relevant Chunks ‚Üí Build Prompt ‚Üí LLM Generates Answer\n",
    "```\n",
    "\n",
    "This is the core RAG pattern used in production systems worldwide!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer_question(question: str, k: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        k: Number of relevant chunks to retrieve (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer, sources, and metadata\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    print(f\"üîç Step 1: Retrieving top {k} relevant chunks...\")\n",
    "    relevant_chunks = vectorstore.similarity_search(question, k=k)\n",
    "    \n",
    "    # Step 2: Build context from retrieved chunks\n",
    "    print(f\"üìù Step 2: Building context from retrieved chunks...\")\n",
    "    context = \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "    \n",
    "    # Step 3: Create a prompt with context\n",
    "    print(f\"‚ú® Step 3: Creating prompt for Granite...\")\n",
    "    \n",
    "    # Granite uses a specific chat format\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful AI assistant. Use the provided context to answer questions accurately and concisely.\n",
    "If the context doesn't contain enough information to answer the question, say so honestly.\n",
    "<|endofsystem|>\n",
    "\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "<|endofuser|>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer using Granite\n",
    "    print(f\"ü§ñ Step 4: Generating answer with Granite...\\n\")\n",
    "    start_time = time.time()\n",
    "    answer = granite_model.generate_text(prompt=prompt)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Compile results\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer.strip(),\n",
    "        \"sources\": [chunk.metadata for chunk in relevant_chunks],\n",
    "        \"num_chunks\": len(relevant_chunks),\n",
    "        \"response_time\": elapsed\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ RAG function created!\")\n",
    "print(\"Ready to answer questions using Retrieval-Augmented Generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñ Understanding the RAG Function\n",
    "\n",
    "**Let's break down what happens:**\n",
    "\n",
    "1. **Retrieval** (`similarity_search`)\n",
    "   - Converts question to embedding\n",
    "   - Finds k most similar chunks\n",
    "   - Returns ranked results\n",
    "\n",
    "2. **Context Building** (`join`)\n",
    "   - Combines all relevant chunks into one text\n",
    "   - Separates with newlines for readability\n",
    "\n",
    "3. **Prompt Engineering** (Granite format)\n",
    "   - `<|system|>`: Instructions for the AI\n",
    "   - `<|user|>`: Context + Question\n",
    "   - `<|assistant|>`: Where answer appears\n",
    "\n",
    "4. **Generation** (`generate_text`)\n",
    "   - Granite reads context + question\n",
    "   - Generates answer based on provided info\n",
    "   - Returns focused, factual response\n",
    "\n",
    "**This is production-quality code!** Used in real enterprise applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Test the RAG System üéØ\n",
    "\n",
    "Let's ask some questions and see how well our RAG system performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 1\n",
    "question1 = \"What is watsonx.ai and what can it do?\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Question: {question1}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "result1 = rag_answer_question(question1)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìù ANSWER:\")\n",
    "print(\"=\"*80)\n",
    "print(result1['answer'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìö SOURCES:\")\n",
    "print(\"=\"*80)\n",
    "for i, source in enumerate(result1['sources'], 1):\n",
    "    print(f\"{i}. {source['source']} ({source['category']})\")\n",
    "print(f\"\\n‚ö° Response time: {result1['response_time']:.2f} seconds\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 2\n",
    "question2 = \"How does RAG reduce hallucinations in AI?\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Question: {question2}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "result2 = rag_answer_question(question2)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìù ANSWER:\")\n",
    "print(\"=\"*80)\n",
    "print(result2['answer'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìö SOURCES:\")\n",
    "print(\"=\"*80)\n",
    "for i, source in enumerate(result2['sources'], 1):\n",
    "    print(f\"{i}. {source['source']} ({source['category']})\")\n",
    "print(f\"\\n‚ö° Response time: {result2['response_time']:.2f} seconds\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question 3  \n",
    "question3 = \"Tell me about IBM Granite models\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Question: {question3}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "result3 = rag_answer_question(question3)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìù ANSWER:\")\n",
    "print(\"=\"*80)\n",
    "print(result3['answer'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìö SOURCES:\")\n",
    "print(\"=\"*80)\n",
    "for i, source in enumerate(result3['sources'], 1):\n",
    "    print(f\"{i}. {source['source']} ({source['category']})\")\n",
    "print(f\"\\n‚ö° Response time: {result3['response_time']:.2f} seconds\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé® Try Your Own Questions!\n",
    "\n",
    "Now it's your turn! Ask your own questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Ask any question about our documents!\n",
    "my_question = \"What are vector databases used for?\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Your Question: {my_question}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "my_result = rag_answer_question(my_question)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìù ANSWER:\")\n",
    "print(\"=\"*80)\n",
    "print(my_result['answer'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìö SOURCES:\")\n",
    "print(\"=\"*80)\n",
    "for i, source in enumerate(my_result['sources'], 1):\n",
    "    print(f\"{i}. {source['source']} ({source['category']})\")\n",
    "print(f\"\\n‚ö° Response time: {my_result['response_time']:.2f} seconds\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì What You've Accomplished!\n",
    "\n",
    "Congratulations! You've just built a **production-grade RAG system** using enterprise AI! üéâ\n",
    "\n",
    "### ‚úÖ Skills Mastered:\n",
    "1. ‚úÖ **Authentication**: Securely connect to watsonx.ai\n",
    "2. ‚úÖ **Embeddings**: Convert text to vectors using IBM models\n",
    "3. ‚úÖ **Vector Storage**: Store and search embeddings in Chroma\n",
    "4. ‚úÖ **Retrieval**: Find relevant context using semantic search\n",
    "5. ‚úÖ **Generation**: Use Granite models to create answers\n",
    "6. ‚úÖ **RAG Pipeline**: Combine all components into a working system\n",
    "\n",
    "### üèÜ Industry Skills You've Learned:\n",
    "- Using enterprise AI platforms (watsonx.ai)\n",
    "- Implementing RAG architecture\n",
    "- Working with foundation models (Granite)\n",
    "- Managing embeddings and vector databases\n",
    "- Building production-ready AI systems\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "### Immediate:\n",
    "1. **Experiment**: Try different questions, adjust parameters (k, chunk_size)\n",
    "2. **Compare**: How does this compare to Lab 2.1 (local RAG)?\n",
    "3. **Extend**: Add your own documents (PDFs, text files, web pages)\n",
    "\n",
    "### Advanced (Optional):\n",
    "1. **Try Elasticsearch**: Replace Chroma with enterprise vector database\n",
    "2. **Add Metadata Filtering**: Search only specific document categories\n",
    "3. **Implement Hybrid Search**: Combine keyword + semantic search\n",
    "4. **Add Re-ranking**: Improve retrieval accuracy\n",
    "5. **Build a Web UI**: Create a Streamlit app for your RAG system\n",
    "\n",
    "### Continue Learning:\n",
    "- **Lab 2.3**: Compare Ollama vs watsonx RAG side-by-side\n",
    "- **Lab 2.4**: Build evaluation metrics for RAG systems\n",
    "- **Day 3**: Add agents and orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Takeaways\n",
    "\n",
    "### 1. **Enterprise vs Local RAG**\n",
    "- **Local (Ollama)**: Fast, free, private, offline-capable\n",
    "- **Enterprise (watsonx)**: Scalable, governed, supported, production-ready\n",
    "\n",
    "### 2. **RAG Benefits**\n",
    "- ‚úÖ Reduces hallucinations\n",
    "- ‚úÖ Provides up-to-date information\n",
    "- ‚úÖ Enables source attribution\n",
    "- ‚úÖ Works with your private data\n",
    "\n",
    "### 3. **Production Considerations**\n",
    "- Security: Never hardcode credentials\n",
    "- Scalability: Use proper vector databases\n",
    "- Monitoring: Track performance metrics\n",
    "- Governance: Maintain audit logs\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Challenge Yourself!\n",
    "\n",
    "Ready for more? Try these challenges:\n",
    "\n",
    "### Challenge 1: Add Your Own Documents\n",
    "```python\n",
    "# Add documents about your favorite topic!\n",
    "custom_doc = Document(\n",
    "    page_content=\"Your content here...\",\n",
    "    metadata={\"source\": \"my_doc.txt\", \"category\": \"custom\"}\n",
    ")\n",
    "```\n",
    "\n",
    "### Challenge 2: Experiment with Parameters\n",
    "- Try different `chunk_size` values (200, 500, 1000)\n",
    "- Adjust `k` (number of retrieved chunks)\n",
    "- Modify `temperature` in model params\n",
    "\n",
    "### Challenge 3: Compare Response Quality\n",
    "Ask the same question with different k values:\n",
    "- k=1 (single chunk)\n",
    "- k=3 (default)\n",
    "- k=5 (more context)\n",
    "\n",
    "Which gives the best answers?\n",
    "\n",
    "---\n",
    "\n",
    "## üÜò Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**1. Authentication Errors**\n",
    "- Check your `.env` file\n",
    "- Verify API key in IBM Cloud\n",
    "- Ensure Project ID is correct\n",
    "\n",
    "**2. Slow Performance**\n",
    "- Normal for first run (creating embeddings)\n",
    "- Subsequent runs use cached database\n",
    "- Network speed affects watsonx calls\n",
    "\n",
    "**3. Out of Memory**\n",
    "- Reduce `chunk_size`\n",
    "- Process fewer documents\n",
    "- Use batch processing\n",
    "\n",
    "**4. Poor Answer Quality**\n",
    "- Increase `k` (retrieve more chunks)\n",
    "- Improve your prompts\n",
    "- Add more relevant documents\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- [watsonx.ai Documentation](https://www.ibm.com/docs/en/watsonx-as-a-service)\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Chroma Documentation](https://docs.trychroma.com/)\n",
    "- [RAG Research Paper](https://arxiv.org/abs/2005.11401)\n",
    "\n",
    "---\n",
    "\n",
    "**Great job completing Lab 2.2! You're now equipped to build enterprise RAG systems! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
