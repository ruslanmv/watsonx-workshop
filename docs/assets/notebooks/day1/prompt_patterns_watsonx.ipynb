{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2 - Prompt Patterns with watsonx.ai\n",
    "\n",
    "**Duration**: 20 minutes | **Difficulty**: Intermediate\n",
    "\n",
    "## Objectives\n",
    "- Create reusable prompt templates using watsonx.ai\n",
    "- Implement three core patterns with IBM Granite models\n",
    "- Compare results with Ollama implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup credentials\n",
    "credentials = Credentials(\n",
    "    url=os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\"),\n",
    "    api_key=os.getenv(\"WATSONX_APIKEY\")\n",
    ")\n",
    "\n",
    "PROJECT_ID = os.getenv(\"WATSONX_PROJECT_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Granite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model parameters\n",
    "model_params = {\n",
    "    GenParams.MAX_NEW_TOKENS: 200,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 50\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "granite_model = ModelInference(\n",
    "    model_id=\"ibm/granite-13b-chat-v2\",\n",
    "    credentials=credentials,\n",
    "    project_id=PROJECT_ID,\n",
    "    params=model_params\n",
    ")\n",
    "\n",
    "print(\"✅ Granite model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template 1: Summarization\n",
    "\n",
    "Create a function that summarizes text using Granite's chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_watsonx(text: str, num_sentences: int = 3) -> str:\n",
    "    \"\"\"Summarize text using watsonx.ai Granite model.\"\"\"\n",
    "    \n",
    "    # Granite uses a specific chat format\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful AI assistant that provides concise summaries.\n",
    "<|endofsystem|>\n",
    "\n",
    "<|user|>\n",
    "Summarize the following text in {num_sentences} sentences.\n",
    "Focus on the main points and key takeaways.\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "<|endofuser|>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    response = granite_model.generate_text(prompt=prompt)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that \n",
    "enables systems to learn from data without explicit programming.\n",
    "It uses algorithms to identify patterns and make predictions.\n",
    "Applications include image recognition, natural language processing,\n",
    "and recommendation systems.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_watsonx(test_text, num_sentences=2)\n",
    "print(\"Summary (Granite):\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template 2: Style Transfer\n",
    "\n",
    "Rewrite text in different tones using Granite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_style_watsonx(text: str, target_tone: str) -> str:\n",
    "    \"\"\"Rewrite text in a different style using watsonx.ai.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are an expert writer who can adapt text to different tones and styles.\n",
    "<|endofsystem|>\n",
    "\n",
    "<|user|>\n",
    "Rewrite the following text in a {target_tone} tone:\n",
    "\n",
    "Original text:\n",
    "{text}\n",
    "<|endofuser|>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    response = granite_model.generate_text(prompt=prompt)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = \"Hey team, the API is down. Can someone check it ASAP?\"\n",
    "\n",
    "formal = rewrite_style_watsonx(original, \"formal business\")\n",
    "print(\"Formal version (Granite):\")\n",
    "print(formal)\n",
    "print()\n",
    "\n",
    "casual = rewrite_style_watsonx(original, \"very casual and friendly\")\n",
    "print(\"Casual version (Granite):\")\n",
    "print(casual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template 3: Q&A with Context\n",
    "\n",
    "Answer questions based on provided context - preparation for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_with_context_watsonx(context: str, question: str) -> str:\n",
    "    \"\"\"Answer a question based on provided context using watsonx.ai.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful AI assistant that answers questions based on provided context.\n",
    "If the context doesn't contain the answer, say \"I don't have enough information.\"\n",
    "<|endofsystem|>\n",
    "\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "<|endofuser|>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    \n",
    "    response = granite_model.generate_text(prompt=prompt)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Q&A Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "watsonx.ai was released by IBM in 2023 as an enterprise AI platform.\n",
    "It provides access to IBM Granite models and integrates with IBM Cloud services.\n",
    "\"\"\"\n",
    "\n",
    "question1 = \"When was watsonx.ai released?\"\n",
    "answer1 = qa_with_context_watsonx(context, question1)\n",
    "print(f\"Q: {question1}\")\n",
    "print(f\"A (Granite): {answer1}\")\n",
    "print()\n",
    "\n",
    "question2 = \"What programming languages does it support?\"\n",
    "answer2 = qa_with_context_watsonx(context, question2)\n",
    "print(f\"Q: {question2}\")\n",
    "print(f\"A (Granite): {answer2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Ollama vs watsonx\n",
    "\n",
    "Run the same prompts on both backends and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example comparison (if you have ollama running)\n",
    "# import ollama\n",
    "# \n",
    "# test_prompt = \"What is the capital of France?\"\n",
    "# \n",
    "# ollama_result = ollama.chat(\n",
    "#     model=\"qwen2.5:0.5b-instruct\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": test_prompt}]\n",
    "# )[\"message\"][\"content\"]\n",
    "# \n",
    "# granite_result = granite_model.generate_text(prompt=test_prompt)\n",
    "# \n",
    "# print(\"Ollama:\", ollama_result)\n",
    "# print(\"Granite:\", granite_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Section\n",
    "\n",
    "Try your own prompts with Granite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n",
    "# Try different texts, tones, and questions with Granite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- ✅ Created three reusable prompt templates for watsonx.ai\n",
    "- ✅ Learned Granite's chat format (<|system|>, <|user|>, <|assistant|>)\n",
    "- ✅ Tested enterprise-grade LLM for summarization, style transfer, and Q&A\n",
    "- ✅ Ready to build RAG systems with watsonx.ai\n",
    "\n",
    "**Next**: Lab 1.3 - Micro-Evaluation to compare both backends systematically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
