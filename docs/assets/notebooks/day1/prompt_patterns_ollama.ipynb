{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2 - Prompt Patterns with Ollama\n",
    "\n",
    "**Duration**: 20 minutes | **Difficulty**: Intermediate\n",
    "\n",
    "## Objectives\n",
    "- Create reusable prompt templates in Python\n",
    "- Implement three core patterns: Summarization, Style Transfer, Q&A with Context\n",
    "- Test templates with various inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template 1: Summarization\n",
    "\n",
    "Create a function that summarizes text in a specified number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_ollama(text: str, num_sentences: int = 3) -> str:\n",
    "    \"\"\"Summarize text using Ollama.\"\"\"\n",
    "    template = f\"\"\"Summarize the following text in {num_sentences} sentences.\n",
    "Focus on the main points and key takeaways.\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"qwen2.5:0.5b-instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": template}]\n",
    "    )\n",
    "    \n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that \n",
    "enables systems to learn from data without explicit programming.\n",
    "It uses algorithms to identify patterns and make predictions.\n",
    "Applications include image recognition, natural language processing,\n",
    "and recommendation systems.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_ollama(test_text, num_sentences=2)\n",
    "print(\"Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template 2: Style Transfer\n",
    "\n",
    "Rewrite text in different tones or styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_style_ollama(text: str, target_tone: str) -> str:\n",
    "    \"\"\"Rewrite text in a different style using Ollama.\"\"\"\n",
    "    template = f\"\"\"Rewrite the following text in a {target_tone} tone:\n",
    "\n",
    "Original text:\n",
    "{text}\n",
    "\n",
    "Rewritten text:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"qwen2.5:0.5b-instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": template}]\n",
    "    )\n",
    "    \n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = \"Hey team, the API is down. Can someone check it ASAP?\"\n",
    "\n",
    "formal = rewrite_style_ollama(original, \"formal business\")\n",
    "print(\"Formal version:\")\n",
    "print(formal)\n",
    "print()\n",
    "\n",
    "casual = rewrite_style_ollama(original, \"very casual and friendly\")\n",
    "print(\"Casual version:\")\n",
    "print(casual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template 3: Q&A with Context\n",
    "\n",
    "Answer questions based on provided context - a mini-RAG pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_with_context_ollama(context: str, question: str) -> str:\n",
    "    \"\"\"Answer a question based on provided context using Ollama.\"\"\"\n",
    "    template = f\"\"\"Based on the following information, answer the question.\n",
    "If the information doesn't contain the answer, say \"I don't have enough information.\"\n",
    "\n",
    "Information:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"qwen2.5:0.5b-instruct\",\n",
    "        messages=[{\"role\": \"user\", \"content\": template}]\n",
    "    )\n",
    "    \n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Q&A Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "watsonx.ai was released by IBM in 2023 as an enterprise AI platform.\n",
    "It provides access to IBM Granite models and integrates with IBM Cloud services.\n",
    "\"\"\"\n",
    "\n",
    "question1 = \"When was watsonx.ai released?\"\n",
    "answer1 = qa_with_context_ollama(context, question1)\n",
    "print(f\"Q: {question1}\")\n",
    "print(f\"A: {answer1}\")\n",
    "print()\n",
    "\n",
    "question2 = \"What programming languages does it support?\"\n",
    "answer2 = qa_with_context_ollama(context, question2)\n",
    "print(f\"Q: {question2}\")\n",
    "print(f\"A: {answer2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Section\n",
    "\n",
    "Try your own prompts and texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n",
    "# Try different texts, tones, and questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- ✅ Created three reusable prompt templates\n",
    "- ✅ Tested summarization, style transfer, and Q&A patterns\n",
    "- ✅ Learned how to structure prompts for consistent results\n",
    "- ✅ Built foundation for RAG systems (Q&A with context)\n",
    "\n",
    "**Next**: Implement the same patterns with watsonx.ai in `prompt_patterns_watsonx.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
