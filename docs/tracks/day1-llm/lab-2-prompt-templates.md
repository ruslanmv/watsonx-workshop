# Lab 1.2 – Prompt Templates in Ollama & watsonx

**Duration**: 60 minutes  
**Difficulty**: Intermediate

---

## Lab Overview

In this lab, you'll build reusable prompt templates for common tasks (summarization, style rewriting, Q&A) and implement them in both Ollama and watsonx.ai environments.

---

## Learning Objectives

- Create reusable prompt templates using Python
- Implement templates across different LLM backends
- Compare outputs systematically
- Design prompts for the accelerator's RAG system

---

## Prerequisites

- ✅ Lab 1.1 completed
- ✅ Understanding of prompt patterns from theory section 1.2

---

Due to length constraints, please view the complete lab instructions for Lab 1.2 in the repository. The lab covers:

**Part A: Ollama Templates**
- Summarization template
- Style rewrite template  
- Q&A with context template

**Part B: watsonx.ai Templates**
- Same templates implemented for watsonx.ai

**Part C: Comparative Experiments**
- Run same prompts across both backends
- Measure quality, latency, consistency

**Part D: Accelerator Integration**
- Plan production prompts for `rag/prompt.py`

## Deliverables

By the end of this lab, you should have:
- ✅ `prompt_patterns_ollama.ipynb` - working templates
- ✅ `prompt_patterns_watsonx.ipynb` - working templates
- ✅ Comparison results documented
- ✅ Draft design for production prompts

## Next Steps

Continue to **Lab 1.3 – Micro-Evaluation** to build a systematic evaluation framework.
