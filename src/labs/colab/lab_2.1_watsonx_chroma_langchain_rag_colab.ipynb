{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1 â€“ RAG with Watsonx, Chroma, and LangChain (Google Colab Version)\n",
    "\n",
    "![watsonx](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Run this notebook in Google Colab\n",
    "\n",
    "**Prerequisites:**\n",
    "- IBM Cloud API Key ([Create one here](https://cloud.ibm.com/iam/apikeys))\n",
    "- Watsonx Project ID ([Find it in your watsonx.ai project](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp))\n",
    "\n",
    "This notebook demonstrates **Retrieval Augmented Generation (RAG)** using:\n",
    "- **Watsonx.ai** for LLM inference (Granite models)\n",
    "- **Chroma** as vector database\n",
    "- **LangChain** for RAG orchestration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, we install all required libraries for this RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "!pip install -qU wget\n",
    "!pip install -qU \"langchain>=0.3,<0.4\"\n",
    "!pip install -qU \"ibm_watsonx_ai>=1.1.22\"\n",
    "!pip install -qU \"langchain_ibm>=0.3,<0.4\"\n",
    "!pip install -qU \"langchain_chroma>=0.1,<0.2\"\n",
    "\n",
    "print(\"âœ… All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Credentials\n",
    "\n",
    "Enter your IBM Cloud API Key and Watsonx Project ID when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Prompt for Watsonx credentials\n",
    "watsonx_api_key = getpass.getpass(\"Enter IBM Cloud API Key: \")\n",
    "project_id = getpass.getpass(\"Enter Watsonx Project ID: \")\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"  # Default region\n",
    "\n",
    "# Store in environment variables\n",
    "os.environ[\"WATSONX_APIKEY\"] = watsonx_api_key\n",
    "os.environ[\"PROJECT_ID\"] = project_id\n",
    "os.environ[\"WATSONX_URL\"] = url\n",
    "\n",
    "print(\"âœ… Credentials configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Watsonx API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=url,\n",
    "    api_key=watsonx_api_key,\n",
    ")\n",
    "\n",
    "api_client = APIClient(credentials=credentials, project_id=project_id)\n",
    "\n",
    "print(\"âœ… Watsonx API Client initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download Sample Data\n",
    "\n",
    "We'll use the State of the Union speech as our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "filename = 'state_of_the_union.txt'\n",
    "url_data = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    wget.download(url_data, out=filename)\n",
    "    print(f\"\\nâœ… Downloaded {filename}\")\n",
    "else:\n",
    "    print(f\"âœ… {filename} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build Knowledge Base with Chroma\n",
    "\n",
    "We'll:\n",
    "1. Load the document\n",
    "2. Split it into chunks\n",
    "3. Create embeddings using Watsonx\n",
    "4. Store in Chroma vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load document\n",
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"âœ… Split document into {len(texts)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Embeddings with Watsonx\n",
    "\n",
    "View available embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available embedding models\n",
    "api_client.foundation_models.EmbeddingModels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxEmbeddings\n",
    "\n",
    "embeddings = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-30m-english-rtrvr\",\n",
    "    url=credentials[\"url\"],\n",
    "    apikey=credentials[\"apikey\"],\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "# Create Chroma vector store\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "print(\"âœ… Vector database created with Chroma!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Initialize Watsonx LLM (Granite Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "model_id = ModelTypes.GRANITE_13B_CHAT_V2\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n",
    "}\n",
    "\n",
    "watsonx_granite = WatsonxLLM(\n",
    "    model_id=model_id.value,\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "print(\"âœ… Watsonx Granite LLM initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=watsonx_granite,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")\n",
    "\n",
    "print(\"âœ… RAG chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson?\"\n",
    "result = qa.invoke(query)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Question: {result['query']}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Try Your Own Questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try asking different questions about the State of the Union\n",
    "your_question = input(\"Enter your question: \")\n",
    "result = qa.invoke(your_question)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Question: {result['query']}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You successfully completed this notebook! You learned how to:\n",
    "\n",
    "âœ… Set up Watsonx.ai credentials in Google Colab  \n",
    "âœ… Create embeddings using Watsonx embedding models  \n",
    "âœ… Build a vector database with Chroma  \n",
    "âœ… Implement RAG using LangChain and Watsonx Granite models  \n",
    "\n",
    "For more information:\n",
    "- [Watsonx.ai Documentation](https://ibm.github.io/watsonx-ai-python-sdk/samples.html)\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "\n",
    "---\n",
    "\n",
    "**Copyright Â© 2023, 2024 IBM. This notebook and its source code are released under the terms of the MIT License.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
