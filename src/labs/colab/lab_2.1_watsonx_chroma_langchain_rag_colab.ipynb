{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "\n",
    "# Build a Question-Answering System using RAG with watsonx, Chroma, and LangChain\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this tutorial, you'll learn how to build a **Retrieval-Augmented Generation (RAG)** system that can answer questions based on a document. By the end of this notebook, you will be able to:\n",
    "\n",
    "- ‚úÖ Understand what RAG is and why it's useful\n",
    "- ‚úÖ Load and prepare documents for question-answering\n",
    "- ‚úÖ Create a vector database using Chroma\n",
    "- ‚úÖ Use watsonx.ai's Granite models with LangChain\n",
    "- ‚úÖ Build an end-to-end RAG pipeline\n",
    "- ‚úÖ Ask questions and get accurate answers from your documents\n",
    "\n",
    "## What is RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "RAG is a powerful technique that combines:\n",
    "- **Retrieval**: Finding relevant information from your documents\n",
    "- **Generation**: Using an AI model to create natural language answers\n",
    "\n",
    "**Why use RAG?**\n",
    "- Answer questions about your specific documents\n",
    "- Keep answers grounded in factual content\n",
    "- Avoid AI \"hallucinations\" (making up information)\n",
    "- Work with documents that weren't in the AI's training data\n",
    "\n",
    "## How RAG Works (Simple 3-Step Process)\n",
    "\n",
    "1. **üìö Index**: Break documents into chunks and store them in a searchable database (one-time setup)\n",
    "2. **üîç Retrieve**: When asked a question, find the most relevant chunks from the database\n",
    "3. **üí¨ Generate**: Feed the relevant chunks to an AI model to generate a natural answer\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, you'll need:\n",
    "- An IBM Cloud account (free tier available)\n",
    "- A Watson Machine Learning service instance\n",
    "- Basic familiarity with Python\n",
    "\n",
    "## Tutorial Contents\n",
    "\n",
    "- [Step 1: Install Dependencies](#install)\n",
    "- [Step 2: Set Up watsonx.ai Connection](#setup)\n",
    "- [Step 3: Load Your Document](#data)\n",
    "- [Step 4: Build the Knowledge Base](#build_base)\n",
    "- [Step 5: Configure the AI Model](#models)\n",
    "- [Step 6: Ask Questions and Get Answers](#predict)\n",
    "- [Summary and Next Steps](#summary)\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install",
   "metadata": {},
   "source": [
    "<a id=\"install\"></a>\n",
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, we need to install the necessary Python packages. Here's what each package does:\n",
    "\n",
    "- **wget**: Downloads files from the internet\n",
    "- **langchain**: Framework for building LLM applications\n",
    "- **ibm_watsonx_ai**: IBM's watsonx.ai SDK\n",
    "- **langchain_ibm**: Integration between LangChain and watsonx.ai\n",
    "- **langchain_chroma**: Vector database for storing document embeddings\n",
    "- **langchain_community**: Community integrations for LangChain\n",
    "- **langchain_text_splitters**: Tools to split documents into manageable chunks\n",
    "\n",
    "**Note**: The installation may take 1-2 minutes. You might see some dependency warnings - these are usually safe to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "# Using | tail -n 1 to show only the last line of output (keeps the notebook clean)\n",
    "\n",
    "!pip install wget | tail -n 1\n",
    "!pip install -U \"langchain>=0.3,<0.4\" | tail -n 1\n",
    "!pip install -U \"ibm_watsonx_ai>=1.1.22\" | tail -n 1\n",
    "!pip install -U \"langchain_ibm>=0.3,<0.4\" | tail -n 1\n",
    "!pip install -U \"langchain_chroma>=0.1,<0.2\" | tail -n 1\n",
    "!pip install -U \"langchain_community>=0.3,<0.4\" | tail -n 1\n",
    "!pip install -U \"langchain_text_splitters>=0.3,<0.4\" | tail -n 1\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Step 2: Set Up watsonx.ai Connection\n",
    "\n",
    "To use watsonx.ai, you need two things:\n",
    "1. **API Key**: Your personal access key\n",
    "2. **Project ID**: The ID of your watsonx.ai project\n",
    "\n",
    "### How to Get Your API Key\n",
    "\n",
    "1. Go to [IBM Cloud](https://cloud.ibm.com)\n",
    "2. Click on **Manage** ‚Üí **Access (IAM)**\n",
    "3. Select **API keys** from the left menu\n",
    "4. Click **Create an IBM Cloud API key**\n",
    "5. Give it a name (e.g., \"watsonx-notebook\")\n",
    "6. Copy the API key (you won't be able to see it again!)\n",
    "\n",
    "üìñ [Detailed Instructions](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)\n",
    "\n",
    "### How to Get Your Project ID\n",
    "\n",
    "1. Open [watsonx.ai](https://dataplatform.cloud.ibm.com/wx/home)\n",
    "2. Open your project\n",
    "3. Click on the **Manage** tab\n",
    "4. Find **Project ID** in the **General** section\n",
    "5. Copy the Project ID\n",
    "\n",
    "**Security Tip**: Never hardcode your API key in the notebook. We'll use `getpass` to enter it securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-credentials",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials\n",
    "\n",
    "# Set up your credentials\n",
    "# The API key will be hidden as you type it\n",
    "credentials = Credentials(\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",  # watsonx.ai API endpoint\n",
    "    api_key=getpass.getpass(\"Please enter your watsonx.ai API key: \"),\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Credentials configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your project ID\n",
    "# This can come from environment variable or manual input\n",
    "try:\n",
    "    project_id = os.environ[\"PROJECT_ID\"]\n",
    "    print(f\"‚úÖ Found project ID from environment: {project_id[:8]}...\")\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your watsonx.ai project ID: \")\n",
    "    print(\"‚úÖ Project ID configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient\n",
    "\n",
    "# Create API client to interact with watsonx.ai\n",
    "api_client = APIClient(credentials=credentials, project_id=project_id)\n",
    "\n",
    "print(\"‚úÖ API client created successfully!\")\n",
    "print(\"You're now connected to watsonx.ai! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Step 3: Load Your Document\n",
    "\n",
    "For this tutorial, we'll use the **State of the Union** address as our example document. This is a speech given by the U.S. President.\n",
    "\n",
    "### What's Happening Here?\n",
    "\n",
    "We'll download a text file from the internet and save it locally. Then we'll be able to ask questions about its content!\n",
    "\n",
    "**You can replace this with your own documents later** (PDFs, text files, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "# Define the document to download\n",
    "filename = 'state_of_the_union.txt'\n",
    "url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "if not os.path.isfile(filename):\n",
    "    print(f\"üì• Downloading document: {filename}...\")\n",
    "    wget.download(url, out=filename)\n",
    "    print(f\"\\n‚úÖ Document downloaded successfully!\")\n",
    "else:\n",
    "    print(f\"‚úÖ Document already exists: {filename}\")\n",
    "\n",
    "# Check the file size\n",
    "file_size = os.path.getsize(filename) / 1024  # Convert to KB\n",
    "print(f\"üìÑ File size: {file_size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-base-section",
   "metadata": {},
   "source": [
    "<a id=\"build_base\"></a>\n",
    "## Step 4: Build the Knowledge Base\n",
    "\n",
    "Now we'll create a **vector database** - a searchable knowledge base from our document.\n",
    "\n",
    "### The Process (Broken Down):\n",
    "\n",
    "1. **Load the document**: Read the text file\n",
    "2. **Split into chunks**: Break the document into smaller pieces (1000 characters each)\n",
    "   - Why? Large documents need to be split for better search results\n",
    "   - Each chunk should contain complete thoughts/sentences\n",
    "3. **Create embeddings**: Convert text chunks into numbers (vectors) that represent their meaning\n",
    "4. **Store in Chroma**: Save these vectors in a database that can quickly find similar content\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings are numerical representations of text that capture semantic meaning. Similar text has similar embeddings, allowing us to find relevant content even if the words are different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Step 1: Load the document\n",
    "print(\"üìñ Loading document...\")\n",
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "print(f\"‚úÖ Loaded {len(documents)} document(s)\")\n",
    "\n",
    "# Step 2: Split the document into chunks\n",
    "print(\"\\n‚úÇÔ∏è Splitting document into chunks...\")\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,        # Each chunk will be ~1000 characters\n",
    "    chunk_overlap=0         # No overlap between chunks (you can add overlap for better context)\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Created {len(texts)} text chunks\")\n",
    "print(f\"\\nüìù Example chunk (first 200 characters):\")\n",
    "print(f\"{texts[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings-section",
   "metadata": {},
   "source": [
    "### Choose an Embedding Model\n",
    "\n",
    "watsonx.ai provides several embedding models. Let's see what's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available embedding models\n",
    "print(\"üîç Available embedding models in watsonx.ai:\\n\")\n",
    "available_models = api_client.foundation_models.EmbeddingModels.show()\n",
    "\n",
    "for name, model_id in available_models.items():\n",
    "    print(f\"  ‚Ä¢ {name}\")\n",
    "    print(f\"    Model ID: {model_id}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-embeddings-section",
   "metadata": {},
   "source": [
    "### Create Embeddings and Build Vector Database\n",
    "\n",
    "We'll use the **all-minilm-l6-v2** model - it's fast, efficient, and works well for English text.\n",
    "\n",
    "This step will:\n",
    "1. Create embeddings for all text chunks\n",
    "2. Store them in Chroma (a vector database)\n",
    "3. Make the data searchable\n",
    "\n",
    "**Note**: You might see some ChromaDB telemetry warnings - these are harmless and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Step 3: Create embeddings using watsonx.ai\n",
    "print(\"üßÆ Creating embeddings using watsonx.ai...\")\n",
    "embeddings = WatsonxEmbeddings(\n",
    "    model_id=\"sentence-transformers/all-minilm-l6-v2\",  # Fast and efficient model\n",
    "    url=credentials[\"url\"],\n",
    "    apikey=credentials[\"apikey\"],\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "# Step 4: Create the vector database\n",
    "print(\"üíæ Building vector database with Chroma...\")\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "print(f\"\\n‚úÖ Knowledge base created successfully!\")\n",
    "print(f\"üìä Total chunks indexed: {len(texts)}\")\n",
    "print(\"\\nYour document is now searchable! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-search-section",
   "metadata": {},
   "source": [
    "### Test the Search (Optional)\n",
    "\n",
    "Let's verify that our vector database can find relevant content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retrieval system\n",
    "test_query = \"What did the president say about the economy?\"\n",
    "print(f\"üîç Test search: '{test_query}'\\n\")\n",
    "\n",
    "# Search for relevant chunks\n",
    "relevant_docs = docsearch.similarity_search(test_query, k=2)  # Get top 2 matches\n",
    "\n",
    "print(f\"Found {len(relevant_docs)} relevant chunks:\\n\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(doc.page_content[:300] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-section",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "## Step 5: Configure the AI Model\n",
    "\n",
    "Now we'll set up the **Granite** language model from watsonx.ai. This model will read the retrieved chunks and generate natural language answers.\n",
    "\n",
    "### About Granite Models\n",
    "\n",
    "Granite is IBM's family of enterprise-ready AI models:\n",
    "- Designed for business use cases\n",
    "- Trained on high-quality data\n",
    "- Optimized for accuracy and reliability\n",
    "\n",
    "### Model Parameters Explained\n",
    "\n",
    "- **DECODING_METHOD**: How the model generates text\n",
    "  - `GREEDY`: Always picks the most likely next word (more predictable)\n",
    "  - `SAMPLING`: Introduces randomness (more creative)\n",
    "- **MIN_NEW_TOKENS**: Minimum words to generate (at least 1)\n",
    "- **MAX_NEW_TOKENS**: Maximum words to generate (up to 100 - keeps answers concise)\n",
    "- **STOP_SEQUENCES**: When to stop generating (e.g., end of text marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "# Select the model to use\n",
    "# Using Granite 3.2 8B Instruct - a powerful and efficient model\n",
    "model_id = \"ibm/granite-3-2-8b-instruct\"\n",
    "print(f\"ü§ñ Selected model: {model_id}\")\n",
    "\n",
    "# Configure model parameters\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,  # Deterministic output\n",
    "    GenParams.MIN_NEW_TOKENS: 1,                         # Generate at least 1 word\n",
    "    GenParams.MAX_NEW_TOKENS: 100,                       # Generate at most 100 words\n",
    "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]          # Stop at end of text marker\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Model parameters configured!\")\n",
    "print(\"\\nParameters:\")\n",
    "for key, value in parameters.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-llm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Create the LangChain wrapper for watsonx.ai\n",
    "print(\"üîß Initializing Granite model with LangChain...\")\n",
    "\n",
    "watsonx_granite = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Granite model ready to use!\")\n",
    "print(\"\\nYou can now ask questions about your document! üí¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict-section",
   "metadata": {},
   "source": [
    "<a id=\"predict\"></a>\n",
    "## Step 6: Ask Questions and Get Answers\n",
    "\n",
    "Now comes the exciting part! We'll create a **question-answering chain** that:\n",
    "\n",
    "1. Takes your question\n",
    "2. Searches the vector database for relevant chunks\n",
    "3. Sends those chunks to the Granite model\n",
    "4. Returns a natural language answer\n",
    "\n",
    "### About RetrievalQA\n",
    "\n",
    "RetrievalQA is a LangChain component that automates the RAG process:\n",
    "- **llm**: The language model to use (our Granite model)\n",
    "- **chain_type**: \"stuff\" means all retrieved chunks are sent together to the model\n",
    "- **retriever**: The search component (our Chroma database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-qa-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Create the question-answering chain\n",
    "print(\"üîó Creating the RAG question-answering chain...\")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=watsonx_granite,                    # Our Granite model\n",
    "    chain_type=\"stuff\",                     # Send all retrieved chunks to the model\n",
    "    retriever=docsearch.as_retriever()      # Our vector database retriever\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG system is ready!\")\n",
    "print(\"\\nüéâ You can now ask questions about the State of the Union address!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-questions-section",
   "metadata": {},
   "source": [
    "### Example Questions\n",
    "\n",
    "Let's try asking some questions about the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "question-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: About Ketanji Brown Jackson\n",
    "query = \"What did the president say about Ketanji Brown Jackson?\"\n",
    "\n",
    "print(f\"‚ùì Question: {query}\")\n",
    "print(\"\\nü§î Thinking...\\n\")\n",
    "\n",
    "result = qa.invoke(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üí° Answer:\")\n",
    "print(\"=\"*80)\n",
    "print(result['result'])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "question-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: About the economy\n",
    "query = \"What economic policies were mentioned?\"\n",
    "\n",
    "print(f\"‚ùì Question: {query}\")\n",
    "print(\"\\nü§î Thinking...\\n\")\n",
    "\n",
    "result = qa.invoke(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üí° Answer:\")\n",
    "print(\"=\"*80)\n",
    "print(result['result'])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "question-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: About Ukraine\n",
    "query = \"What was said about Ukraine?\"\n",
    "\n",
    "print(f\"‚ùì Question: {query}\")\n",
    "print(\"\\nü§î Thinking...\\n\")\n",
    "\n",
    "result = qa.invoke(query)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üí° Answer:\")\n",
    "print(\"=\"*80)\n",
    "print(result['result'])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-section",
   "metadata": {},
   "source": [
    "### Try Your Own Questions!\n",
    "\n",
    "Now it's your turn! Run the cell below and ask your own questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-qa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Q&A\n",
    "print(\"üí¨ Ask your own question about the State of the Union!\")\n",
    "print(\"   (Press Enter without typing anything to skip)\\n\")\n",
    "\n",
    "user_query = input(\"Your question: \")\n",
    "\n",
    "if user_query.strip():\n",
    "    print(f\"\\n‚ùì Question: {user_query}\")\n",
    "    print(\"\\nü§î Thinking...\\n\")\n",
    "    \n",
    "    result = qa.invoke(user_query)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üí° Answer:\")\n",
    "    print(\"=\"*80)\n",
    "    print(result['result'])\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No question entered. Try the next cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tips-section",
   "metadata": {},
   "source": [
    "## üí° Tips for Better Results\n",
    "\n",
    "### Ask Better Questions\n",
    "- ‚úÖ **Good**: \"What did the president say about climate change?\"\n",
    "- ‚ùå **Vague**: \"Tell me about stuff\"\n",
    "\n",
    "### Be Specific\n",
    "- ‚úÖ **Good**: \"What economic policies were mentioned for small businesses?\"\n",
    "- ‚ùå **Too broad**: \"What about the economy?\"\n",
    "\n",
    "### Understand the Limitations\n",
    "- The answer is based ONLY on the document you provided\n",
    "- If the information isn't in the document, the model may say \"I don't know\" or give a generic answer\n",
    "- The model can only see the most relevant chunks (not the entire document at once)\n",
    "\n",
    "## üîß Troubleshooting\n",
    "\n",
    "### \"No answer found\" or irrelevant answers?\n",
    "- Try rephrasing your question\n",
    "- Make sure the information exists in the document\n",
    "- Try increasing the chunk size or retrieval count\n",
    "\n",
    "### ChromaDB warnings?\n",
    "- These telemetry warnings are harmless and can be ignored\n",
    "- They don't affect functionality\n",
    "\n",
    "### Dependency conflicts?\n",
    "- These usually don't cause issues in Colab\n",
    "- If you encounter errors, try restarting the runtime and running all cells again\n",
    "\n",
    "## üöÄ Next Steps & Customization\n",
    "\n",
    "Now that you understand the basics, try these enhancements:\n",
    "\n",
    "### 1. Use Your Own Documents\n",
    "```python\n",
    "# Replace the document with your own\n",
    "filename = 'your_document.txt'\n",
    "# Then run the same pipeline!\n",
    "```\n",
    "\n",
    "### 2. Adjust Chunk Size\n",
    "```python\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=500,      # Smaller chunks\n",
    "    chunk_overlap=50     # Add overlap for better context\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Get More Context\n",
    "```python\n",
    "# Retrieve more chunks per question\n",
    "retriever = docsearch.as_retriever(search_kwargs={\"k\": 4})  # Get 4 chunks instead of default\n",
    "```\n",
    "\n",
    "### 4. Try Different Models\n",
    "```python\n",
    "# Use a different Granite model\n",
    "model_id = \"ibm/granite-3-8b-instruct\"\n",
    "```\n",
    "\n",
    "### 5. Adjust Generation Parameters\n",
    "```python\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE,  # More creative\n",
    "    GenParams.MAX_NEW_TOKENS: 200,                       # Longer answers\n",
    "    GenParams.TEMPERATURE: 0.7,                          # Add randomness\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## üéì Summary and Next Steps\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "Congratulations! You've successfully built a complete RAG system! üéâ\n",
    "\n",
    "You now know how to:\n",
    "\n",
    "1. ‚úÖ **Set up watsonx.ai** - Connect to IBM's AI platform\n",
    "2. ‚úÖ **Prepare documents** - Load and split text into searchable chunks\n",
    "3. ‚úÖ **Create embeddings** - Convert text to vector representations\n",
    "4. ‚úÖ **Build a vector database** - Store and search document embeddings with Chroma\n",
    "5. ‚úÖ **Configure LLMs** - Set up Granite models with custom parameters\n",
    "6. ‚úÖ **Build RAG pipelines** - Combine retrieval and generation with LangChain\n",
    "7. ‚úÖ **Ask questions** - Get accurate answers grounded in your documents\n",
    "\n",
    "### Key Concepts Review\n",
    "\n",
    "- **RAG (Retrieval-Augmented Generation)**: Combines document search with AI generation\n",
    "- **Embeddings**: Numerical representations of text that capture meaning\n",
    "- **Vector Database**: Searchable storage for embeddings (we used Chroma)\n",
    "- **LangChain**: Framework for building LLM applications\n",
    "- **Granite Models**: IBM's enterprise AI models on watsonx.ai\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "You can use this RAG system for:\n",
    "\n",
    "- üìö **Document Q&A**: Answer questions about company documents, manuals, reports\n",
    "- üíº **Customer Support**: Build chatbots that answer based on your knowledge base\n",
    "- üîç **Research**: Query large collections of papers or articles\n",
    "- üìñ **Education**: Create study assistants for textbooks and course materials\n",
    "- ‚öñÔ∏è **Legal/Compliance**: Search and query legal documents and regulations\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- üìò [watsonx.ai Documentation](https://ibm.github.io/watsonx-ai-python-sdk/)\n",
    "- üìó [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "- üìô [Chroma Documentation](https://docs.trychroma.com/)\n",
    "- üìï [More watsonx Samples](https://ibm.github.io/watsonx-ai-python-sdk/samples.html)\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Try these advanced topics:\n",
    "\n",
    "1. **Multi-document RAG**: Query across multiple documents\n",
    "2. **PDF Support**: Load and process PDF files\n",
    "3. **Web Scraping**: Build RAG from website content\n",
    "4. **Conversational RAG**: Add chat memory for multi-turn conversations\n",
    "5. **Advanced Retrieval**: Use re-ranking and hybrid search\n",
    "\n",
    "### Share Your Results!\n",
    "\n",
    "Built something cool with this tutorial? We'd love to hear about it!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "license",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìÑ License\n",
    "\n",
    "Copyright ¬© 2023, 2024 IBM. This notebook and its source code are released under the terms of the MIT License.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
