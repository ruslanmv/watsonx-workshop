{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3Q4a1hC0ArCW"
      },
      "source": [
        "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
        "# Use watsonx Granite Model Series, Chroma, and LangChain to answer questions (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "rH_BvR6cArCW"
      },
      "source": [
        "#### Disclaimers\n",
        "\n",
        "- Use only Projects and Spaces that are available in watsonx context.\n",
        "\n",
        "## Notebook content\n",
        "This notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n",
        "\n",
        "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
        "\n",
        "### About Retrieval Augmented Generation\n",
        "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
        "\n",
        "In its simplest form, RAG requires 3 steps:\n",
        "\n",
        "- Index knowledge base passages (once)\n",
        "- Retrieve relevant passage(s) from knowledge base (for every user query)\n",
        "- Generate a response by feeding retrieved passage into a large language model (for every user query)\n",
        "\n",
        "## Contents\n",
        "\n",
        "This notebook contains the following parts:\n",
        "\n",
        "- [Setup](#setup)\n",
        "- [Document data loading](#data)\n",
        "- [Build up knowledge base](#build_base)\n",
        "- [Foundation Models on watsonx](#models)\n",
        "- [Generate a retrieval-augmented response to a question](#predict)\n",
        "- [Summary and next steps](#summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zPUeUR3LArCX"
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "##  Set up the environment\n",
        "\n",
        "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
        "\n",
        "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "b6XQnr_CArCX"
      },
      "source": [
        "### Install and import the dependecies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-jVTe9IArCX",
        "outputId": "e5c101ec-765a-4646-b306-432b85ff6417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.12/dist-packages (3.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4,>=0.3) (1.3.1)\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kubernetes 34.1.0 requires urllib3<2.4.0,>=1.24.2, but you have urllib3 2.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "transformers 4.57.1 requires tokenizers<=0.23.0,>=0.22.0, but you have tokenizers 0.20.3 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed urllib3-2.5.0\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.37->langchain_ibm<0.4,>=0.3) (1.3.1)\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibm-cos-sdk-core 2.14.3 requires urllib3<3,>=2.5.0, but you have urllib3 2.3.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "transformers 4.57.1 requires tokenizers<=0.23.0,>=0.22.0, but you have tokenizers 0.20.3 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed urllib3-2.3.0\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community<0.4,>=0.3) (1.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.75->langchain_text_splitters<0.4,>=0.3) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget | tail -n 1\n",
        "!pip install -U \"langchain>=0.3,<0.4\" | tail -n 1\n",
        "!pip install -U \"ibm_watsonx_ai>=1.1.22\" | tail -n 1\n",
        "!pip install -U \"langchain_ibm>=0.3,<0.4\" | tail -n 1\n",
        "!pip install -U \"langchain_chroma>=0.1,<0.2\" | tail -n 1\n",
        "!pip install -U \"langchain_community>=0.3,<0.4\" | tail -n 1\n",
        "!pip install -U \"langchain_text_splitters>=0.3,<0.4\" | tail -n 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "o74QmQBDArCX"
      },
      "outputs": [],
      "source": [
        "import os, getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "tcsaWf64ArCX"
      },
      "source": [
        "### watsonx API connection\n",
        "This cell defines the credentials required to work with watsonx API for Foundation\n",
        "Model inferencing.\n",
        "\n",
        "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1Od6FzeArCX",
        "outputId": "e6161142-b353-482c-9261-0b25f0b0c8e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your WML api key (hit enter): ··········\n"
          ]
        }
      ],
      "source": [
        "from ibm_watsonx_ai import Credentials\n",
        "\n",
        "credentials = Credentials(\n",
        "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "    api_key=getpass.getpass(\"Please enter your WML api key (hit enter): \"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1TzlJnacArCX"
      },
      "source": [
        "### Defining the project id\n",
        "The API requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id.\n",
        "\n",
        "**Hint**: You can find the `project_id` as follows. Open the prompt lab in watsonx.ai. At the very top of the UI, there will be `Projects / <project name> /`. Click on the `<project name>` link. Then get the `project_id` from Project's Manage tab (Project -> Manage -> General -> Details).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH1cZNC1ArCY",
        "outputId": "9e3a5bae-10e5-4791-d596-76efcc68f56f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your project_id (hit enter): 1e18ec27-f68f-45af-873b-5c8a11968b2f\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    project_id = os.environ[\"PROJECT_ID\"]\n",
        "except KeyError:\n",
        "    project_id = input(\"Please enter your project_id (hit enter): \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMx-qt2NArCY"
      },
      "source": [
        "Create an instance of APIClient with authentication details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XOQTtxrkArCY"
      },
      "outputs": [],
      "source": [
        "from ibm_watsonx_ai import APIClient\n",
        "\n",
        "api_client = APIClient(credentials=credentials, project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "STI6K5tBArCY"
      },
      "source": [
        "<a id=\"data\"></a>\n",
        "## Document data loading\n",
        "\n",
        "Download the file with State of the Union."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "agdIKUSCArCY"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "\n",
        "filename = 'state_of_the_union.txt'\n",
        "url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
        "\n",
        "if not os.path.isfile(filename):\n",
        "    wget.download(url, out=filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LZBpbCE3ArCY"
      },
      "source": [
        "<a id=\"build_base\"></a>\n",
        "## Build up knowledge base\n",
        "\n",
        "The most common approach in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
        "\n",
        "In this basic example, we take the State of the Union speech content (filename), split it into chunks, embed it using an open-source embedding model, load it into <a href=\"https://www.trychroma.com/\" target=\"_blank\" rel=\"noopener no referrer\">Chroma</a>, and then query it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3h0_VzbkArCY"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "loader = TextLoader(filename)\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ZxsNPO2MArCY"
      },
      "source": [
        "The dataset we are using is already split into self-contained passages that can be ingested by Chroma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "eP1VigKVArCY"
      },
      "source": [
        "### Create an embedding function\n",
        "\n",
        "Note that you can feed a custom embedding function to be used by chromadb. The performance of Chroma db may differ depending on the embedding model used. In following example we use watsonx.ai Embedding service. We can check available embedding models using `get_embedding_model_specs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF-vHGWEArCY",
        "outputId": "29959086-c5ed-4ae5-98ae-b9cd296ad94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'GRANITE_EMBEDDING_278M_MULTILINGUAL': 'ibm/granite-embedding-278m-multilingual', 'SLATE_125M_ENGLISH_RTRVR_V2': 'ibm/slate-125m-english-rtrvr-v2', 'SLATE_30M_ENGLISH_RTRVR_V2': 'ibm/slate-30m-english-rtrvr-v2', 'MULTILINGUAL_E5_LARGE': 'intfloat/multilingual-e5-large', 'ALL_MINILM_L6_V2': 'sentence-transformers/all-minilm-l6-v2'}\n"
          ]
        }
      ],
      "source": [
        "api_client.foundation_models.EmbeddingModels.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Mm4FHbgArCY",
        "outputId": "014a538e-55fa-4905-8c77-7fc27551a97b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        }
      ],
      "source": [
        "from langchain_ibm import WatsonxEmbeddings\n",
        "\n",
        "embeddings = WatsonxEmbeddings(\n",
        "    model_id=\"sentence-transformers/all-minilm-l6-v2\",\n",
        "    url=credentials[\"url\"],\n",
        "    apikey=credentials[\"apikey\"],\n",
        "    project_id=project_id\n",
        "    )\n",
        "docsearch = Chroma.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFgN5w6UArCY"
      },
      "source": [
        "#### Compatibility watsonx.ai Embeddings with LangChain\n",
        "\n",
        " LangChain retrievals use `embed_documents` and `embed_query` under the hood to generate embedding vectors for uploaded documents and user query respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn1e7XwdArCY",
        "outputId": "030d6c50-94b6-41bd-dca5-89ce7d49037c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class WatsonxEmbeddings in module langchain_ibm.embeddings:\n",
            "\n",
            "class WatsonxEmbeddings(pydantic.main.BaseModel, langchain_core.embeddings.embeddings.Embeddings)\n",
            " |  WatsonxEmbeddings(*, model_id: str | None = None, model: str | None = None, project_id: str | None = None, space_id: str | None = None, url: pydantic.types.SecretStr = <factory>, apikey: pydantic.types.SecretStr | None = <factory>, token: pydantic.types.SecretStr | None = <factory>, password: pydantic.types.SecretStr | None = <factory>, username: pydantic.types.SecretStr | None = <factory>, instance_id: pydantic.types.SecretStr | None = <factory>, version: pydantic.types.SecretStr | None = None, params: dict | None = None, verify: str | bool | None = None, watsonx_embed: ibm_watsonx_ai.foundation_models.embeddings.embeddings.Embeddings = None, watsonx_embed_gateway: ibm_watsonx_ai.gateway.gateway.Gateway = None, watsonx_client: ibm_watsonx_ai.client.APIClient | None = None) -> None\n",
            " |\n",
            " |  `IBM watsonx.ai` embedding model integration.\n",
            " |\n",
            " |  ???+ info \"Setup\"\n",
            " |\n",
            " |      To use, you should have `langchain_ibm` python package installed,\n",
            " |      and the environment variable `WATSONX_APIKEY` set with your API key, or pass\n",
            " |      it as a named parameter `apikey` to the constructor.\n",
            " |\n",
            " |      ```bash\n",
            " |      pip install -U langchain-ibm\n",
            " |\n",
            " |      # or using uv\n",
            " |      uv add langchain-ibm\n",
            " |      ```\n",
            " |\n",
            " |      ```bash\n",
            " |      export WATSONX_APIKEY=\"your-api-key\"\n",
            " |      ```\n",
            " |\n",
            " |  ??? info \"Instantiate\"\n",
            " |\n",
            " |      ```python\n",
            " |      from langchain_ibm import WatsonxEmbeddings\n",
            " |\n",
            " |      embeddings = WatsonxEmbeddings(\n",
            " |          model_id=\"ibm/granite-embedding-278m-multilingual\",\n",
            " |          url=\"https://us-south.ml.cloud.ibm.com\",\n",
            " |          project_id=\"*****\",\n",
            " |          # apikey=\"*****\"\n",
            " |      )\n",
            " |      ```\n",
            " |\n",
            " |  ??? info \"Embed single text\"\n",
            " |\n",
            " |      ```python\n",
            " |      input_text = \"The meaning of life is 42\"\n",
            " |      vector = embeddings.embed_query(\"hello\")\n",
            " |      print(vector[:3])\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      [-0.0020519258, 0.0147288125, -0.0090887165]\n",
            " |      ```\n",
            " |\n",
            " |  ??? info \"Embed multiple texts\"\n",
            " |\n",
            " |      ```python\n",
            " |      vectors = embeddings.embed_documents([\"hello\", \"goodbye\"])\n",
            " |      # Showing only the first 3 coordinates\n",
            " |      print(len(vectors))\n",
            " |      print(vectors[0][:3])\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      2\n",
            " |      [-0.0020519265, 0.01472881, -0.009088721]\n",
            " |      ```\n",
            " |\n",
            " |  ??? info \"Async\"\n",
            " |\n",
            " |      ```python\n",
            " |      await embeddings.aembed_query(input_text)\n",
            " |      print(vector[:3])\n",
            " |\n",
            " |      # multiple:\n",
            " |      # await embeddings.aembed_documents(input_texts)\n",
            " |      ```\n",
            " |\n",
            " |      ```python\n",
            " |      [-0.0020519258, 0.0147288125, -0.0090887165]\n",
            " |      ```\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      WatsonxEmbeddings\n",
            " |      pydantic.main.BaseModel\n",
            " |      langchain_core.embeddings.embeddings.Embeddings\n",
            " |      abc.ABC\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  async aembed_documents(self, texts: list[str], **kwargs: Any) -> list[list[float]]\n",
            " |      Asynchronous Embed search docs.\n",
            " |\n",
            " |  async aembed_query(self, text: str, **kwargs: Any) -> list[float]\n",
            " |      Asynchronous Embed query text.\n",
            " |\n",
            " |  embed_documents(self, texts: list[str], **kwargs: Any) -> list[list[float]]\n",
            " |      Embed search docs.\n",
            " |\n",
            " |  embed_query(self, text: str, **kwargs: Any) -> list[float]\n",
            " |      Embed query text.\n",
            " |\n",
            " |  validate_environment(self) -> Self\n",
            " |      Validate that credentials and python package exists in environment.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __abstractmethods__ = frozenset()\n",
            " |\n",
            " |  __annotations__ = {'apikey': pydantic.types.SecretStr | None, 'instanc...\n",
            " |\n",
            " |  __class_vars__ = set()\n",
            " |\n",
            " |  __private_attributes__ = {}\n",
            " |\n",
            " |  __pydantic_complete__ = True\n",
            " |\n",
            " |  __pydantic_computed_fields__ = {}\n",
            " |\n",
            " |  __pydantic_core_schema__ = {'function': {'function': <function Watsonx...\n",
            " |\n",
            " |  __pydantic_custom_init__ = False\n",
            " |\n",
            " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
            " |\n",
            " |  __pydantic_fields__ = {'apikey': FieldInfo(annotation=Union[SecretStr,...\n",
            " |\n",
            " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
            " |\n",
            " |  __pydantic_parent_namespace__ = None\n",
            " |\n",
            " |  __pydantic_post_init__ = None\n",
            " |\n",
            " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
            " |      Model...\n",
            " |\n",
            " |  __pydantic_setattr_handlers__ = {'watsonx_embed': <function _model_fie...\n",
            " |\n",
            " |  __pydantic_validator__ = SchemaValidator(title=\"WatsonxEmbeddings\", va...\n",
            " |\n",
            " |  __signature__ = <Signature (*, model_id: str | None = None, mode...tso...\n",
            " |\n",
            " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'forbid', 'p...\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __copy__(self) -> 'Self'\n",
            " |      Returns a shallow copy of the model.\n",
            " |\n",
            " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
            " |      Returns a deep copy of the model.\n",
            " |\n",
            " |  __delattr__(self, item: 'str') -> 'Any'\n",
            " |      Implement delattr(self, name).\n",
            " |\n",
            " |  __eq__(self, other: 'Any') -> 'bool'\n",
            " |      Return self==value.\n",
            " |\n",
            " |  __getattr__(self, item: 'str') -> 'Any'\n",
            " |\n",
            " |  __getstate__(self) -> 'dict[Any, Any]'\n",
            " |      Helper for pickle.\n",
            " |\n",
            " |  __init__(self, /, **data: 'Any') -> 'None'\n",
            " |      Create a new model by parsing and validating input data from keyword arguments.\n",
            " |\n",
            " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
            " |      validated to form a valid model.\n",
            " |\n",
            " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
            " |\n",
            " |  __iter__(self) -> 'TupleGenerator'\n",
            " |      So `dict(model)` works.\n",
            " |\n",
            " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
            " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
            " |\n",
            " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
            " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
            " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
            " |\n",
            " |  __repr__(self) -> 'str'\n",
            " |      Return repr(self).\n",
            " |\n",
            " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
            " |\n",
            " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
            " |      Name of the instance's class, used in __repr__.\n",
            " |\n",
            " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
            " |      Returns the string representation of a recursive object.\n",
            " |\n",
            " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
            " |\n",
            " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
            " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
            " |\n",
            " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
            " |      Implement setattr(self, name, value).\n",
            " |\n",
            " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
            " |\n",
            " |  __str__(self) -> 'str'\n",
            " |      Return str(self).\n",
            " |\n",
            " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
            " |      Returns a copy of the model.\n",
            " |\n",
            " |      !!! warning \"Deprecated\"\n",
            " |          This method is now deprecated; use `model_copy` instead.\n",
            " |\n",
            " |      If you need `include` or `exclude`, use:\n",
            " |\n",
            " |      ```python {test=\"skip\" lint=\"skip\"}\n",
            " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
            " |      data = {**data, **(update or {})}\n",
            " |      copied = self.model_validate(data)\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
            " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
            " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
            " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
            " |\n",
            " |      Returns:\n",
            " |          A copy of the model with included, excluded and updated fields as specified.\n",
            " |\n",
            " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
            " |\n",
            " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
            " |\n",
            " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [`model_copy`](../concepts/serialization.md#model_copy)\n",
            " |\n",
            " |      Returns a copy of the model.\n",
            " |\n",
            " |      !!! note\n",
            " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
            " |          might have unexpected side effects if you store anything in it, on top of the model\n",
            " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
            " |\n",
            " |      Args:\n",
            " |          update: Values to change/add in the new model. Note: the data is not validated\n",
            " |              before creating the new model. You should trust this data.\n",
            " |          deep: Set to `True` to make a deep copy of the model.\n",
            " |\n",
            " |      Returns:\n",
            " |          New model instance.\n",
            " |\n",
            " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [`model_dump`](../concepts/serialization.md#modelmodel_dump)\n",
            " |\n",
            " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
            " |\n",
            " |      Args:\n",
            " |          mode: The mode in which `to_python` should run.\n",
            " |              If mode is 'json', the output will only contain JSON serializable types.\n",
            " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
            " |          include: A set of fields to include in the output.\n",
            " |          exclude: A set of fields to exclude from the output.\n",
            " |          context: Additional context to pass to the serializer.\n",
            " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
            " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
            " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
            " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
            " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
            " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
            " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
            " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
            " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
            " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
            " |\n",
            " |      Returns:\n",
            " |          A dictionary representation of the model.\n",
            " |\n",
            " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'str'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [`model_dump_json`](../concepts/serialization.md#modelmodel_dump_json)\n",
            " |\n",
            " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
            " |\n",
            " |      Args:\n",
            " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
            " |          include: Field(s) to include in the JSON output.\n",
            " |          exclude: Field(s) to exclude from the JSON output.\n",
            " |          context: Additional context to pass to the serializer.\n",
            " |          by_alias: Whether to serialize using field aliases.\n",
            " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
            " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
            " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
            " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
            " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
            " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
            " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
            " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
            " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON string representation of the model.\n",
            " |\n",
            " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
            " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
            " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
            " |\n",
            " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
            " |\n",
            " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
            " |      Hook into generating the model's JSON schema.\n",
            " |\n",
            " |      Args:\n",
            " |          core_schema: A `pydantic-core` CoreSchema.\n",
            " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
            " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
            " |              or just call the handler with the original schema.\n",
            " |          handler: Call into Pydantic's internal JSON schema generation.\n",
            " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
            " |              generation fails.\n",
            " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
            " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
            " |              for a type.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON schema, as a Python object.\n",
            " |\n",
            " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
            " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
            " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
            " |      be present when this is called.\n",
            " |\n",
            " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
            " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
            " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
            " |\n",
            " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
            " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
            " |\n",
            " |      Args:\n",
            " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
            " |              by pydantic.\n",
            " |\n",
            " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
            " |\n",
            " |  from_orm(obj: 'Any') -> 'Self'\n",
            " |\n",
            " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
            " |      Creates a new instance of the `Model` class with validated data.\n",
            " |\n",
            " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
            " |      Default values are respected, but no other validation is performed.\n",
            " |\n",
            " |      !!! note\n",
            " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
            " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
            " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
            " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
            " |          an error if extra values are passed, but they will be ignored.\n",
            " |\n",
            " |      Args:\n",
            " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
            " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
            " |              Otherwise, the field names from the `values` argument will be used.\n",
            " |          values: Trusted or pre-validated data dictionary.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new instance of the `Model` class with validated data.\n",
            " |\n",
            " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
            " |      Generates a JSON schema for a model class.\n",
            " |\n",
            " |      Args:\n",
            " |          by_alias: Whether to use attribute aliases or not.\n",
            " |          ref_template: The reference template.\n",
            " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
            " |              `GenerateJsonSchema` with your desired modifications\n",
            " |          mode: The mode in which to generate the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          The JSON schema for the given model class.\n",
            " |\n",
            " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
            " |      Compute the class name for parametrizations of generic classes.\n",
            " |\n",
            " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
            " |\n",
            " |      Args:\n",
            " |          params: Tuple of types of the class. Given a generic class\n",
            " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
            " |              the value `(str, int)` would be passed to `params`.\n",
            " |\n",
            " |      Returns:\n",
            " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
            " |\n",
            " |      Raises:\n",
            " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
            " |\n",
            " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None'\n",
            " |      Try to rebuild the pydantic-core schema for the model.\n",
            " |\n",
            " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
            " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
            " |\n",
            " |      Args:\n",
            " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
            " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
            " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
            " |          _types_namespace: The types namespace, defaults to `None`.\n",
            " |\n",
            " |      Returns:\n",
            " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
            " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
            " |\n",
            " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
            " |      Validate a pydantic model instance.\n",
            " |\n",
            " |      Args:\n",
            " |          obj: The object to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          from_attributes: Whether to extract data from object attributes.\n",
            " |          context: Additional context to pass to the validator.\n",
            " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
            " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValidationError: If the object could not be validated.\n",
            " |\n",
            " |      Returns:\n",
            " |          The validated model instance.\n",
            " |\n",
            " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
            " |\n",
            " |      Validate the given JSON data against the Pydantic model.\n",
            " |\n",
            " |      Args:\n",
            " |          json_data: The JSON data to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          context: Extra variables to pass to the validator.\n",
            " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
            " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
            " |\n",
            " |      Returns:\n",
            " |          The validated Pydantic model.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
            " |\n",
            " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
            " |      Validate the given object with string data against the Pydantic model.\n",
            " |\n",
            " |      Args:\n",
            " |          obj: The object containing string data to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          context: Extra variables to pass to the validator.\n",
            " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
            " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
            " |\n",
            " |      Returns:\n",
            " |          The validated Pydantic model.\n",
            " |\n",
            " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
            " |\n",
            " |  parse_obj(obj: 'Any') -> 'Self'\n",
            " |\n",
            " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
            " |\n",
            " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
            " |\n",
            " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
            " |\n",
            " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
            " |\n",
            " |  validate(value: 'Any') -> 'Self'\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __fields_set__\n",
            " |\n",
            " |  model_extra\n",
            " |      Get extra fields set during validation.\n",
            " |\n",
            " |      Returns:\n",
            " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
            " |\n",
            " |  model_fields_set\n",
            " |      Returns the set of fields that have been explicitly set on this model instance.\n",
            " |\n",
            " |      Returns:\n",
            " |          A set of strings representing the fields that have been set,\n",
            " |              i.e. that were not filled from defaults.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __pydantic_extra__\n",
            " |\n",
            " |  __pydantic_fields_set__\n",
            " |\n",
            " |  __pydantic_private__\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __hash__ = None\n",
            " |\n",
            " |  __pydantic_root_model__ = False\n",
            " |\n",
            " |  model_computed_fields = {}\n",
            " |\n",
            " |  model_fields = {'apikey': FieldInfo(annotation=Union[SecretStr, NoneTy...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(WatsonxEmbeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "o9fSPA95ArCY"
      },
      "source": [
        "<a id=\"models\"></a>\n",
        "## Foundation Models on `watsonx.ai`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRPve91FArCY"
      },
      "source": [
        "IBM watsonx foundation models are among the <a href=\"https://python.langchain.com/docs/integrations/llms/watsonxllm\" target=\"_blank\" rel=\"noopener no referrer\">list of LLM models supported by Langchain</a>. This example shows how to communicate with <a href=\"https://newsroom.ibm.com/2023-09-28-IBM-Announces-Availability-of-watsonx-Granite-Model-Series,-Client-Protections-for-IBM-watsonx-Models\" target=\"_blank\" rel=\"noopener no referrer\">Granite Model Series</a> using <a href=\"https://python.langchain.com/docs/get_started/introduction\" target=\"_blank\" rel=\"noopener no referrer\">Langchain</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Qz-CqEaGArCY"
      },
      "source": [
        "### Defining model\n",
        "You need to specify `model_id` that will be used for inferencing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ybJlvb3dArCY"
      },
      "outputs": [],
      "source": [
        "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
        "\n",
        "model_id = ModelTypes.GRANITE_13B_CHAT_V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "akq6m_OTArCZ"
      },
      "source": [
        "### Defining the model parameters\n",
        "We need to provide a set of model parameters that will influence the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "B2Xk8jn9ArCZ"
      },
      "outputs": [],
      "source": [
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
        "\n",
        "parameters = {\n",
        "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
        "    GenParams.MIN_NEW_TOKENS: 1,\n",
        "    GenParams.MAX_NEW_TOKENS: 100,\n",
        "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7nOgzdbwC9UG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 'ibm/granite-13b-chat-v2' is not supported for this environment. Supported models: ['cross-encoder/ms-marco-minilm-l-12-v2', 'ibm/granite-3-1-8b-base', 'ibm/granite-3-2-8b-instruct', 'ibm/granite-3-3-8b-instruct', 'ibm/granite-3-8b-instruct', 'ibm/granite-4-h-small', 'ibm/granite-8b-code-instruct', 'ibm/granite-embedding-278m-multilingual', 'ibm/granite-guardian-3-8b', 'ibm/granite-ttm-1024-96-r2', 'ibm/granite-ttm-1536-96-r2', 'ibm/granite-ttm-512-96-r2', 'ibm/slate-125m-english-rtrvr-v2', 'ibm/slate-30m-english-rtrvr-v2', 'intfloat/multilingual-e5-large', 'meta-llama/llama-3-1-70b-gptq', 'meta-llama/llama-3-1-8b', 'meta-llama/llama-3-2-11b-vision-instruct', 'meta-llama/llama-3-2-90b-vision-instruct', 'meta-llama/llama-3-3-70b-instruct', 'meta-llama/llama-3-405b-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct-fp8', 'meta-llama/llama-guard-3-11b-vision', 'mistralai/mistral-medium-2505', 'mistralai/mistral-small-3-1-24b-instruct-2503', 'openai/gpt-oss-120b', 'sentence-transformers/all-minilm-l6-v2']"
      ],
      "metadata": {
        "id": "JPdqVpBqC-CF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB_BCraFArCZ"
      },
      "source": [
        "### LangChain CustomLLM wrapper for watsonx model\n",
        "Initialize the `WatsonxLLM` class from Langchain with defined parameters and `ibm/granite-13b-chat-v2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "t0tPSV1IArCZ"
      },
      "outputs": [],
      "source": [
        "from langchain_ibm import WatsonxLLM\n",
        "\n",
        "watsonx_granite = WatsonxLLM(\n",
        "    model_id=\"ibm/granite-3-2-8b-instruct\",\n",
        "    url=credentials.get(\"url\"),\n",
        "    apikey=credentials.get(\"apikey\"),\n",
        "    project_id=project_id,\n",
        "    params=parameters\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BJKXOG-2ArCZ"
      },
      "source": [
        "<a id=\"predict\"></a>\n",
        "## Generate a retrieval-augmented response to a question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opCdOi6BArCZ"
      },
      "source": [
        "Build the `RetrievalQA` (question answering chain) to automate the RAG task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3eHBhRLiArCZ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jC6XllulArCZ"
      },
      "source": [
        "### Select questions\n",
        "\n",
        "Get questions from the previously loaded test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtCH_2BrArCZ",
        "outputId": "702cee72-6a53-4ec2-ed5a-733288cda480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
              " 'result': \"\\n\\nThe president nominated Ketanji Brown Jackson, a Circuit Court of Appeals Judge, to serve on the United States Supreme Court. He described her as one of the nation's top legal minds and a consensus builder, with a background as a former top litigator in private practice and a federal public defender. She comes from a family of public school educators and police officers. Since her nomination, she has\"}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "qa.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ode50e-iArCZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "lZsIGvh8ArCZ"
      },
      "source": [
        "<a id=\"summary\"></a>\n",
        "## Summary and next steps\n",
        "\n",
        " You successfully completed this notebook!.\n",
        "\n",
        " You learned how to answer question using RAG using watsonx and LangChain.\n",
        "\n",
        "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "L0ucgqVbArCZ"
      },
      "source": [
        "Copyright © 2023, 2024 IBM. This notebook and its source code are released under the terms of the MIT License."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "notebook-samples",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}