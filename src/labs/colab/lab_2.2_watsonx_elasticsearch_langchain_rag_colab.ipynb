{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2 â€“ RAG with Watsonx, Elasticsearch, and LangChain (Google Colab Version)\n",
    "\n",
    "![watsonx](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Run this notebook in Google Colab\n",
    "\n",
    "**Prerequisites:**\n",
    "- IBM Cloud API Key ([Create one here](https://cloud.ibm.com/iam/apikeys))\n",
    "- Watsonx Space ID ([Create a deployment space](https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=wx))\n",
    "- **Elasticsearch Cloud Endpoint** (see setup instructions below)\n",
    "\n",
    "âš ï¸ **Important:** This notebook requires an **Elasticsearch Cloud** instance. You can:\n",
    "- Use [Elastic Cloud](https://cloud.elastic.co/) (14-day free trial available)\n",
    "- Use [IBM Cloud Databases for Elasticsearch](https://cloud.ibm.com/catalog/services/databases-for-elasticsearch)\n",
    "\n",
    "This notebook demonstrates **Retrieval Augmented Generation (RAG)** using:\n",
    "- **Watsonx.ai** for LLM inference\n",
    "- **Elasticsearch** as vector database\n",
    "- **LangChain** for RAG orchestration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "!pip install -qU wget\n",
    "!pip install -qU pandas\n",
    "!pip install -qU humanize\n",
    "!pip install -qU \"langchain>=0.3,<0.4\"\n",
    "!pip install -qU \"ibm_watsonx_ai>=1.1.22\"\n",
    "!pip install -qU \"langchain_ibm>=0.3,<0.4\"\n",
    "!pip install -qU \"langchain-huggingface>=0.1,<0.2\"\n",
    "!pip install -qU \"langchain-elasticsearch>=0.3,<0.4\"\n",
    "!pip install -qU sentence-transformers\n",
    "\n",
    "print(\"âœ… All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Watsonx Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import humanize\n",
    "import random\n",
    "\n",
    "# Prompt for Watsonx credentials\n",
    "watsonx_api_key = getpass.getpass(\"Enter IBM Cloud API Key: \")\n",
    "space_id = getpass.getpass(\"Enter Watsonx Space ID (not project ID): \")\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n",
    "os.environ[\"WATSONX_APIKEY\"] = watsonx_api_key\n",
    "os.environ[\"SPACE_ID\"] = space_id\n",
    "\n",
    "print(\"âœ… Watsonx credentials configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=url,\n",
    "    api_key=watsonx_api_key,\n",
    ")\n",
    "\n",
    "api_client = APIClient(credentials=credentials, space_id=space_id)\n",
    "\n",
    "print(\"âœ… Watsonx API Client initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Elasticsearch Credentials\n",
    "\n",
    "You'll need:\n",
    "- Elasticsearch URL (e.g., `https://my-cluster.es.us-east-1.aws.found.io:9243`)\n",
    "- Username (usually `elastic`)\n",
    "- Password\n",
    "- SSL Certificate (base64 encoded) - Optional for Elastic Cloud\n",
    "\n",
    "**Note:** For Elastic Cloud, you can often skip the SSL certificate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticsearch_url = input(\"Enter Elasticsearch URL (e.g., https://xxx.es.cloud.es.io:9243): \")\n",
    "elasticsearch_username = input(\"Enter Elasticsearch username (usually 'elastic'): \")\n",
    "elasticsearch_password = getpass.getpass(\"Enter Elasticsearch password: \")\n",
    "\n",
    "print(\"\\nâœ… Elasticsearch credentials configured!\")\n",
    "print(\"\\nâ„¹ï¸  For SSL certificate, you can leave it empty if using Elastic Cloud.\")\n",
    "ssl_certificate_input = input(\"Enter SSL certificate (base64) or press Enter to skip: \")\n",
    "ssl_certificate = ssl_certificate_input if ssl_certificate_input else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "questions_test_filename = 'questions_test.csv'\n",
    "questions_train_filename = 'questions_train.csv'\n",
    "questions_test_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/questions_test.csv'\n",
    "questions_train_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/questions_train.csv'\n",
    "\n",
    "if not os.path.isfile(questions_test_filename): \n",
    "    wget.download(questions_test_url, out=questions_test_filename)\n",
    "\n",
    "if not os.path.isfile(questions_train_filename): \n",
    "    wget.download(questions_train_url, out=questions_train_filename)\n",
    "\n",
    "test_data = pd.read_csv(questions_test_filename)\n",
    "train_data = pd.read_csv(questions_train_filename)\n",
    "\n",
    "print(\"\\nâœ… Test data downloaded!\")\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Download Knowledge Base Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_dir = \"./knowledge_base\"\n",
    "\n",
    "os.makedirs(knowledge_base_dir, exist_ok=True)\n",
    "   \n",
    "documents_filename = 'knowledge_base/psgs.tsv'\n",
    "documents_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/psgs.tsv'\n",
    "\n",
    "if not os.path.isfile(documents_filename): \n",
    "    wget.download(documents_url, out=documents_filename)\n",
    "\n",
    "documents = pd.read_csv(f\"{knowledge_base_dir}/psgs.tsv\", sep='\\t', header=0, nrows=1000)\n",
    "documents['indextext'] = documents['title'].astype(str) + \"\\n\" + documents['text']\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(documents)} documents for knowledge base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create LangChain Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "lc_documents = [\n",
    "    Document(page_content=text, metadata={\"id\": doc_id}) \n",
    "    for text, doc_id in zip(documents['indextext'], documents['id'])\n",
    "]\n",
    "\n",
    "print(f\"âœ… Created {len(lc_documents)} LangChain documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Embedding Function\n",
    "\n",
    "We use HuggingFace's `all-MiniLM-L6-v2` model for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "emb_func = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"âœ… Embedding function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Initialize Watsonx LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "text_model_id = api_client.foundation_models.TextModels.FLAN_UL2\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 50\n",
    "}\n",
    "\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=text_model_id,\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    space_id=space_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "print(\"âœ… Watsonx LLM initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create Elasticsearch Connection\n",
    "\n",
    "This creates a connection asset in Watsonx that can be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticsearch_data_source_type_id = api_client.connections.get_datasource_type_uid_by_name(\n",
    "    \"elasticsearch\"\n",
    ")\n",
    "\n",
    "connection_properties = {\n",
    "    \"url\": elasticsearch_url,\n",
    "    \"username\": elasticsearch_username,\n",
    "    \"password\": elasticsearch_password,\n",
    "    \"use_anonymous_access\": \"false\",\n",
    "}\n",
    "\n",
    "if ssl_certificate:\n",
    "    connection_properties[\"ssl_certificate\"] = ssl_certificate\n",
    "\n",
    "connections_details = api_client.connections.create(\n",
    "    {\n",
    "        api_client.connections.ConfigurationMetaNames.NAME: \"Elasticsearch Connection (Colab)\",\n",
    "        api_client.connections.ConfigurationMetaNames.DESCRIPTION: \"Connection created from Colab notebook\",\n",
    "        api_client.connections.ConfigurationMetaNames.DATASOURCE_TYPE: elasticsearch_data_source_type_id,\n",
    "        api_client.connections.ConfigurationMetaNames.PROPERTIES: connection_properties,\n",
    "    }\n",
    ")\n",
    "\n",
    "elasticsearch_connection_id = api_client.connections.get_id(connections_details)\n",
    "\n",
    "print(f\"âœ… Elasticsearch connection created: {elasticsearch_connection_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Create Elasticsearch Index and Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from ibm_watsonx_ai.foundation_models.extensions.rag.vector_stores.vector_store import VectorStore\n",
    "\n",
    "index_name = f\"elastic_index_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}\"\n",
    "\n",
    "knowledge_base = VectorStore(\n",
    "    api_client,\n",
    "    connection_id=elasticsearch_connection_id,\n",
    "    embeddings=emb_func,\n",
    "    index_name=index_name,\n",
    ")\n",
    "\n",
    "elasticsearch_client = knowledge_base.get_client().client\n",
    "\n",
    "print(f\"âœ… Elasticsearch index created: {index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Embed and Index Documents\n",
    "\n",
    "âš ï¸ **This may take several minutes** to embed 1000 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indexing documents... This may take a few minutes.\")\n",
    "stored_documents = knowledge_base.add_documents(content=lc_documents)\n",
    "\n",
    "doc_count = knowledge_base.count()\n",
    "print(f\"\\nâœ… Indexed {doc_count} documents into Elasticsearch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Verify Index Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show index structure\n",
    "index_info = dict(elasticsearch_client.indices.get(index=index_name))\n",
    "print(\"Index mapping:\")\n",
    "print(index_info[index_name]['mappings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Create RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=watsonx_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=knowledge_base.as_langchain_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"âœ… RAG chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_and_answers = {\n",
    "    \"names of founding fathers of the united states?\": \"Thomas Jefferson::James Madison::John Jay::George Washington::John Adams::Benjamin Franklin::Alexander Hamilton\",\n",
    "    \"who played in the super bowl in 2013?\": \"Baltimore Ravens::San Francisco 49ers\",\n",
    "    \"when did bucharest become the capital of romania?\": \"1862\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for question in questions_and_answers.keys():\n",
    "    result = qa.invoke({\"query\": question})\n",
    "    results.append(result)\n",
    "\n",
    "print(\"âœ… Generated answers for test questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, result in enumerate(results):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Question: {result['query']}\")\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    print(f\"Expected: {questions_and_answers[result['query']]}\")\n",
    "    print(\"\\nSource documents:\")\n",
    "    for i, doc in enumerate(result['source_documents'][:2], 1):\n",
    "        print(f\"  {i}. {doc.page_content[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Try Your Own Questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_question = input(\"Enter your question: \")\n",
    "result = qa.invoke({\"query\": your_question})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Question: {result['query']}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"\\nRelevant sources:\")\n",
    "for i, doc in enumerate(result['source_documents'][:3], 1):\n",
    "    print(f\"\\n{i}. {doc.page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You successfully completed this notebook! You learned how to:\n",
    "\n",
    "âœ… Set up Elasticsearch Cloud connection from Google Colab  \n",
    "âœ… Create embeddings using HuggingFace models  \n",
    "âœ… Index documents into Elasticsearch vector database  \n",
    "âœ… Implement RAG using LangChain with Watsonx and Elasticsearch  \n",
    "\n",
    "**Next Steps:**\n",
    "- Try different embedding models\n",
    "- Experiment with different LLM models from Watsonx\n",
    "- Index your own documents\n",
    "- Deploy as an AI service (see original notebook for deployment code)\n",
    "\n",
    "For more information:\n",
    "- [Watsonx.ai Documentation](https://ibm.github.io/watsonx-ai-python-sdk/samples.html)\n",
    "- [Elasticsearch Documentation](https://www.elastic.co/guide/index.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Copyright Â© 2024 IBM. This notebook and its source code are released under the terms of the MIT License.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
