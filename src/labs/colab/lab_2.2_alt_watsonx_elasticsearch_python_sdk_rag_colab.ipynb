{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2 Alternative ‚Äì RAG with Watsonx and Elasticsearch Python SDK (Google Colab Version)\n",
    "\n",
    "![watsonx](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Run this notebook in Google Colab\n",
    "\n",
    "**Prerequisites:**\n",
    "- IBM Cloud API Key ([Create one here](https://cloud.ibm.com/iam/apikeys))\n",
    "- Watsonx Project ID ([Find it in your watsonx.ai project](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp))\n",
    "- **Elasticsearch Cloud Endpoint** (see setup instructions below)\n",
    "\n",
    "‚ö†Ô∏è **Important:** This notebook requires an **Elasticsearch Cloud** instance. You can:\n",
    "- Use [Elastic Cloud](https://cloud.elastic.co/) (14-day free trial available)\n",
    "- Use [IBM Cloud Databases for Elasticsearch](https://cloud.ibm.com/catalog/services/databases-for-elasticsearch)\n",
    "\n",
    "This notebook demonstrates **Retrieval Augmented Generation (RAG)** using:\n",
    "- **Watsonx.ai** for LLM inference\n",
    "- **Elasticsearch Python SDK** directly (no LangChain wrapper)\n",
    "- **SentenceTransformers** for embeddings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "!pip install -qU \"langchain==0.0.340\"\n",
    "!pip install -qU elasticsearch\n",
    "!pip install -qU sentence-transformers\n",
    "!pip install -qU pandas\n",
    "!pip install -qU rouge_score\n",
    "!pip install -qU nltk\n",
    "!pip install -qU wget\n",
    "!pip install -qU evaluate\n",
    "!pip install -qU \"pydantic==1.10.0\"\n",
    "!pip install -qU \"ibm-watsonx-ai>=1.0.312\"\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Watsonx Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "\n",
    "# Prompt for Watsonx credentials\n",
    "watsonx_api_key = getpass.getpass(\"Enter IBM Cloud API Key: \")\n",
    "project_id = getpass.getpass(\"Enter Watsonx Project ID: \")\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": watsonx_api_key\n",
    "}\n",
    "\n",
    "os.environ[\"WATSONX_APIKEY\"] = watsonx_api_key\n",
    "os.environ[\"PROJECT_ID\"] = project_id\n",
    "\n",
    "print(\"‚úÖ Watsonx credentials configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Elasticsearch Credentials\n",
    "\n",
    "**For Google Colab with Elasticsearch Cloud:**\n",
    "- Elasticsearch Host (e.g., `my-cluster.es.us-east-1.aws.found.io`)\n",
    "- Port (usually `9243` for Elastic Cloud)\n",
    "- Username (usually `elastic`)\n",
    "- Password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eshost = input(\"Enter Elasticsearch hostname (e.g., xxx.es.cloud.es.io): \")\n",
    "esport = input(\"Enter Elasticsearch port (usually 9243): \")\n",
    "esuser = input(\"Enter Elasticsearch username (usually 'elastic'): \")\n",
    "espassword = getpass.getpass(\"Enter Elasticsearch password: \")\n",
    "\n",
    "print(\"\\n‚úÖ Elasticsearch credentials configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get SSL Fingerprint\n",
    "\n",
    "‚ö†Ô∏è **Note:** In Colab, we'll use a simplified approach. For Elastic Cloud, you can often skip SSL verification in development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get SSL fingerprint (may not work in Colab)\n",
    "try:\n",
    "    es_ssl_fingerprint_raw = !openssl s_client -connect $eshost:$esport -showcerts </dev/null 2>/dev/null | openssl x509 -fingerprint -sha256 -noout -in /dev/stdin\n",
    "    if es_ssl_fingerprint_raw:\n",
    "        es_ssl_fingerprint = es_ssl_fingerprint_raw[0].split(\"=\")[1]\n",
    "        print(f\"‚úÖ SSL Fingerprint: {es_ssl_fingerprint}\")\n",
    "    else:\n",
    "        es_ssl_fingerprint = None\n",
    "        print(\"‚ö†Ô∏è  Could not retrieve SSL fingerprint. Will attempt connection without it.\")\n",
    "except:\n",
    "    es_ssl_fingerprint = None\n",
    "    print(\"‚ö†Ô∏è  Could not retrieve SSL fingerprint. Will attempt connection without it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Download Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "questions_test_filename = 'questions_test.csv'\n",
    "questions_train_filename = 'questions_train.csv'\n",
    "questions_test_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/questions_test.csv'\n",
    "questions_train_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/questions_train.csv'\n",
    "\n",
    "if not os.path.isfile(questions_test_filename): \n",
    "    wget.download(questions_test_url, out=questions_test_filename)\n",
    "\n",
    "if not os.path.isfile(questions_train_filename): \n",
    "    wget.download(questions_train_url, out=questions_train_filename)\n",
    "\n",
    "test_data = pd.read_csv(questions_test_filename)\n",
    "train_data = pd.read_csv(questions_train_filename)\n",
    "\n",
    "print(\"\\n‚úÖ Test data downloaded!\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Download Knowledge Base Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_dir = \"./knowledge_base\"\n",
    "os.makedirs(knowledge_base_dir, exist_ok=True)\n",
    "\n",
    "documents_filename = 'knowledge_base/psgs.tsv'\n",
    "documents_url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/RAG/psgs.tsv'\n",
    "\n",
    "if not os.path.isfile(documents_filename): \n",
    "    wget.download(documents_url, out=documents_filename)\n",
    "\n",
    "documents = pd.read_csv(f\"{knowledge_base_dir}/psgs.tsv\", sep='\\t', header=0)\n",
    "documents['indextext'] = documents['title'].astype(str) + \"\\n\" + documents['text']\n",
    "documents = documents[:1000]\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(documents)} documents for knowledge base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Embedding Function\n",
    "\n",
    "Using SentenceTransformers `all-MiniLM-L6-v2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "emb_func = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Get embedding dimensions\n",
    "dims = emb_func.client.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"‚úÖ Embedding function created! Dimensions: {dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Initialize Watsonx Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "\n",
    "model_id = ModelTypes.FLAN_UL2\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 50\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    model_id=model_id,\n",
    "    params=parameters,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Watsonx model initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Connect to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Try with SSL fingerprint first, fall back to basic auth without verification\n",
    "if es_ssl_fingerprint:\n",
    "    elastic_client = Elasticsearch(\n",
    "        [f\"https://{esuser}:{espassword}@{eshost}:{esport}\"],\n",
    "        basic_auth=(esuser, espassword),\n",
    "        request_timeout=None,\n",
    "        ssl_assert_fingerprint=es_ssl_fingerprint\n",
    "    )\n",
    "else:\n",
    "    # For Elastic Cloud in development (not recommended for production)\n",
    "    elastic_client = Elasticsearch(\n",
    "        [f\"https://{eshost}:{esport}\"],\n",
    "        basic_auth=(esuser, espassword),\n",
    "        verify_certs=True,\n",
    "        request_timeout=None\n",
    "    )\n",
    "\n",
    "# Test connection\n",
    "if elastic_client.ping():\n",
    "    print(\"‚úÖ Successfully connected to Elasticsearch!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to connect to Elasticsearch. Please check your credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Create Elasticsearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"elastic_knn_index_colab\"\n",
    "\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"text\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"embedding\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": dims,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"l2_norm\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Delete index if exists\n",
    "if elastic_client.indices.exists(index=index_name):\n",
    "    elastic_client.indices.delete(index=index_name)\n",
    "    print(f\"Deleted existing index: {index_name}\")\n",
    "\n",
    "# Create new index\n",
    "elastic_client.indices.create(index=index_name, mappings=mapping)\n",
    "\n",
    "print(f\"‚úÖ Created Elasticsearch index: {index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Index Documents\n",
    "\n",
    "‚ö†Ô∏è **This may take several minutes** to embed and index 1000 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "texts = documents.indextext.tolist()\n",
    "\n",
    "print(\"Embedding documents... This may take a few minutes.\")\n",
    "embedded_docs = emb_func.embed_documents(texts)\n",
    "\n",
    "print(\"Indexing into Elasticsearch...\")\n",
    "document_list = []\n",
    "batch_size = 500\n",
    "\n",
    "for i, (text, vector) in enumerate(zip(texts, embedded_docs)):\n",
    "    document = {\"_id\": i, \"_index\": index_name, \"embedding\": vector, \"text\": text}\n",
    "    document_list.append(document)\n",
    "    \n",
    "    if i % batch_size == batch_size - 1:\n",
    "        success, failed = bulk(elastic_client, document_list)\n",
    "        print(f\"  Indexed {i+1} documents...\")\n",
    "        document_list = []\n",
    "\n",
    "# Index remaining documents\n",
    "if document_list:\n",
    "    success, failed = bulk(elastic_client, document_list)\n",
    "\n",
    "elastic_client.indices.refresh(index=index_name)\n",
    "\n",
    "print(f\"\\n‚úÖ Indexed {len(texts)} documents into Elasticsearch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_and_answers = [\n",
    "    ('names of founding fathers of the united states?', \"Thomas Jefferson::James Madison::John Jay::George Washington::John Adams::Benjamin Franklin::Alexander Hamilton\"),\n",
    "    ('who played in the super bowl in 2013?', 'Baltimore Ravens::San Francisco 49ers'),\n",
    "    ('when did bucharest become the capital of romania?', '1862')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Run Semantic Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_contexts = []\n",
    "\n",
    "for question_text, _ in questions_and_answers:\n",
    "    embedded_question = emb_func.embed_query(question_text)\n",
    "    \n",
    "    relevant_chunks = elastic_client.search(\n",
    "        index=index_name,\n",
    "        knn={\n",
    "            \"field\": \"embedding\",\n",
    "            \"query_vector\": embedded_question,\n",
    "            \"k\": 4,\n",
    "            \"num_candidates\": 50,\n",
    "        },\n",
    "        _source=[\"text\"],\n",
    "        size=5\n",
    "    )\n",
    "    \n",
    "    relevant_contexts.append(relevant_chunks)\n",
    "\n",
    "print(\"‚úÖ Retrieved relevant contexts for all questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: View Retrieved Contexts for First Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_context = relevant_contexts[0]\n",
    "hits = relevant_context['hits']['hits']\n",
    "\n",
    "print(f\"Question: {questions_and_answers[0][0]}\\n\")\n",
    "for hit in hits:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Paragraph index: {hit['_id']}\")\n",
    "    print(f\"Paragraph: {hit['_source']['text'][:300]}...\")\n",
    "    print(f\"Distance: {hit['_score']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Create Prompts and Generate Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(context, question_text):\n",
    "    return (\n",
    "        f\"Please answer the following.\\n\"\n",
    "        f\"{context}:\\n\\n\"\n",
    "        f\"{question_text}\"\n",
    "    )\n",
    "\n",
    "prompt_texts = []\n",
    "\n",
    "for relevant_context, (question_text, _) in zip(relevant_contexts, questions_and_answers):\n",
    "    hits = [hit for hit in relevant_context[\"hits\"][\"hits\"]]\n",
    "    context = \"\\n\\n\\n\".join([rel_ctx[\"_source\"]['text'] for rel_ctx in hits])\n",
    "    prompt_text = make_prompt(context, question_text)\n",
    "    prompt_texts.append(prompt_text)\n",
    "\n",
    "print(f\"‚úÖ Created {len(prompt_texts)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Generate Answers with Watsonx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for prompt_text in prompt_texts:\n",
    "    results.append(model.generate_text(prompt_text))\n",
    "\n",
    "print(\"‚úÖ Generated all answers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, result in enumerate(results):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Question: {questions_and_answers[idx][0]}\")\n",
    "    print(f\"Answer: {result}\")\n",
    "    print(f\"Expected: {questions_and_answers[idx][1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Calculate RougeL Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load('rouge')\n",
    "scores = rouge.compute(\n",
    "    predictions=results,\n",
    "    references=[answer for _, answer in questions_and_answers]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ROUGE Scores:\")\n",
    "print(\"=\" * 80)\n",
    "for metric, score in scores.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19: Try Your Own Question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_question = input(\"Enter your question: \")\n",
    "\n",
    "# Embed and search\n",
    "embedded_question = emb_func.embed_query(your_question)\n",
    "\n",
    "search_results = elastic_client.search(\n",
    "    index=index_name,\n",
    "    knn={\n",
    "        \"field\": \"embedding\",\n",
    "        \"query_vector\": embedded_question,\n",
    "        \"k\": 4,\n",
    "        \"num_candidates\": 50,\n",
    "    },\n",
    "    _source=[\"text\"],\n",
    "    size=5\n",
    ")\n",
    "\n",
    "# Create context from search results\n",
    "hits = search_results['hits']['hits']\n",
    "context = \"\\n\\n\\n\".join([hit[\"_source\"]['text'] for hit in hits])\n",
    "\n",
    "# Generate answer\n",
    "prompt = make_prompt(context, your_question)\n",
    "answer = model.generate_text(prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Question: {your_question}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"\\nRelevant sources:\")\n",
    "for i, hit in enumerate(hits[:3], 1):\n",
    "    print(f\"\\n{i}. {hit['_source']['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You successfully completed this notebook! You learned how to:\n",
    "\n",
    "‚úÖ Connect to Elasticsearch Cloud from Google Colab  \n",
    "‚úÖ Use SentenceTransformers for embeddings  \n",
    "‚úÖ Create and index vectors using Elasticsearch Python SDK  \n",
    "‚úÖ Perform k-NN semantic search  \n",
    "‚úÖ Generate RAG responses with Watsonx  \n",
    "‚úÖ Evaluate results using ROUGE metrics  \n",
    "\n",
    "**Next Steps:**\n",
    "- Try different embedding models (e.g., `all-mpnet-base-v2`)\n",
    "- Experiment with different similarity metrics (cosine, dot product)\n",
    "- Index your own documents\n",
    "- Fine-tune retrieval parameters (k, num_candidates)\n",
    "\n",
    "For more information:\n",
    "- [Watsonx.ai Documentation](https://ibm.github.io/watsonx-ai-python-sdk/samples.html)\n",
    "- [Elasticsearch Python Client](https://elasticsearch-py.readthedocs.io/)\n",
    "\n",
    "---\n",
    "\n",
    "**Copyright ¬© 2023 IBM. This notebook and its source code are released under the terms of the MIT License.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
