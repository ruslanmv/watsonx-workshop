{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Minimal LangGraph Agent over Accelerator RAG\n",
        "\n",
        "This notebook shows a minimal LangGraph-based agent that:\n",
        "\n",
        "- Accepts a user question.\n",
        "- Calls the accelerator `/ask` endpoint in a **retrieval node**.\n",
        "- Uses an LLM in a **generation node** to produce the final answer.\n",
        "\n",
        "It is intentionally simple and mirrors the Evaluation Studio example at a smaller scale.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install dependencies (run once per environment)\n",
        "!pip install -q langgraph langchain-core langchain-openai requests\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "from typing_extensions import TypedDict\n",
        "from typing import Dict, Any\n",
        "\n",
        "import requests\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "ACCELERATOR_API_URL = os.getenv(\"ACCELERATOR_API_URL\", \"http://localhost:8000/ask\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or input(\"OPENAI_API_KEY: \")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, api_key=OPENAI_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class GraphState(TypedDict):\n",
        "    \"\"\"State for our simple LangGraph app.\n",
        "\n",
        "    - input_text: user question.\n",
        "    - rag_answer: raw response from accelerator RAG service.\n",
        "    - final_answer: polished answer from LLM.\n",
        "    \"\"\"\n",
        "\n",
        "    input_text: str\n",
        "    rag_answer: Dict[str, Any]\n",
        "    final_answer: str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Nodes\n",
        "\n",
        "1. `rag_node` \u2013 calls the accelerator `/ask` endpoint.\n",
        "2. `generation_node` \u2013 uses the LLM to polish the answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def rag_node(state: GraphState) -> Dict[str, Any]:\n",
        "    question = state[\"input_text\"]\n",
        "    resp = requests.post(ACCELERATOR_API_URL, json={\"question\": question}, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    return {\"rag_answer\": data}\n",
        "\n",
        "\n",
        "def generation_node(state: GraphState) -> Dict[str, Any]:\n",
        "    question = state[\"input_text\"]\n",
        "    rag_answer = state.get(\"rag_answer\", {})\n",
        "    answer = rag_answer.get(\"answer\") or rag_answer.get(\"result\") or \"(no answer field)\"\n",
        "    citations = rag_answer.get(\"citations\") or rag_answer.get(\"chunks\") or []\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"You are a helpful assistant.\\n\"\n",
        "        \"Here is a draft answer from a RAG service and its citations.\\n\\n\"\n",
        "        \"Question: {question}\\n\\n\"\n",
        "        \"Draft answer: {draft}\\n\\n\"\n",
        "        \"Citations: {citations}\\n\\n\"\n",
        "        \"Please rewrite the answer in a clear, concise way. If the draft looks incomplete, say so.\"\n",
        "    )\n",
        "\n",
        "    formatted = prompt.invoke({\n",
        "        \"question\": question,\n",
        "        \"draft\": answer,\n",
        "        \"citations\": str(citations),\n",
        "    })\n",
        "    result = llm.invoke(formatted)\n",
        "    return {\"final_answer\": result.content}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build and Compile the Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "graph = StateGraph(GraphState)\n",
        "graph.add_node(\"rag_node\", rag_node)\n",
        "graph.add_node(\"generation_node\", generation_node)\n",
        "\n",
        "graph.add_edge(START, \"rag_node\")\n",
        "graph.add_edge(\"rag_node\", \"generation_node\")\n",
        "graph.add_edge(\"generation_node\", END)\n",
        "\n",
        "app = graph.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run a Test Question\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "state = {\"input_text\": \"Explain what RAG is and why it's useful.\"}\n",
        "result = app.invoke(state)\n",
        "result\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}