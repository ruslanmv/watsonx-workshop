{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4720397e-7460-4be5-8164-764c3c03a91f"
      },
      "source": "# Ingestion of Expert profile data to vector db and retrieval"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69f2f6f7-d4fe-41e4-856b-feb2879dc31a"
      },
      "source": "This notebook extracts a json with expert profile information, transforms it into a document format, and then stores them into a vector database for subsequent processing. \n\nThis is an additional index created alongside the watsonx docs rag vector index created in the **Process and Ingest into vector DB** notebook. \n\nThe indexed documents are further used in the **Create and Deploy QnA AI Service** notebook which creates QnA RAG AI service, deploys it on watsonx.ai and can additionally retrieve the expert profile information in case the LLM has no answer to a specific question.\n\nThe process involves extracting content from the expert profiles file, segmenting the data, converting it into a document format, and finally indexing the content into a vector database. \n\n**Note:** A sample profile data is shipped with the accelerator. Alteratively you can index your own expert profile data instead.\n\nThe accelerator currently supports Elasticsearch and Milvus or Datastax vector databases. The ingestion process uses vector embeddings to enhance data storage and retrieval within either Elasticsearch or Milvus or Datastax vector databases, ensuring both efficiency and effectiveness.\n\n* Establishing a connection to the chosen vector database (Elasticsearch or Milvus or Datastax) and loading input data from processed documents.\n* Generating unique IDs for documents.\n* Inserting the documents with embeddings into Elasticsearch or Milvus or Datastax using these generated IDs, with progress monitoring provided by a progress bar.\n\n## Contents\n* [Pre-Requisite Libraries and Dependencies](#setup)\n* [Import Dependencies](#import)\n* [Extract data from input file](#input)\n* [Connect to Vector Database](#connect)\n* [Index Documents using langchains vectorstore](#insert)\n* [Search and Retrieve using Vectorstore and Query templates](#search)\n\n**Note:** Datastax is not supported in this cloud version."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "199e5b66-aebb-4eab-8ded-d83f3f8ba1c0"
      },
      "source": "<a id=\"setup\"></a>\n### Pre-Requisite Libraries and Dependencies\nDownload and import mandatory libraries and dependencies. \n\n**Note** : Some of the versions of the libraries may throw warnings after installation. These library versions are crucial for successful execution of the accelerator. Please ignore the warning/error and proceed with your execution. "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5abb2a8d-d8cc-416f-a4ac-60a7a784603a"
      },
      "outputs": [],
      "source": "!pip install elasticsearch==8.18.1 | tail -n 1\n!pip install langchain | tail -n 1\n!pip install ibm_watsonx_ai==1.3.26 | tail -n 1\n!pip install langchain_elasticsearch==0.3.2 | tail -n 1\n!pip install langchain_milvus==0.2.0 | tail -n 1\n!pip install pymilvus==2.5.11 | tail -n 1\n!pip install cassio==0.1.10"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc99b572-3b76-4020-be81-91cccbf607be"
      },
      "source": "Restart the kernel after performing the pip install if the below cell fails to import all the libraries."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a7a1f8a-a0a4-4128-b71d-33ec72b0fdfd"
      },
      "outputs": [],
      "source": "from langchain.schema import Document\nfrom ibm_watsonx_ai import APIClient,Credentials\nimport os,re\nfrom tqdm import tqdm\nimport json\nfrom ibm_watsonx_ai import __version__\nimport warnings\nimport hashlib\nfrom ibm_watsonx_ai.foundation_models import Embeddings\nfrom elasticsearch import Elasticsearch, helpers\nwarnings.filterwarnings(\"ignore\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ee1679-7e20-4cfe-8fb9-dbad97896580"
      },
      "source": "#### Get Environment variables"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "675c28ac-8ef6-41e6-b744-201d19885bd6"
      },
      "outputs": [],
      "source": "project_id = os.environ['PROJECT_ID']\n# Environment and host url\nhostname = os.environ['RUNTIME_ENV_APSX_URL']\n\nif hostname.endswith(\"cloud.ibm.com\") == True:\n    environment = \"cloud\"\n    runtime_region = os.environ[\"RUNTIME_ENV_REGION\"] \nelse:\n    environment = \"on-prem\"\n    from ibm_watson_studio_lib import access_project_or_space\n    wslib = access_project_or_space()"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a51e3bc2-8758-4272-9579-0a4a38557023"
      },
      "source": "<a id=\"import\"></a>\n### Import Parameter Sets, credentials and Helper functions script."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c63f510-7cba-4ab8-998d-5be4c4a7d3e1"
      },
      "source": "#### Parameter sets import\nBelow cells imports parameter sets values, sets the watsonx.ai credentials and imports the helper functions script."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf5d1e46-109c-4212-bac4-dd02d2e8222f"
      },
      "source": "#### RAG helper functions script import"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "396b3d7f-fc32-477b-ae29-b466ebfb3bc6"
      },
      "outputs": [],
      "source": "try:\n    filename = 'rag_helper_functions.py'\n    wslib.download_file(filename)\n    import rag_helper_functions\n    print(\"rag_helper_functions imported from the project assets\")\nexcept NameError as e:\n    print(str(e))\n    print(\"If running watsonx.ai aaS on IBM Cloud, check that the first cell in the notebook contains a project token. If not, select the vertical ellipsis button from the notebook toolbar and `insert project token`. Also check that you have specified your ibm_api_key in the second code cell of the notebook\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "880758cf-548b-45ef-8ddf-79503f1a55b9"
      },
      "outputs": [],
      "source": "parameter_sets = [\"RAG_parameter_set\",\"RAG_advanced_parameter_set\"]\n\nparameters=rag_helper_functions.get_parameter_sets(wslib, parameter_sets)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "023b24ab-2dcb-4104-8cf0-b2fe2194d524"
      },
      "outputs": [],
      "source": "ibm_api_key=parameters['watsonx_ai_api_key']\nif environment == \"cloud\":\n    WML_SERVICE_URL=f\"https://{runtime_region}.ml.cloud.ibm.com\" \n    wml_credentials = Credentials(api_key=parameters['watsonx_ai_api_key'], url=WML_SERVICE_URL)\nelse:\n    token = os.environ['USER_ACCESS_TOKEN']\n    wml_credentials=Credentials(token=os.environ['USER_ACCESS_TOKEN'],url=hostname,instance_id='openshift')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "888fbfed-666b-4a3e-8def-7aa192d75fc8"
      },
      "source": "#### Setup the watsonx.ai client\nBelow cell uses the watson machine learning credentials to create an API client to interact with the project and deployment space."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "586e495b-a28a-469d-b113-02e05929f378"
      },
      "outputs": [],
      "source": "client = APIClient(wml_credentials)\nclient.set.default_project(project_id=project_id)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6f3d0db-b2bc-43af-919a-4f341305ae89"
      },
      "source": "#### Import Expert Profiles\n\nChange the `expert_profiles_document` parameter if you wish to use your own Expert Data to Ingest into the vector database"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d07e396-eacd-4ea7-b16d-b9e3ab3cc652"
      },
      "outputs": [],
      "source": "# Get the ingestion document for indexing - Breakpoint for folks indexing their own expert profile data\n\nfilename = parameters['expert_profiles_document']\nwslib.download_file(filename)\nwith open(filename) as f:\n    expert_profiles = json.load(f)\n    \nprint(len(expert_profiles), \"expert profiles have been imported.\")    "
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47e0bb5e-8711-4d34-ae0e-115225677c43"
      },
      "source": "<a id=\"input\"></a>\n### Extract the json document and convert the data into Langchain format for indexing\nThe cell below prepares documents for insertion into a vector database. "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3377c8a6-fbfb-4389-b2fa-9f40c3ffddac"
      },
      "outputs": [],
      "source": "def convert_json_to_documents(json_data):\n    documents = []\n    for item in json_data:\n        \n        text = item.get(\"text\", \"\")   \n        # Extract the text with the profile information & rest is put into metadata\n        metadata = {key: value for key, value in item.items() if key != \"text\"}  \n        documents.append(Document(page_content=text, metadata=metadata))\n    return documents\n\ndocuments = convert_json_to_documents(expert_profiles)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0118c668-6374-400f-99d5-b204a3b7d08a"
      },
      "outputs": [],
      "source": "import hashlib\nfrom langchain.schema import Document\n\ndef process_profiles_to_documents(json_data):\n    \"\"\"\n    Converts JSON data to Document objects, extracts metadata and content, generates unique IDs,\n    and identifies duplicate documents.\n\n    Parameters:\n    - json_data (list): List of dictionaries representing expert profiles.\n\n    Returns:\n    - profile_documents (list): List of Document objects with processed metadata and content.\n    - id_list (list): List of unique IDs generated for each document.\n    - duplicate_count (int): Number of duplicate documents found.\n    \"\"\"\n    # Convert JSON data to Document objects\n    documents = []\n    for item in json_data:\n        text = item.get(\"text\", \"\")  # Extract the main text content\n        metadata = {key: value for key, value in item.items() if key != \"text\"}  # Extract metadata\n        documents.append(Document(page_content=text, metadata=metadata))\n\n    # Prepare content and metadata for profile documents\n    profile_content = []\n    profile_metadata = []\n    for doc in documents:\n        profile_metadata.append({\n            \"source\": doc.metadata['datainfo']['source'],\n            \"entry_number\": doc.metadata['datainfo']['entry_number'],\n            \"name\": doc.metadata['datainfo']['name'],\n            \"email\": doc.metadata['datainfo']['email'],\n            \"phone\": doc.metadata['datainfo']['phone'],\n            \"domain\": doc.metadata['datainfo']['domain'],\n            \"position\": doc.metadata['datainfo']['position'],\n            \"document_id\": doc.metadata['document_id']\n        })\n        profile_content.append(\"Document Content: \" + doc.page_content)\n\n    # Create profile documents\n    profile_documents = [\n        Document(page_content=text, metadata=meta)\n        for text, meta in zip(profile_content, profile_metadata)\n    ]\n\n    # Generate unique IDs for profile documents\n    id_list = [\n        hashlib.sha256(doc.page_content.encode()).hexdigest()\n        for doc in profile_documents\n    ]\n\n    # Identify duplicates\n    duplicate_count = len(id_list) - len(set(id_list))\n    print(f\"{duplicate_count} duplicate profiles found.\")\n    print(f\"{len(id_list)} profiles returned.\")\n\n    return profile_documents"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab30edf9-2dc7-47ba-9c1f-e217ba15914a"
      },
      "outputs": [],
      "source": "profile_documents = process_profiles_to_documents(expert_profiles)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16941425-3de7-4ebd-bb89-377a5e45f9ca"
      },
      "source": "<a id=\"connect\"></a>\n## Connecting to Vector Database\n\nThe notebook, by default, will look for a connection asset in the project named milvus_connect or elasticsearch_connect or datastax_connect. You can set this up by following the instructions in the project readme. This code checks if a specified connection exists in the project. If found, it retrieves the connection details and identifies the connection type. Depending on the connection type, it establishes a connection to the appropriate database. If the connection is not found, it raises an error indicating the absence of the specified connection in the project."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9c07f63-7d80-43b0-9b9e-79f44a970599"
      },
      "outputs": [],
      "source": "connection_name=parameters[\"connection_asset\"]\nif(next((conn for conn in wslib.list_connections() if conn['name'] == connection_name), None)):\n    print(connection_name, \"Connection found in the project\")\n    db_connection = wslib.get_connection(connection_name)\n    \n    connection_datatypesource_id=client.connections.get_details(db_connection['.']['asset_id'])['entity']['datasource_type']\n    connection_type = client.connections.get_datasource_type_details_by_id(connection_datatypesource_id)['entity']['name']\n    \n    print(\"Successfully retrieved the connection details\")\n    print(\"Connection type is identified as:\",connection_type)\n\n    if connection_type==\"elasticsearch\":\n        es_client=rag_helper_functions.create_and_check_elastic_client(db_connection, parameters['elastic_search_model_id'])\n    elif connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n        milvus_credentials = rag_helper_functions.connect_to_milvus_database(db_connection, parameters)\n    elif connection_type==\"datastax\":\n        if environment == \"cloud\":\n            raise ValueError(f\"ERROR! we don't support datastax connection for Cloud as of now\")\n        datastax_session,datastax_cluster = rag_helper_functions.connect_to_datastax(db_connection, parameters)\n        import cassio\n        cassio.init(session=datastax_session, keyspace=db_connection.get('keyspace'))\nelse:\n    db_connection=\"\"\n    raise ValueError(f\"No connection named {connection_name} found in the project.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceb78ee6-eb40-40af-b755-4a0f0d705bed"
      },
      "source": "<a id=\"insert\"></a>\n### Create vector database for Expert Profiles and index using Langchain vector store"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637c86c8-8eaf-4022-8e03-b3ed0b4e69d0"
      },
      "source": "Below code utilizes Langchains vector store extension to store documents.\nThe code sets up a vector store based on the specified connection type, either \"elasticsearch\" or \"milvus\".\n\n* If connection_type is `\"elasticsearch\"`, it imports `ElasticsearchStore` from the `langchain_elasticsearch` library and initializes it with an Elasticsearch client and specified parameters with the given model ID. The Elastic Search vector store is then created using the `model_id`, connection parameters and index settings\n* If connection_type is `\"milvus\"`, the code imports `langchain_milvus` and configures credentials using an API key and service URL. It initializes the embedding model specified in the parameters to generate embeddings, and sets up index parameters with specific metrics and configurations. The Milvus vector store is then created using the embedding function, connection details, and index settings. For cloud environments, it uses the `ibm_watsonx_ai` library to initialize the embedding function with the required API key, URL, and project ID. On Cloud Pak for Data software, it downloads the embedding model from HuggingFace via the `HuggingFaceEmbeddings` class to generate embeddings for the chunked documents.\n* If `connection_type` is `\"datastax\"`, it uses `langchain_community.vectorstores.Cassandra` to initialize a `Cassandra-based vector store`. The embedding model is initialized similarly using `get_embedding()`, and the store is configured using a specified keyspace and table name. This setup supports vector search on a DataStax Astra DB or Cassandra cluster.\n\nFirstly, it creates a connection to the vector database and defines how documents will be retrieved later.\nThen, it defines a function to add these documents to the vector store. This function takes the documents and additional parameters for efficient processing, such as splitting the documents into smaller chunks and setting a timeout for requests.\n\nOverall, this code efficiently adds a list of documents to a vector store, thereby making them searchable"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab05066a-ef64-407c-b1e2-285a1503879e"
      },
      "source": "Below code creates the `Elasticsearch` dense vector index for indexing of documents if dense embedding model like `E5 multilingual` is used and also creates elasticsearch pipeline for indexing."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "307f20c4-91b4-434f-b608-1c74c3783089"
      },
      "outputs": [],
      "source": "def create_es_dense_index(index_name):\n    try:\n        es_client.options(ignore_status=400).indices.create(\n                    index=index_name,\n                    mappings={\n                        'properties': {\n                            'vector.tokens': {\n                                'type': 'dense_vector',\n                            },\n                        }\n                    },\n                    settings={\n                        'index': {\n                            'default_pipeline': 'dense-ingest-pipeline',\n                        },\n                        \"number_of_shards\": parameters[\"es_number_of_shards\"],\n                    }\n        )\n        es_client.ingest.put_pipeline(\n                    id='dense-ingest-pipeline',\n                    processors=[\n                        {\n                            'inference': {\n                                'model_id': parameters['elastic_search_model_id'],\n                                'input_output': [\n                                    {\n                                        'input_field': 'text',\n                                        'output_field': 'vector.tokens',\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                )\n        print('Elastic search index created!')\n    except Exception as e:\n        print('Error creating elastic index', e)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "169b8e0f-1429-4c09-98a4-e1199bb5832d"
      },
      "outputs": [],
      "source": "def get_embedding(environment, parameters, project_id, wml_credentials, WML_SERVICE_URL=None):\n    if environment == \"cloud\":\n        credentials = Credentials(\n            api_key=parameters['watsonx_ai_api_key'],\n            url=WML_SERVICE_URL\n        )\n        embedding = Embeddings(\n            model_id=parameters['embedding_model_id'],\n            credentials=credentials,\n            project_id=project_id,\n            verify=True\n        )\n    elif environment == \"on-prem\":\n        try:\n            if client.foundation_models.EmbeddingModels.__members__:\n                if client.foundation_models.EmbeddingModels(parameters[\"embedding_model_id\"]).name:\n                    embedding = Embeddings(\n                        model_id=parameters['embedding_model_id'],\n                        credentials=wml_credentials,\n                        project_id=project_id,\n                        verify=True\n                    )\n                else:\n                    print(\"Local on-prem embedding models not found, using models from IBM Cloud API\")\n                    credentials = Credentials(\n                        api_key=parameters['watsonx_ai_api_key'],\n                        url=parameters['watsonx_ai_url']\n                    )\n                    embedding = Embeddings(\n                        model_id=parameters['embedding_model_id'],\n                        credentials=credentials,\n                        space_id=parameters[\"wx_ai_inference_space_id\"],\n                        verify=True\n                    )\n        except Exception as e:\n            print(f\"Exception in loading Embedding Models: {str(e)}\")\n            raise\n    else:\n        raise ValueError(f\"Invalid environment: {environment}. Must be 'cloud' or 'on-prem'.\")\n    \n    return embedding"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd44717f-829d-4207-8ad6-acae1334c3e9"
      },
      "outputs": [],
      "source": "\ndef create_vector_store(connection_type,index_name,parameters,milvus_index_params=None):\n    if connection_type==\"elasticsearch\":\n        from langchain_elasticsearch import ElasticsearchStore\n        if 'dense' in parameters['elastic_search_vector_type']:\n            create_es_dense_index(index_name)\n            vector_store=ElasticsearchStore(\n                            es_connection=es_client,\n                            index_name=index_name,\n                            strategy=ElasticsearchStore.ApproxRetrievalStrategy(query_model_id=parameters['elastic_search_model_id']),\n                            )\n        else:\n            vector_store=ElasticsearchStore(\n                            es_connection=es_client,\n                            index_name=index_name,\n                            strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(model_id=parameters['elastic_search_model_id']),\n                            custom_index_settings={\"number_of_shards\": parameters[\"es_number_of_shards\"]}\n                            )\n        print(\"Elastic Search Vector Store Created with\",parameters['elastic_search_model_id'])\n    elif connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n        from langchain_milvus import Milvus\n        \n        #milvus_credentials={'database': db_connection['database'],'password':db_connection['password'] ,'port': db_connection['port'] ,'host': db_connection['host'],\"secure\": True,'user': db_connection['username']}\n        print(\"using the model\",parameters['embedding_model_id'], \"to create embeddings\")\n        embedding = get_embedding(environment, parameters, project_id, wml_credentials, WML_SERVICE_URL) if environment == \"cloud\" else get_embedding(environment, parameters, project_id, wml_credentials, None)  \n            \n        vector_store = Milvus(\n            embedding_function=embedding,\n            connection_args=milvus_credentials,\n            index_params=milvus_index_params,\n            primary_field='id',\n            #text_field=\"page_content\",\n            collection_name=index_name\n             \n        )\n        print(\"Milvus Vector Store Created\")\n    elif connection_type == \"datastax\":\n        if environment == \"cloud\":\n            raise ValueError(f\"ERROR! we don't support datastax connection for Cloud as of now\")\n        print(\"using the model\",parameters['embedding_model_id'], \"to create embeddings\")\n        embedding = get_embedding(environment, parameters, project_id, wml_credentials, WML_SERVICE_URL) if environment == \"cloud\" else get_embedding(environment, parameters, project_id, wml_credentials, None)  \n        from langchain_community.vectorstores import Cassandra\n        vector_store = Cassandra(\n            embedding=embedding,\n            table_name=index_name\n        )\n        print(\"Datastax Vector Store Created\")\n        \n    return vector_store"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14d993f2-2c77-4f9f-a694-c1f5da4a020d"
      },
      "source": "\n\nBelow code defines a function to generates a list of unique IDs for each document by hashing their page_content. The code sets a chunk size for batch processing. It iterates over the documents in chunks, inserting each chunk into the vector store with corresponding IDs. The progress bar is updated to reflect the number of documents processed.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ef01c13-a734-4f1c-823b-60a3425adb67"
      },
      "outputs": [],
      "source": "def generate_hash(content):\n    return hashlib.sha256(content.encode()).hexdigest()\n\n\ndef insert_docs_to_vector_store(vector_store,split_docs,insert_type=\"docs\" ):\n    with tqdm(total=len(split_docs), desc=\"Inserting Documents\", unit=\"docs\") as pbar:\n        try:\n            for i in range(0, len(split_docs), parameters['index_chunk_size']):\n                chunk = split_docs[i:i + parameters['index_chunk_size']]\n                if insert_type==\"docs\":\n                    id_chunk = [generate_hash(doc.page_content+'\\nTitle: '+doc.metadata['title']+'\\nUrl: '+doc.metadata['document_url']+'\\nPage: '+doc.metadata['page_number']) for doc in chunk]\n                elif insert_type==\"profiles\":\n                    id_chunk = [generate_hash(doc.page_content) for doc in chunk]\n                vector_store.add_documents(chunk, ids=id_chunk)\n                pbar.update(len(chunk))\n            print(\"Documents are inserted into vector database\")\n        except Exception as e:\n            print(f\"An error occurred: {e}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b79e340f-e1dc-42b2-a5e6-69a2bc6016d2"
      },
      "outputs": [],
      "source": "milvus_index_params = {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 1024}}\n\nprofile_vector_store=create_vector_store(connection_type,parameters['expert_profiles_index'],parameters, milvus_index_params)\ninsert_docs_to_vector_store(profile_vector_store,profile_documents,\"profiles\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7500af2b-ce42-4fff-9588-3a37c93d5e6e"
      },
      "source": "Above cell is a synchronous call & may take time to complete based on the size of the documents. You can proceed to **Create and Deploy Q&A Python Function** notebook to create and deploy the RAG python function without waiting for the previous cell to complete."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c0519b8-8826-4a35-8300-ae034cb1f7a5"
      },
      "source": "<a id=\"search\"></a>\n### Querying the vector index/collection for the expert profiles\n\nThe following sections of the notebook are designed to test a sample Question and Answer (QnA) interaction on the vector store. The subsequent cell in the notebook executes this test and provides a response that includes several key pieces of information."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47ab0549-d900-45ba-86b5-a635aaee502c"
      },
      "outputs": [],
      "source": "question =\"how to perform decision optimization?\""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6afa484-aab6-4552-9d36-1b7c4973ce56"
      },
      "source": "In case of **Elastic Search**, The following section of the notebook is designed to test a sample Question and Answer (QnA) interaction using sample template of ELSER model or multilingual model, assuming it is utilized. This response comprises of: \n\n* `Document ID` : A unique identifier for the document within the database or index.\n* `Document Context` : Expert Profile data content or text from the document that is relevant to the queried question.\n* `Relevance Score` : A numerical value indicating the relevance or confidence level of the answer provided by the model.\n\nThis setup allows for a practical demonstration of the model's capabilities in retrieving and presenting information in response to a specific query. There are 3 ways to perform this step, depending on the `elastic_search_template_file` parameter provided in the parameter set by the user.\n\n* **ELSER**: An ELSER exclusive search query is invoked.\n* **ELSER + BM25**: A hybrid search query that is a combination of a tradition vector search and ELSER is invoked.\n* **Multilingual**: A dense vector search query is invoked.\n\n\nIn case of **Milvus** & **Datastax**, below code initializes a Milvus collection using a specified collection name and loads it into memory. It then generates embeddings for a given question using above specified embedding model. The code performs a vector search in the collection based on these embeddings, and retrieves the top closest result.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "796e3c4a-880f-4ba1-87d7-6679c86417e8"
      },
      "outputs": [],
      "source": "############################ ELASTIC #################################\n\nmatch connection_type:\n    case \"elasticsearch\":\n        wslib.download_file(parameters['elastic_search_template_file'])\n        with open(parameters['elastic_search_template_file']) as f:\n            es_query_json = json.load(f)\n\n        es_query_str = json.dumps(es_query_json)  \n        if 'dense' in parameters['elastic_search_vector_type']:\n            from langchain_elasticsearch import ElasticsearchEmbeddings\n            embeddings = ElasticsearchEmbeddings.from_es_connection(\n                        model_id=parameters['elastic_search_model_id'],\n                        es_connection=es_client,\n                    )\n            query_vector = embeddings.embed_documents([question])[0]\n            es_query_str = es_query_str.replace('\"{{query_vector}}\"', str(query_vector))\n        else:\n            es_query_str = es_query_str.replace(\"{{model_id}}\", parameters['elastic_search_model_id'])\n            es_query_str = es_query_str.replace(\"{{model_text}}\", question)\n        \n        # Convert back to dictionary\n        es_query_template = json.loads(es_query_str)\n        es_query=es_query_template.get(\"query\",es_query_template)\n        print(es_query)\n\n        query_temp_args = {'query': es_query}\n        if 'sub_searches' in es_query:\n            query_temp_args = {'body': es_query}\n\n        try:\n            response = es_client.search(\n                index=parameters[\"expert_profiles_index\"], \n                size=parameters['top_k_experts'],\n                **query_temp_args\n            )\n            print(\"\\nResponse:\")\n            for hit in response['hits']['hits']:\n                score = hit['_score']\n                source = hit['_source']\n                doc_id = source['metadata']['document_id']\n                page_content = source['text']\n                name = source['metadata']['name']\n                phone = source['metadata']['phone']\n                email = source['metadata']['email']            \n                domain = source['metadata']['domain']\n                position = source['metadata']['position']\n                source = source['metadata']['source']\n                print(f\"\\nRelevance Score  : {score}\\nDocument ID : {doc_id}\\nExpert Name : {name}\\nEmail : {email}\\nPhone Number : {phone}\\nDomain : {domain}\\nPosition : {position}\\nSource : {source}\\n\\n{page_content}\")\n        \n\n        except Exception as e:\n                print(\"\\nAn error occurred while querying elastic search, please retry after sometime:\", e)\n\n############################ MILVUS #################################\n\n    case \"milvus\" | \"milvuswxd\":\n        print(\"using the model\",parameters['embedding_model_id'], \"to create embeddings\")\n        embedding = get_embedding(environment, parameters, project_id, wml_credentials, WML_SERVICE_URL) if environment == \"cloud\" else get_embedding(environment, parameters, project_id, wml_credentials, None)  \n\n        search_params = {\"metric_type\": \"L2\", \"params\": {\"radius\": 1.07}}\n        \n        query_embeddings = embedding.embed_documents(texts=[question])\n\n        result = profile_vector_store.similarity_search_with_score_by_vector(embedding.embed_query(question), k=1, param = search_params)\n        try: \n            for res in result:\n                #print(res)\n                print(f\"\\nRelevance Score  : {res[1]}\\n\\nMetadata:-->\\n\\n{res[0].metadata}\\n\\nText :-->\\n\\n{res[0].page_content}\")\n                print('-----------')\n        except Exception as e: \n            print(\"ERROR: No relevant information found. Please check all the parameters and try again.\", e)\n\n############################ Datastax #################################\n    case \"datastax\":\n        if environment == \"cloud\":\n            raise ValueError(f\"ERROR! we don't support datastax connection for Cloud as of now\")\n        print(\"using the model\",parameters['embedding_model_id'], \"to create embeddings\")\n        embedding = get_embedding(environment, parameters, project_id, wml_credentials, WML_SERVICE_URL) if environment == \"cloud\" else get_embedding(environment, parameters, project_id, wml_credentials, None)  \n        from langchain_community.vectorstores import Cassandra\n        vector_store = Cassandra(\n            embedding=embedding,\n            table_name=parameters[\"vector_store_index_name\"] \n        )\n        print(\"Datastax vector store Created on the index\",parameters[\"vector_store_index_name\"] )\n        \n        search_result= vector_store.similarity_search_with_score_by_vector(embedding.embed_query(question), k=1)\n        print(\"\\nQuestion:\",question, \"\\nSearch Results:\", search_result)\n\n    case _:\n        raise ValueError(f\"Unsupported connection_type: {connection_type}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Note** It's recommended to close the datastax session once you are done with ingestion in this notebook for optimal performance. once you execute this cell existing datastax connections are closed. if have to re run above code cells you have to create new connection for datastax by re running cells from `Connect to Vector Database`"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fbe56aa-c90f-4a0e-8710-6553a704a419"
      },
      "outputs": [],
      "source": "if connection_type==\"datastax\" and environment != \"cloud\":\n    if not datastax_session.is_shutdown:\n        datastax_session.shutdown()\n        print(f\"datastax_session got shutdown : {datastax_session.is_shutdown}\")\n    if not datastax_cluster.is_shutdown:\n        datastax_cluster.shutdown()\n        print(f\"datastax_cluster got shutdown : {datastax_cluster.is_shutdown}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c89ade9f-791e-45d4-a91d-5bf4435a9cf1"
      },
      "source": "Proceed to the **Create and Deploy QnA AI Service** notebook to create and deploy the RAG AI service python function."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cae9b763-8364-4047-b8de-902648037b10"
      },
      "source": "\n**Sample Materials, provided under license.** <br>\n**Licensed Materials - Property of IBM.** <br>\n**Â© Copyright IBM Corp. 2024, 2025. All Rights Reserved.** <br>\n**US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.** <br>"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
