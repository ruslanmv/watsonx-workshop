{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "d1331fdc-cf93-45c2-b839-858171c473cc",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "#  RAG and AI Service Python Function Deployment\n",
        "\n",
        "In this notebook, we create and deploy an AI service python function that takes a user defined question as input and generates an answer using the RAG process. The function does the following:\n",
        "\n",
        "\n",
        "- Vector Search: Connects to Elasticsearch or Milvus or Datastax vector databases to retrieve the top N relevant documents from the vector index, filtering results based on a configurable score threshold.\n",
        "- Prompt Construction: Combines the user’s question and retrieved documents into an optimized prompt template for the language model.\n",
        "- LLM Inference: Executes inference on IBM Watsonx.ai to generate a response, supporting both streaming and non-streaming modes.\n",
        "- Hallucination Detection: Validates the generated response for accuracy using either word overlap or embedding-based cosine similarity techniques, applying configurable threshold values.\n",
        "- Feedback Logging: Updates log records with user feedback on the generated response, stored in Elasticsearch, Datastax or Milvus, with PII suppression for compliance.\n",
        "- Expert Recommendation: Retrieves the top K expert profiles from a vector index in Elasticsearch or Milvus or Datastax, matching the question based on a relevance threshold.\n",
        "- Expert Profile Logging: Updates log records with recommended expert profiles for future reference.\n",
        "- Autocomplete Functionality: Provides a function to retrieve answer suggestions by auto-completing the question based on a provided prefix, leveraging logged interactions in Elasticsearch or Milvus or Datastax.\n",
        "- Guardrail Checks: Detects Personally Identifiable Information (PII) and Harmful, Abusive, or Profane (HAP) content in responses to ensure compliance.\n",
        "- Performance Monitoring: Tracks execution times for key pipeline stages, enabling optimization and debugging.\n",
        "\n",
        "**Note**: It is recommended to run this notebook in a Python environment on CPD software with a GPU-enabled or high vCPU and RAM hardware configuration, as generating embeddings may require significant memory.\n",
        "## Contents\n",
        "\n",
        "This notebook contains the following parts:\n",
        "\n",
        "- [Pre-Requisite Libraries and Dependencies](#Setup)\n",
        "- [Import Parameter Set, Credentials and Helper function script](#parameterimport)\n",
        "- [Initialise Deployment Space](#DeploymentSpace)\n",
        "- [Promote Assets to Deployment Space](#promote)\n",
        "- [Create Deployable AI Service Code](#ScoringFunction)\n",
        "- [Deploy AI Function to Deployment Space](#DeployScoringFunction)\n",
        "- [Test the deployed AI function](#scoring)\n",
        "- [Test the expert recommendation function](#expert-recommendation)\n",
        "- [Update the feedback log](#feedback-logging)\n",
        "- [Test Auto complete](#test-auto-complete)\n",
        "- [Update parameter set in the project & deployment space](#updateParameters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a256272bbed64c7caeb0c475152f8d0d"
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "### Pre-Requisite Libraries and Dependencies\n",
        "Download and import mandatory libraries and dependencies. \n",
        "\n",
        "Note : Some of the versions of the libraries may throw warnings after installation. These library versions are crucial for successful execution of the accelerator. Please ignore the warning/error and proceed with your execution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e8421de124a45d19eb59ac0616319d0"
      },
      "outputs": [],
      "source": [
        "!pip install elasticsearch==8.18.1 | tail -n 1\n",
        "!pip install ibm_watsonx_ai==1.3.26 | tail -n 1\n",
        "!pip install langchain_milvus==0.2.0 | tail -n 1\n",
        "!pip install cassio==0.1.10 | tail -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e5cf8cd-9aed-4d59-8eb8-c084187ddd34"
      },
      "source": [
        "Restart the kernel after performing the pip install if the below cell fails to import all the libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33018f4dd1b94d51afb9a97280e8dec1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import string\n",
        "import time\n",
        "import uuid\n",
        "import re\n",
        "from ibm_watsonx_ai import APIClient,Credentials\n",
        "from ibm_watsonx_ai import __version__\n",
        "from ibm_watsonx_ai.deployments import RuntimeContext\n",
        "from ibm_watsonx_ai.foundation_models.prompts import PromptTemplate, PromptTemplateManager\n",
        "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, PromptTemplateFormats\n",
        "from pymilvus import(connections,FieldSchema,DataType,Collection,CollectionSchema,utility)\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54c98119-621b-4e70-a6a1-ac9a90fb618d"
      },
      "outputs": [],
      "source": [
        "project_id=os.environ['PROJECT_ID']\n",
        "# Environment and host url\n",
        "hostname = os.environ['RUNTIME_ENV_APSX_URL']\n",
        "\n",
        "if hostname.endswith(\"cloud.ibm.com\") == True:\n",
        "    environment = \"cloud\"\n",
        "    runtime_region = os.environ[\"RUNTIME_ENV_REGION\"] \n",
        "else:\n",
        "    environment = \"on-prem\"\n",
        "    from ibm_watson_studio_lib import access_project_or_space\n",
        "    wslib = access_project_or_space()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b63f381e-97b3-4a83-9d00-3e4b2469af86"
      },
      "source": [
        "<a id=\"parameterimport\"></a>\n",
        "### Import Parameter Sets, credentials and Helper functions script.\n",
        "\n",
        "The below cells imports the parameter set, credentials for watsonx.ai and helper function script. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "886c8e65-2ddf-4216-899a-0599b499436c"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    filename = 'rag_helper_functions.py'\n",
        "    wslib.download_file(filename)\n",
        "    import rag_helper_functions\n",
        "    print(\"rag_helper_functions imported from the project assets\")\n",
        "except NameError as e:\n",
        "    print(str(e))\n",
        "    print(\"If running watsonx.ai aaS on IBM Cloud, check that the first cell in the notebook contains a project token. If not, select the vertical ellipsis button from the notebook toolbar and `insert project token`. Also check that you have specified your ibm_api_key in the second code cell of the notebook\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e78b0ede-a3f8-4526-9fef-d32f6eda64b7"
      },
      "outputs": [],
      "source": [
        "parameter_sets = [\"RAG_parameter_set\",\"RAG_advanced_parameter_set\"]\n",
        "\n",
        "parameters=rag_helper_functions.get_parameter_sets(wslib, parameter_sets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3894eb64-c364-428c-8cf9-8025903bb96a"
      },
      "source": [
        "### Set Watsonx.ai client\n",
        "Below cell uses the watson machine learning credentials to create an API client to interact with the project and deployment space. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d8f9213-7afa-447d-abd7-d828b56fc115"
      },
      "outputs": [],
      "source": [
        "ibm_api_key=parameters['watsonx_ai_api_key']\n",
        "space_uid = parameters['watsonx_ai_space_id']\n",
        "\n",
        "if environment == \"cloud\":\n",
        "    WML_SERVICE_URL = f\"https://{runtime_region}.ml.cloud.ibm.com\"\n",
        "    wml_credentials = Credentials(api_key=parameters['watsonx_ai_api_key'], url=WML_SERVICE_URL)\n",
        "else:\n",
        "    token = os.environ['USER_ACCESS_TOKEN']\n",
        "    wml_credentials=Credentials(token=os.environ['USER_ACCESS_TOKEN'],url=hostname,instance_id='openshift')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd94f9bf-4dd9-4684-a95f-c78b05c89f30"
      },
      "outputs": [],
      "source": [
        "client = APIClient(wml_credentials)\n",
        "client.set.default_project(project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e7ad7e1fbc745ce9bee3f51c7f3d9b9"
      },
      "source": [
        "### Import Prompt Template\n",
        "\n",
        "Imports the prompt template based on the parameter set by the user. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a820cc8586d04a57953d80d42d8eff22"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt_mgr = PromptTemplateManager(\n",
        "                credentials=wml_credentials,\n",
        "                project_id=project_id\n",
        "                )\n",
        "\n",
        "\n",
        "df_prompt = prompt_mgr.list()\n",
        "prompt_template_id=df_prompt.loc[df_prompt['NAME'] == parameters['llm_prompt_template_file'], 'ID'].values[0]\n",
        "prompt_model = prompt_mgr.load_prompt(prompt_template_id)\n",
        "prompt_model_id=prompt_model.model_id\n",
        "\n",
        "print(\"Currently using the \", parameters['llm_prompt_template_file'], \"prompt template.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a826172-7b60-4257-816e-b47669319d78"
      },
      "source": [
        "### Create Connections from the project\n",
        "\n",
        "Retrieves the connection information from the connection asset you created if the asset exists. \\\n",
        "Connects to the vector database depending on whether its an elasticsearch or milvus or watsonx.data milvus or datastax enterprise connection and initilizes a client respectively.\n",
        "\n",
        "**Note:** Datastax is not supported in this cloud version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6484f81b-e85a-41e1-a8b5-16037d58bac2"
      },
      "outputs": [],
      "source": [
        "connection_name=parameters[\"connection_asset\"]\n",
        "if(next((conn for conn in wslib.list_connections() if conn['name'] == connection_name), None)):\n",
        "    print(connection_name, \"Connection found in the project\")\n",
        "    db_connection = wslib.get_connection(connection_name)\n",
        "    \n",
        "    connection_datatypesource_id=client.connections.get_details(db_connection['.']['asset_id'])['entity']['datasource_type']\n",
        "    connection_type = client.connections.get_datasource_type_details_by_id(connection_datatypesource_id)['entity']['name']\n",
        "    \n",
        "    print(\"Successfully retrieved the connection details\")\n",
        "    print(\"Connection type is identified as:\",connection_type)\n",
        "\n",
        "    \n",
        "    ### Testing connection\n",
        "    if connection_type==\"elasticsearch\":\n",
        "        es_client=rag_helper_functions.create_and_check_elastic_client(db_connection, parameters['elastic_search_model_id'])\n",
        "    elif connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "        milvus_credentials = rag_helper_functions.connect_to_milvus_database(db_connection, parameters)\n",
        "    elif connection_type==\"datastax\":\n",
        "        if environment == \"cloud\":\n",
        "            raise ValueError(f\"ERROR! we don't support datastax connection for Cloud as of now\")\n",
        "        datastax_session,datastax_cluster = rag_helper_functions.connect_to_datastax(db_connection, parameters)\n",
        "        #since this is just for a test. we don't need to keep the session alive.\n",
        "        if not datastax_session.is_shutdown:\n",
        "            datastax_session.shutdown()\n",
        "        if not datastax_cluster.is_shutdown:\n",
        "            datastax_cluster.shutdown()\n",
        "\n",
        "    project_connection_id = db_connection['.']['asset_id']\n",
        "    \n",
        "\n",
        "else:\n",
        "    db_connection=\"\"\n",
        "    raise ValueError(f\"No connection named {connection_name} found in the project.\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de20cfef-ba69-40da-9e6c-3c3fc8ab0c1c"
      },
      "source": [
        "The notebook, by default, will look for a log connection asset in the **RAG_parameter_set** (by default named `milvus_connect` or `elasticsearch_connect` or `datastax_connect`). You can set this up by following the instructions in the project readme. This code checks if a specified connection exists in the project. If found, it retrieves the connection details and identifies the connection type. This can either be Elasticsearch or Milvus or Datastax. \\\n",
        "Depending on the connection type, it establishes a connection to the appropriate database. If the connection is not found, it raises an error indicating the absence of the specified connection in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "500f0165-9d6e-4dfe-97f6-f982688e15e4"
      },
      "outputs": [],
      "source": [
        "log_connection_name=parameters[\"log_connection_asset\"]\n",
        "if(next((conn for conn in wslib.list_connections() if conn['name'] == log_connection_name), None)):\n",
        "    print(log_connection_name, \"log Connection found in the project\")\n",
        "    log_db_connection = wslib.get_connection(log_connection_name)\n",
        "    \n",
        "    connection_datatypesource_id=client.connections.get_details(log_db_connection['.']['asset_id'])['entity']['datasource_type']\n",
        "    log_connection_type = client.connections.get_datasource_type_details_by_id(connection_datatypesource_id)['entity']['name']\n",
        "    \n",
        "    print(\"Successfully retrieved the log connection details\")\n",
        "    print(\"Log Connection type is identified as:\",log_connection_type)\n",
        "\n",
        "    ### Testing connection\n",
        "    if log_connection_type==\"elasticsearch\":\n",
        "        log_client=rag_helper_functions.create_and_check_elastic_client(log_db_connection, parameters['elastic_search_model_id'])\n",
        "    elif log_connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "        log_client = rag_helper_functions.connect_to_milvus_database(log_db_connection, parameters)\n",
        "    elif connection_type==\"datastax\":\n",
        "        if environment == \"cloud\":\n",
        "            raise ValueError(f\"ERROR! we don't support datastax connection for Cloud as of now\")\n",
        "        datastax_log_session,datastax_log_cluster = rag_helper_functions.connect_to_datastax(log_db_connection, parameters)\n",
        "        #since this is just for a test. we don't need to keep the session alive.\n",
        "        if not datastax_log_session.is_shutdown:\n",
        "            datastax_log_session.shutdown()\n",
        "        if not datastax_log_cluster.is_shutdown:\n",
        "            datastax_log_cluster.shutdown()\n",
        "        \n",
        "    project_log_connection_id = log_db_connection['.']['asset_id']\n",
        "\n",
        "    \n",
        "else:\n",
        "    db_connection=\"\"\n",
        "    raise ValueError(f\"No connection named {log_connection_name} found in the project.\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "540b3f32647644918144b5847695c2a4"
      },
      "source": [
        " \n",
        "<a id=\"DeploymentSpace\"></a>\n",
        "### Get the Deployment Space details and set default space.\n",
        "\n",
        "In the next steps we save and deploy the pipeline. The pipeline can be saved and deployed in the same way we save and deploy models.\n",
        "\n",
        "Before we deploy a function we must create a deployment space. Watson Machine Learning provides deployment spaces where the user can save, configure and deploy their models. We can save models, functions and data assets in this space.\n",
        "\n",
        "**Creating a Deployment Space** <br>\n",
        "Before we save the function we must create a deployment space. Watson Machine Learning provides deployment spaces where the user can save, configure and deploy their functions or models. We can save models, functions and data assets in this space.\n",
        "If you do not have space already created, you can use Deployment Spaces Dashboard to create one. Follow the steps : \n",
        "* Navigate to Deployments\n",
        "* Click New Deployment Space\n",
        "* Enter Deployment space name, for eg : **'RAG Deployment Space'**\n",
        "\n",
        "**In case of watsonx as a service**\n",
        "* Follow the steps to create the space same as above. \n",
        "* Select Cloud Object Storage\n",
        "* Select Watson Machine Learning instance and press Create\n",
        "* `Under Manage project` > `Services & Integrations` ensure that your WML service that you provisioned is associated there. \n",
        "\n",
        "\n",
        "The steps involved in saving and deploying the pipeline are detailed in the following cells. We will use the `ibm-watson-machine-learning` package to complete these steps.\n",
        "\n",
        "**Setting space id in parameters set**\n",
        "* In your Deployment Space, copy the space ID by going to **Manage >** copy the Space GUID\n",
        "* In your Project, open the `RAG_parameter_set` found in **Data Asset > Configuration**.\n",
        "* Edit the parameter name `watsonx_ai_space_id` and paste the guid here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce293380a4b44d1684386b30e262863c"
      },
      "outputs": [],
      "source": [
        "space = client.set.default_space(space_uid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b4a92a3943f462184e0b324154e7dd7"
      },
      "source": [
        "<a id=\"promote\"></a>\n",
        "### Promote Necessary Assets to the Deployment Space.\n",
        "\n",
        "The following assets are promoted to the deployment space to be used in the deployed function\n",
        "* Elasticsearch / Milvus / Datstax Connection\n",
        "* Parameter sets\n",
        "* LLM Prompt template \n",
        "* RAG Helper functions python script\n",
        "\n",
        "`promote_assets` method in rag_helper function that promotes a specified asset (data assets, connections or parameter) to a deployment space. It first checks if an asset with the given name already exists in the deployment space and, if the parameter `reuse_existing_space_assets` is set to 'True', reuses the existing asset's ID. If the asset does not exist or reuse is not enabled, the function promotes the asset from the project to the deployment space, creating a new asset ID. Finally, it returns the deployment space asset ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b199109-ecd7-4997-9083-c2de51ef442c"
      },
      "outputs": [],
      "source": [
        "connection_id  = rag_helper_functions.promote_assets(client, \"connections\", connection_name, parameters, project_connection_id, project_id, space_uid)\n",
        "\n",
        "if log_connection_name != connection_name:\n",
        "    log_connection_id=rag_helper_functions.promote_assets(client, \"connections\", log_connection_name, parameters, project_log_connection_id, project_id, space_uid)\n",
        "else:\n",
        "    log_connection_id = connection_id\n",
        "        \n",
        "# Depending on elasticsearch connection type, also promote the ELSER/ Multilingual template to the deployment space\n",
        "promote_files_to_space = [\"rag_helper_functions.py\"] + [parameters['elastic_search_template_file']] * (connection_type == \"elasticsearch\")\n",
        "\n",
        "space_asset_dict={}\n",
        "for file in wslib.assets.list_assets('data_asset'):\n",
        "    if file['name'] in promote_files_to_space:\n",
        "        space_asset_dict[file['name']]=rag_helper_functions.promote_assets(client, 'data_assets', file['name'], parameters, file['asset_id'], project_id, space_uid)\n",
        "\n",
        "\n",
        "for parms in wslib.assets.list_assets('parameter_set'):\n",
        "    if parms['name'] in parameter_sets:\n",
        "        space_asset_dict[parms['name']] = rag_helper_functions.promote_assets(client, 'parameter_sets', parms['name'], parameters, parms['asset_id'], project_id, space_uid)\n",
        "        \n",
        "\n",
        "space_asset_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14849b31f28643178d3cd07dcece6bf0"
      },
      "source": [
        "Promote the assets and the prompt template to the space to be used by the deployed function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d3f1ae1-cc26-44e6-b541-9518f3c102bc"
      },
      "source": [
        "## Deploy prompt template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b493980-a809-4c92-b3ec-f0773ea9b88d"
      },
      "source": [
        "This code automates the deployment of a \"QnA with RAG prompt template\" based on whether a deployment ID is provided. If `parameters['prompt_deployment_id']` is empty, the code promotes the prompt template (`prompt_template_id`) to a deployment space (`space_uid`), defines deployment metadata (including the name, configuration, and base model ID), and deploys it, saving the deployment ID. If a deployment ID is already specified, it skips promotion and deployment. This ensures efficient handling of new or existing deployments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbed0464-c209-4d5b-987e-edf5b8488e5f"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE_NAME = \"QnA with RAG prompt template\"\n",
        "if parameters['prompt_deployment_id']:\n",
        "    try:\n",
        "        if not re.match(r\"^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$\", parameters['prompt_deployment_id']):\n",
        "            raise ValueError(\n",
        "                f\"Invalid prompt deployment ID format: '{parameters['prompt_deployment_id']}'. \"\n",
        "                f\"Expected UUID format like 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'. \"\n",
        "                f\"Please provide a valid deployment ID.\"\n",
        "            )\n",
        "        # Verify if the deployment exists\n",
        "        deployment_details = client.deployments.get_details(parameters['prompt_deployment_id'])\n",
        "        prompt_deployment_id = parameters['prompt_deployment_id']\n",
        "        print(f\"Using existing prompt template deployment: {prompt_deployment_id}\")\n",
        "    except Exception as e:\n",
        "        # If deployment does not exist, FAIL instead of creating a new one\n",
        "        error_msg = (\n",
        "            f\"Error: Provided 'prompt_deployment_id' ({parameters['prompt_deployment_id']}) \"\n",
        "            \"is invalid or not found in the deployment space. \"\n",
        "            \"Please provide a correct deployment ID or leave it empty to promote new one.\"\n",
        "        )\n",
        "        raise ValueError(error_msg) from e\n",
        "    \n",
        "else:\n",
        "    try:\n",
        "        # No existing deployment → Promote + Deploy new one\n",
        "        print(\"Promoting & deploy new prompt template to deployment space.\")\n",
        "        space_prompt_template_id = client.spaces.promote(prompt_template_id, project_id, space_uid)\n",
        "        meta_props = {\n",
        "            client.deployments.ConfigurationMetaNames.NAME: PROMPT_TEMPLATE_NAME,\n",
        "            client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
        "            client.deployments.ConfigurationMetaNames.BASE_MODEL_ID: prompt_model_id\n",
        "        }\n",
        "        prompt_deployment_details = client.deployments.create(artifact_uid=space_prompt_template_id, meta_props=meta_props)\n",
        "        prompt_deployment_id = prompt_deployment_details.get(\"metadata\").get(\"id\")\n",
        "        print(f\"Deployed new prompt template: {prompt_deployment_id}\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to fetch/create deployment prompt template: {str(e)}\") from e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f63ae6a440364570ae65fd7259f77148"
      },
      "source": [
        "Asset details of all data assets promoted to the space to be used by the deployed function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4809ba4-6d02-48a5-b7b0-26585e70307d"
      },
      "outputs": [],
      "source": [
        "ai_params = {'space_id': space_uid, \n",
        "             'space_asset_dict': space_asset_dict, \n",
        "             'environment': environment, \n",
        "             'connection_name': connection_name, \n",
        "             'connection_id': connection_id,\n",
        "             \"connection_type\":connection_type,\n",
        "             \"project_id\":parameters.get('watsonx_ai_project_id') or project_id,\n",
        "             'log_connection_id': log_connection_id, \n",
        "             'log_connection_type': log_connection_type,\n",
        "             'prompt_deployment_id':parameters['prompt_deployment_id'] if parameters['prompt_deployment_id'] else prompt_deployment_id,\n",
        "             'log_pii_removal': ('log_pii_removal' in parameters and parameters['log_pii_removal'].lower() == 'true'),\n",
        "             'wml_credentials':wml_credentials.to_dict()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f1e5329-d00f-4b73-bb39-97af9221691a"
      },
      "source": [
        "### Create Software Specifications\n",
        "Below code snippet performs the following actions:\n",
        "\n",
        "It defines various configuration constants for setting up a Python runtime environment, including the base software specification, custom software specification name, package extension name, and a configuration file path. It creates a configuration content string in YAML format that specifies the dependencies needed. The configuration content is then written to a file named `config.yaml`. Metadata properties are prepared for storing the package extension using the configuration file, and these properties are saved to the IBM Watson Machine Learning client. The unique identifier (UID) for the stored package extension is retrieved. Metadata properties are also prepared for a new software specification that uses the base specification, and this is stored to the Watson Machine Learning client. The unique identifier (UID) for the newly created software specification is retrieved. Finally, the package extension is associated with the new software specification by adding the package extension UID to it.\n",
        "\n",
        "The respective software specifications uid as a result is provided as a parameter during the deployment of function, so that pipeline function will have the dependent libraries for execution on the Deployment Space at runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0daefef1-5d93-48d8-9a86-4d2e1a4c3306"
      },
      "outputs": [],
      "source": [
        "BASE_SW_SPEC_NAME = \"runtime-24.1-py3.11\"\n",
        "sw_spec_name = \"rag_qna_sw_spec_\"+str(uuid.uuid1()).split('-')[0]\n",
        "pkg_ext_name = \"rag_qna-py3.11\"\n",
        "CONFIG_PATH = \"config.yaml\"\n",
        "CONFIG_TYPE = \"conda_yml\"\n",
        "CONFIG_CONTENT = f\"\"\"\n",
        "        name: python310\n",
        "        channels:\n",
        "          - empty\n",
        "        dependencies:\n",
        "          - pip:\n",
        "            - langchain\n",
        "            - setuptools\n",
        "            - langchain_milvus==0.1.8\n",
        "            - pymilvus==2.5.6\n",
        "            - langchain-core\n",
        "            - langchain_ibm\n",
        "            - ibm-watsonx-ai=={__version__}\n",
        "            - elasticsearch\n",
        "            - langchain-community\n",
        "            - langchain_elasticsearch==0.3.2\n",
        "            - cassio==0.1.10\n",
        "            - torch>=2.3.0\n",
        "        prefix: /opt/anaconda3/envs/python310\n",
        "\"\"\"\n",
        "with open(CONFIG_PATH, 'w', encoding='utf-8') as f:\n",
        "    f.write(CONFIG_CONTENT)\n",
        "pkg_extn_meta_props = {\n",
        "    client.package_extensions.ConfigurationMetaNames.NAME: pkg_ext_name,\n",
        "    client.package_extensions.ConfigurationMetaNames.TYPE: CONFIG_TYPE\n",
        "}\n",
        "\n",
        "pkg_extn_details = client.package_extensions.store(meta_props=pkg_extn_meta_props, file_path=CONFIG_PATH)\n",
        "pkg_extn_uid = client.package_extensions.get_id(pkg_extn_details)\n",
        "\n",
        "sw_spec_meta_props = {\n",
        "    client.software_specifications.ConfigurationMetaNames.NAME: sw_spec_name,\n",
        "    client.software_specifications.ConfigurationMetaNames.BASE_SOFTWARE_SPECIFICATION: {\n",
        "        'guid': client.software_specifications.get_id_by_name(BASE_SW_SPEC_NAME)\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    sw_spec_details = client.software_specifications.store(meta_props=sw_spec_meta_props)\n",
        "    sw_spec_id = client.software_specifications.get_id(sw_spec_details)\n",
        "\n",
        "    client.software_specifications.add_package_extension(sw_spec_id, pkg_extn_uid)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"\\nExisting software_specification will be used\")\n",
        "    sw_spec_id=client.software_specifications.get_id_by_name(sw_spec_name)\n",
        "    client.software_specifications.add_package_extension(sw_spec_id, pkg_extn_uid)\n",
        "    \n",
        "import os\n",
        "os.remove(CONFIG_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58b8e4320aef447396ba6e88de7d636d"
      },
      "source": [
        "<a id=\"ScoringFunction\"></a>\n",
        "\n",
        "### Deployable AI Service Code\n",
        "The provided Python code implements a Retrieval-Augmented Generation (RAG) pipeline for an AI service, integrating document retrieval, language model inference, hallucination detection, and logging. Below is a concise overview of its key functionalities:\n",
        "\n",
        "**Core Functionalities** \n",
        "\n",
        "`RAG Pipeline`: Orchestrates document retrieval and text generation using IBM Watsonx.ai, supporting both cloud and on-premises environments. <br>\n",
        "`Vector Search`: Utilizes Elasticsearch or Milvus or Datastax for vector-based document retrieval, with optional hybrid search combining dense and BM25 sparse embeddings.<br>\n",
        "`Embedding Generation`: Creates embeddings for queries and documents via Watsonx.ai’s embedding models, configurable for different setups.<br>\n",
        "`Document Retrieval`: Retrieves and ranks relevant documents, merging them to optimize context while respecting size constraints.<br>\n",
        "`Text Generation`: Generates responses using Watsonx.ai models, offering streaming (generate_stream) and non-streaming (generate) modes.<br>\n",
        "`Guardrail Checks`: Detects PII (Personally Identifiable Information) and HAP (Harmful, Abusive, or Profane) content in responses to ensure compliance.<br>\n",
        "`Hallucination Detection`: Validates responses for accuracy using embedding-based cosine similarity or word overlap techniques, flagging potential hallucinations.<br>\n",
        "`Logging System`: Logs queries, responses, timestamps, and metadata in Elasticsearch or Milvus or Datastax, with PII suppression and performance tracking.<br>\n",
        "`Autocomplete Feature`: Provides question autocomplete suggestions based on logged interactions, improving user experience.<br>\n",
        "`Expert Recommendation`: Matches queries to expert profiles stored in a vector index, recommending relevant experts and updating logs.<br>\n",
        "\n",
        "**Key Components**\n",
        "\n",
        "`Config`: Manages configuration, initializes WML API client, and loads parameters and helper functions from Watson Machine Learning (WML) spaces.<br>\n",
        "`EmbeddingModel`: Factory for creating embedding models, adaptable to cloud or on-premises environments.<br>\n",
        "`VectorStoreInterface`: Abstract interface for vector stores, implemented by ElasticsearchVectorStore , MilvusVectorStore and DatastaxVectorStore.<br>\n",
        "`Retriever`: Handles document retrieval with scoring and merging logic.<br>\n",
        "`InferenceModel`: Manages Watsonx.ai model inference for response generation.<br>\n",
        "`GuardrailChecker`: Implements PII and HAP detection.<br>\n",
        "`HallucinationDetector`: Detects inaccuracies in generated text.<br>\n",
        "`rag_logger`: Manages logging, autocomplete, and expert recommendations.<br>\n",
        "\n",
        "Additional Features\n",
        "\n",
        "`Modular Design`: Abstract interfaces and configurable parameters ensure extensibility and adaptability.<br>\n",
        "`Error Handling`: Robust error handling for initialization, API calls, and logging, with detailed error messages.<br>\n",
        "`Performance Monitoring`: Tracks execution times for pipeline stages, aiding optimization.<br>\n",
        "\n",
        "This code provides a comprehensive, enterprise-grade AI service framework, emphasizing accuracy, compliance, and user interaction enhancements.\n",
        "\n",
        "**NOTE**: Hybrid search is not enabled if documents are Bulk ingested in Elasticsearch or Milvus. \n",
        "          Bulk ingestion or Hybrid search is not supported by Datastax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c43ec49-37ff-4fcf-a6b1-fb0806486206"
      },
      "source": [
        "**If the custom query template for Elasticsearch has a different structure/format from the one as expected in the RAG pipeline, then you need to provide a mapper function which can format the response as required in the pipeline.**\n",
        "Follow below steps to add custom document mapper for Elasticsearch query template:\n",
        "1. Create a document mapper function inside the scoring pipeline function. Below is the sample document mapper function for nested query template.\n",
        "   ```\n",
        "       # Document Mapper for nested query template\n",
        "       def document_mapper(hit):\n",
        "           from langchain_core.documents import Document\n",
        "           # 'passages' is the nested field\n",
        "           if 'passages' in hit[\"_source\"]:\n",
        "                passages = hit[\"_source\"]['passages']\n",
        "                    \n",
        "                return [Document(\n",
        "                    vector=passage['sparse'],\n",
        "                    page_content=passage['text'],\n",
        "                    metadata={'_source': {'metadata': {\"page_number\": '', \"source\": hit['_source']['url_path'], \"title\": passage['title'], \"document_url\": passage['url']}}, \"_score\": hit['_score']},\n",
        "                ) for passage in passages ]\n",
        "   ```\n",
        "     <br>\n",
        "2. Comment out the `content_field` and Uncomment the `document_mapper` field in `ElasticsearchRetriever` and provide the added mapper function name as the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fa3563a-9045-4e73-a347-6b270f2fe413"
      },
      "outputs": [],
      "source": [
        "def qna_with_rag_ai_service(context, params=ai_params):\n",
        "    from abc import ABC, abstractmethod\n",
        "    from typing import List, Dict, Any, Optional, Callable, Tuple\n",
        "    import importlib.util\n",
        "    from ibm_watsonx_ai import APIClient, Credentials\n",
        "    \n",
        "    from langchain_ibm import WatsonxLLM\n",
        "    import requests\n",
        "    import os\n",
        "    import json\n",
        "    import time\n",
        "    import hashlib\n",
        "    from datetime import datetime\n",
        "    import re\n",
        "    import math\n",
        "    from functools import reduce\n",
        "    import elasticsearch\n",
        "    from elasticsearch import Elasticsearch\n",
        "    from ibm_watsonx_ai.foundation_models import Embeddings\n",
        "    from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
        "    from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
        "    from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "    from ibm_watsonx_ai.foundation_models import ModelInference\n",
        "    from ibm_watsonx_ai.foundation_models.prompts import PromptTemplate, PromptTemplateManager\n",
        "    from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, PromptTemplateFormats, DecodingMethods\n",
        "    from ibm_watson_studio_lib import access_project_or_space\n",
        "    import string\n",
        "    from langchain.chains import LLMChain\n",
        "    from langchain.schema.runnable import RunnableMap\n",
        "    from collections import Counter\n",
        "    import numpy as np\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    from pymilvus import(IndexType,Status,connections,FieldSchema,DataType,Collection,CollectionSchema,utility)\n",
        "    from langchain_milvus import Milvus, BM25BuiltInFunction    \n",
        "    import copy\n",
        "    from ibm_watsonx_ai.foundation_models.utils import HAPDetectionWarning\n",
        "    import warnings    \n",
        "    from langchain_community.vectorstores import Cassandra\n",
        "    import cassio\n",
        "    warnings.filterwarnings(\"always\", category=HAPDetectionWarning)\n",
        "    \n",
        "\n",
        "    \n",
        "    class Config:\n",
        "        \"\"\"Configuration management for the RAG pipeline\"\"\"\n",
        "    \n",
        "        def __init__(self,context: Any, params: dict):\n",
        "            \"\"\"\n",
        "            Initialize configuration with parameters\n",
        "            \n",
        "            Args:\n",
        "                params: Dictionary containing configuration parameters\n",
        "            \"\"\"\n",
        "            self.context = context\n",
        "            self.params = params\n",
        "            self.client = self._initialize_client()\n",
        "            self.environment = params.get('environment', 'cloud')\n",
        "            self.project_id = params.get('project_id')\n",
        "            self.space_id = params.get('space_id')\n",
        "            self.connection_type = params.get('connection_type')\n",
        "            self.connection_id = params.get('connection_id')\n",
        "            self.log_connection_id = params.get('log_connection_id')\n",
        "            self.space_asset_dict = params.get('space_asset_dict', {})\n",
        "            self.rag_helper = None\n",
        "            self.parameters = {}   \n",
        "            self.streaming = False\n",
        "    \n",
        "            self._load_helper_functions()\n",
        "            self._load_parameters()\n",
        "            self.validate_params()\n",
        "    \n",
        "        def _initialize_client(self) -> APIClient:\n",
        "            \"\"\"Initialize and configure WML API client\"\"\"\n",
        "            try:\n",
        "                wml_credentials = self.params.get('wml_credentials')\n",
        "                space_id = self.params.get('space_id')                                            \n",
        "                \n",
        "                client = APIClient(\n",
        "                    credentials=wml_credentials,\n",
        "                    space_id=space_id\n",
        "                )\n",
        "                \n",
        "                return client\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Error initializing WML client: {str(e)}\")\n",
        "    \n",
        "        def get_client(self) -> APIClient:\n",
        "            \"\"\"Return the initialized WML API client\"\"\"\n",
        "            return self.client\n",
        "    \n",
        "    \n",
        "        def _load_helper_functions(self) -> None:\n",
        "            \"\"\"Load rag_helper_functions from WML space\"\"\"\n",
        "            try:\n",
        "                helper_function_name = 'rag_helper_functions.py'\n",
        "                if helper_function_name not in self.space_asset_dict:\n",
        "                    raise ValueError(f\"{helper_function_name} not found in space_asset_dict\")\n",
        "                    \n",
        "                helper_function_path = self.client.data_assets.download(\n",
        "                    self.space_asset_dict[helper_function_name],\n",
        "                    helper_function_name\n",
        "                )\n",
        "                module_name = os.path.basename(helper_function_path).replace('.py', '')\n",
        "                \n",
        "                spec = importlib.util.spec_from_file_location(module_name, helper_function_path)\n",
        "                if spec is None:\n",
        "                    raise ValueError(f\"Failed to create module spec for {helper_function_path}\")\n",
        "                    \n",
        "                module = importlib.util.module_from_spec(spec)\n",
        "                spec.loader.exec_module(module)\n",
        "                self.rag_helper = module\n",
        "                \n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Error loading helper functions: {str(e)}\")\n",
        "    \n",
        "        def _load_parameters(self) -> None:\n",
        "            \"\"\"Load parameters from WML space parameter sets\"\"\"\n",
        "            try:\n",
        "                space_parameter_dict = {\n",
        "                    key: value for key, value in self.space_asset_dict.items()\n",
        "                    if \"parameter\" in key.lower()\n",
        "                }\n",
        "                parameters_list = []\n",
        "                for param_set_id in space_parameter_dict.values():\n",
        "                    param_details = self.client.parameter_sets.get_details(param_set_id)\n",
        "                    parameters_list.extend(\n",
        "                        param_details['entity']['parameter_set']['parameters']\n",
        "                    )\n",
        "                self.parameters = {param['name']: param['value'] for param in parameters_list}\n",
        "                \n",
        "                # Set specific parameters\n",
        "    \n",
        "                self.index_name = self.parameters.get('vector_store_index_name')\n",
        "                self.model_id = self.parameters.get('elastic_search_model_id')\n",
        "                self.hybrid_search = (\n",
        "                    str(self.parameters.get('milvus_hybrid_search', 'false')).lower() == 'true'\n",
        "                )\n",
        "                self.expert_profiles_index = self.parameters.get('expert_profiles_index', '')\n",
        "                \n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Error initializing parameters: {str(e)}\")\n",
        "    \n",
        "        def validate_params(self) -> None:\n",
        "            \"\"\"Validate required configuration parameters\"\"\"\n",
        "            required = [ 'environment', 'space_id', 'space_asset_dict']\n",
        "            missing = [param for param in required if not getattr(self, param)]\n",
        "    \n",
        "            if missing:\n",
        "                raise ValueError(f\"Missing required parameters: {missing}\")\n",
        "            \n",
        "            #if not self.parameters.get('vector_store_index_name'):\n",
        "            #    raise ValueError(\"vector_store_index_name is required in parameters\")\n",
        "            #if not self.parameters.get('elastic_search_model_id'):\n",
        "            #    raise ValueError(\"elastic_search_model_id is required in parameters\")\n",
        "            #if not self.rag_helper:\n",
        "            #    raise ValueError(\"rag_helper_functions not loaded\")\n",
        "    \n",
        "        def get_parameters(self) -> Dict[str, Any]:\n",
        "            \"\"\"Return all loaded parameters\"\"\"\n",
        "            return self.parameters\n",
        "\n",
        "        def set_streaming():\n",
        "            self.streaming=True\n",
        "\n",
        "        \n",
        "    class EmbeddingModel:\n",
        "        \"\"\"Factory for creating embedding models\"\"\"\n",
        "        \n",
        "        def __init__(self, config: Config):\n",
        "            \"\"\"\n",
        "            Initialize with Config instance\n",
        "            \n",
        "            Args:\n",
        "                config: Config instance containing parameters and credentials\n",
        "            \"\"\"\n",
        "            self.config = config\n",
        "    \n",
        "        def create_embedding(self) -> Embeddings:\n",
        "            \"\"\"Create embedding model based on environment\"\"\"\n",
        "            params = self.config.parameters\n",
        "            if self.config.environment == \"cloud\":\n",
        "                return Embeddings(\n",
        "                    model_id=params['embedding_model_id'],\n",
        "                    credentials=self.config.get_client().wml_credentials, \n",
        "                    space_id=self.config.space_id,\n",
        "                    verify=True\n",
        "                )\n",
        "            elif self.config.environment == \"on-prem\":\n",
        "                if params.get('wx_ai_inference_space_id'):\n",
        "                    print(\"Using IBM Cloud API for on-prem embeddings\")\n",
        "                    credentials = Credentials(\n",
        "                        api_key=params['watsonx_ai_api_key'],\n",
        "                        url=params['watsonx_ai_url']\n",
        "                    )\n",
        "                    return Embeddings(\n",
        "                        model_id=params['embedding_model_id'],\n",
        "                        credentials=credentials,\n",
        "                        space_id=params[\"wx_ai_inference_space_id\"],\n",
        "                        verify=True\n",
        "                    )\n",
        "                return Embeddings(\n",
        "                    model_id=params['embedding_model_id'],\n",
        "                    credentials=self.config.get_client().wml_credentials,\n",
        "                    space_id=self.config.space_id,\n",
        "                    verify=True\n",
        "                )\n",
        "            \n",
        "    class VectorStoreInterface(ABC):\n",
        "        @abstractmethod\n",
        "        def search(self, query: str, **kwargs) -> List[dict]:\n",
        "            pass\n",
        "    \n",
        "        @abstractmethod\n",
        "        def connect(self, connection_details: dict, rag_helper) -> None:\n",
        "            pass\n",
        "    \n",
        "    \n",
        "    class ElasticsearchVectorStore(VectorStoreInterface):\n",
        "        \n",
        "        \"\"\"Elasticsearch vector store implementation\"\"\"\n",
        "        def __init__(self, config: Config):\n",
        "            \"\"\"\n",
        "            Initialize with Config instance\n",
        "            \n",
        "            Args:\n",
        "                config: Config instance containing parameters and credentials\n",
        "            \"\"\"\n",
        "            self.config = config\n",
        "            self.es_client = None\n",
        "    \n",
        "        def connect(self, connection_details: dict, rag_helper: Any) -> None:\n",
        "            \"\"\"Connect to Elasticsearch using helper function\"\"\"\n",
        "            params = self.config.parameters \n",
        "            print(\"reading from vectorstore\")\n",
        "            self.es_client = rag_helper.create_and_check_elastic_client(\n",
        "                connection_details, \n",
        "                params['elastic_search_model_id']\n",
        "            )\n",
        "    \n",
        "        def search(self, query: str, **kwargs) -> List[dict]:\n",
        "            \"\"\"Search Elasticsearch index\"\"\"\n",
        "            params = self.config.parameters\n",
        "            from langchain_elasticsearch import ElasticsearchRetriever\n",
        "            if 'filter' in kwargs:\n",
        "                original_filter = kwargs.pop('filter')\n",
        "                es_filter = [{'match': {f'{k}.keyword': {'query': v}}} for k, v in original_filter.items()]\n",
        "                kwargs['filter'] = es_filter     \n",
        "                \n",
        "                \n",
        "            retriever = ElasticsearchRetriever(\n",
        "                es_client=self.es_client,\n",
        "                index_name=params[\"vector_store_index_name\"],\n",
        "                body_func=self._create_query_template,\n",
        "                content_field=\"text\",\n",
        "                search_kwargs=kwargs\n",
        "            )\n",
        "            results = retriever.invoke(query)\n",
        "            docs = [\n",
        "                {\n",
        "                    \"page_content\": doc.page_content,\n",
        "                    \"metadata\": doc.metadata['_source']['metadata'],\n",
        "                    \"score\": doc.metadata.get('_score', 0) or doc.metadata.get('_rank', 0)\n",
        "                } for doc in results\n",
        "            ]\n",
        "            return self.config.rag_helper.merge_documents(\n",
        "                docs, \n",
        "                params.get('document_source_field', '')\n",
        "            )\n",
        "    \n",
        "        def _create_query_template(self, query: str) -> dict:\n",
        "            \"\"\"Create Elasticsearch query template\"\"\"\n",
        "            params = self.config.parameters\n",
        "            template_content = self.config.client.data_assets.get_content(\n",
        "                self.config.space_asset_dict[params['elastic_search_template_file']]\n",
        "            )\n",
        "            template = json.loads(template_content)\n",
        "            template_str = json.dumps(template)\n",
        "            if 'dense' in params['elastic_search_vector_type']:\n",
        "                from langchain_elasticsearch import ElasticsearchEmbeddings\n",
        "                embeddings = ElasticsearchEmbeddings.from_es_connection(\n",
        "                            model_id=params['elastic_search_model_id'],\n",
        "                            es_connection=self.es_client,\n",
        "                        )\n",
        "                query_vector = embeddings.embed_documents([query])[0]\n",
        "                template_str = template_str.replace('\"{{query_vector}}\"', str(query_vector))\n",
        "            else:\n",
        "                template_str = template_str.replace(\"{{model_id}}\", params['elastic_search_model_id'])\n",
        "                template_str = template_str.replace(\"{{model_text}}\", query)\n",
        "            return json.loads(template_str)\n",
        "    \n",
        "    \n",
        "    class MilvusVectorStore(VectorStoreInterface):\n",
        "        \"\"\"Milvus vector store implementation\"\"\"\n",
        "        \n",
        "        def __init__(self, config: Config, embedding_model: EmbeddingModel):\n",
        "            \"\"\"\n",
        "            Initialize with Config instance and EmbeddingModel\n",
        "            \n",
        "            Args:\n",
        "                config: Config instance containing parameters and credentials\n",
        "                embedding_model: EmbeddingModel instance for creating embeddings\n",
        "            \"\"\"\n",
        "            self.config = config\n",
        "            self.vector_store = None\n",
        "            self.expert_profile_vector_store = None\n",
        "            self.embedding = embedding_model.create_embedding()\n",
        "    \n",
        "        def connect(self, connection_details: dict, rag_helper: Any) -> None:\n",
        "            \"\"\"Connect to Milvus using helper function\"\"\"\n",
        "            params = self.config.parameters\n",
        "            connection_details['database'] = connection_details.get('database', 'default')\n",
        "            milvus_credentials = rag_helper.connect_to_milvus_database(connection_details, params)\n",
        "            \n",
        "            print(f\"Using model {params['embedding_model_id']} to create dense embeddings\")\n",
        "            dense_index_param = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 1024}}\n",
        "            \n",
        "            if self.config.hybrid_search:\n",
        "                sparse_index_param = {\n",
        "                    \"metric_type\": \"BM25\",\n",
        "                    \"index_type\": \"SPARSE_INVERTED_INDEX\",\n",
        "                    \"params\": {\"drop_ratio_build\": 0.2}\n",
        "                }\n",
        "                print(\"Using BM25 sparse embeddings\")\n",
        "                self.vector_store = Milvus(\n",
        "                    embedding_function=self.embedding,\n",
        "                    builtin_function=BM25BuiltInFunction(output_field_names=\"sparse\"),\n",
        "                    index_params=[dense_index_param, sparse_index_param],\n",
        "                    vector_field=[\"dense\", \"sparse\"],\n",
        "                    connection_args=milvus_credentials,\n",
        "                    primary_field='id',\n",
        "                    consistency_level=\"Strong\",\n",
        "                    collection_name=params[\"vector_store_index_name\"]\n",
        "                )\n",
        "            else:\n",
        "                self.vector_store = Milvus(\n",
        "                    embedding_function=self.embedding,\n",
        "                    index_params=dense_index_param,\n",
        "                    connection_args=milvus_credentials,\n",
        "                    primary_field='id',\n",
        "                    consistency_level=\"Strong\",\n",
        "                    collection_name=params[\"vector_store_index_name\"]\n",
        "                )\n",
        "            print(\"Milvus Vector Store Created\")\n",
        "            \n",
        "            if params[\"expert_profiles_index\"]:\n",
        "                \n",
        "                self.expert_profile_vector_store = Milvus(\n",
        "                    embedding_function=self.embedding,\n",
        "                    connection_args=milvus_credentials,\n",
        "                    index_params=dense_index_param,\n",
        "                    primary_field='id',\n",
        "                    collection_name=params[\"expert_profiles_index\"]\n",
        "                )\n",
        "                print(\"Milvus Vector Store Created for expert profile for \", self.expert_profile_vector_store.collection_name)\n",
        "    \n",
        "        def search(self, query: str, **kwargs) -> List[dict]:\n",
        "            \"\"\"Search Milvus index\"\"\"\n",
        "            if 'filter' in kwargs:\n",
        "                filter_value = kwargs.pop('filter')  # Remove 'filter' and get its value\n",
        "                expr = ' and '.join(f'{k.split(\".\")[-1]} == \"{v}\"' for k, v in filter_value.items())\n",
        "                kwargs['expr'] = expr\n",
        "\n",
        "            params = self.config.parameters\n",
        "            if self.config.hybrid_search:\n",
        "                results = self.vector_store.similarity_search_with_score(\n",
        "                    query, \n",
        "                    ranker_type=\"weighted\", \n",
        "                    ranker_params={\"weights\": [0.5, 0.5]}, \n",
        "                    **kwargs\n",
        "                )\n",
        "            else:\n",
        "                results = self.vector_store.similarity_search_with_score_by_vector(\n",
        "                    self.embedding.embed_query(query), \n",
        "                    **kwargs\n",
        "                )\n",
        "            docs = [\n",
        "                {\"page_content\": doc.page_content, \"metadata\": doc.metadata, \"score\": score}\n",
        "                for doc, score in results\n",
        "            ]\n",
        "            return self.config.rag_helper.merge_documents(\n",
        "                docs, \n",
        "                params.get('document_source_field', '')\n",
        "            )\n",
        "    \n",
        "    class DataStaxVectorStore(VectorStoreInterface):\n",
        "        def __init__(self, config: Config, embedding_model: EmbeddingModel):\n",
        "            \"\"\"\n",
        "            Initialize with Config instance and EmbeddingModel\n",
        "    \n",
        "            Args:\n",
        "                config: Config instance containing parameters and credentials\n",
        "                embedding_model: EmbeddingModel instance for creating embeddings\n",
        "            \"\"\"\n",
        "            self.config = config\n",
        "            self.vector_store = None\n",
        "            self.datastax_session= None\n",
        "            self.datastax_cluster = None\n",
        "            self.expert_profile_vector_store = None\n",
        "            self.embedding = embedding_model.create_embedding()\n",
        "            self.status=True\n",
        "    \n",
        "        def connect(self) -> None:\n",
        "            \"\"\"Connect to DataStax (Cassandra) using DatastaxConnector\"\"\"\n",
        "            \n",
        "            params = self.config.parameters\n",
        "            try:\n",
        "                connection_details=self.config.client.connections.get_details(self.config.connection_id)['entity']['properties']\n",
        "                self.datastax_session, self.datastax_cluster = self.config.rag_helper.connect_to_datastax(\n",
        "                    connection_details,\n",
        "                    self.config.parameters\n",
        "                )\n",
        "                import cassio\n",
        "                cassio.init(\n",
        "                    session=self.datastax_session,\n",
        "                    keyspace=connection_details.get('keyspace')\n",
        "                )\n",
        "                print(f\"Using model {params['embedding_model_id']} to create embeddings\")\n",
        "        \n",
        "                # Initialize main vector store\n",
        "                self.vector_store = Cassandra(\n",
        "                    embedding=self.embedding,\n",
        "                    table_name=params[\"vector_store_index_name\"]\n",
        "                )\n",
        "                print(\"DataStax Vector Store Created\")\n",
        "                \n",
        "        \n",
        "                # Initialize expert profile vector store if specified\n",
        "                if params.get(\"expert_profiles_index\"):\n",
        "                    self.expert_profile_vector_store = Cassandra(\n",
        "                        embedding=self.embedding,\n",
        "                        table_name=params[\"expert_profiles_index\"]\n",
        "                    )\n",
        "                    print(\"DataStax Vector Store Created for expert profile for \", params[\"expert_profiles_index\"])\n",
        "            except Exception as e:\n",
        "                print(f\" ERROR in Datastax Connection : {e}\") \n",
        "                self.status=False\n",
        "        \n",
        "        def search(self, query: str, **kwargs) -> List[dict]:\n",
        "            \"\"\"Search DataStax (Cassandra) index\"\"\"\n",
        "            params = self.config.parameters\n",
        "            # Handle filter if provided\n",
        "            if 'filter' in kwargs:\n",
        "                filter_value = kwargs.pop('filter')\n",
        "                expr = ' and '.join(f'{k.split(\".\")[-1]} == \"{v}\"' for k, v in filter_value[0].items())\n",
        "                kwargs['filter'] = expr\n",
        "    \n",
        "            # Perform similarity search with scores\n",
        "            results = self.vector_store.similarity_search_with_score_by_vector(\n",
        "                self.embedding.embed_query(query),\n",
        "                k=params.get('vectorsearch_top_n_results', 5)\n",
        "            )\n",
        "    \n",
        "            # Format results\n",
        "            docs = [\n",
        "                {\"page_content\": doc.page_content, \"metadata\": doc.metadata, \"score\": score}\n",
        "                for doc, score in results\n",
        "            ]\n",
        "            return self.config.rag_helper.merge_documents(\n",
        "                docs,\n",
        "                params.get('document_source_field', '')\n",
        "            )\n",
        "\n",
        "    \n",
        "    class Retriever:\n",
        "        \"\"\"Handles document retrieval with scoring\"\"\"\n",
        "        \n",
        "        def __init__(self, config: Config, vector_store: VectorStoreInterface):\n",
        "            \"\"\"\n",
        "            Initialize with Config and vector store\n",
        "            \n",
        "            Args:\n",
        "                config: Config instance containing parameters\n",
        "                vector_store: Initialized vector store instance\n",
        "            \"\"\"\n",
        "            self.config = config\n",
        "            self.vector_store = vector_store\n",
        "    \n",
        "        def retrieve_with_scores(self, query: str, filter: Optional[Dict] = None) -> Tuple[List[Dict], str]:\n",
        "            \"\"\"Retrieve documents with relevance scores\"\"\"\n",
        "            params = self.config.parameters\n",
        "            k_factor = (\n",
        "                params['ingestion_chunk_size'] / \n",
        "                (params['ingestion_chunk_size'] - params['ingestion_chunk_overlap'])\n",
        "                if params['ingestion_chunk_size'] > params['ingestion_chunk_overlap'] else 1.0\n",
        "            )\n",
        "    \n",
        "            search_kwargs = {\n",
        "                \"k\": math.floor(params['vectorsearch_top_n_results'] * k_factor),\n",
        "                \"score_threshold\": float(params['rag_es_min_score']),\n",
        "                \"include_scores\": True,\n",
        "                \"verbose\": True\n",
        "            }\n",
        "            if filter:\n",
        "                search_kwargs[\"filter\"] = filter\n",
        "\n",
        "    \n",
        "            docs_with_scores = self.vector_store.search(query, **search_kwargs)\n",
        "            docs_with_scores_all = sorted(docs_with_scores[0], key=lambda x: x['score'], reverse=True)\n",
        "            last_doc_index = params['vectorsearch_top_n_results']\n",
        "            if last_doc_index > len(docs_with_scores_all):\n",
        "                last_doc_index = len(docs_with_scores_all)\n",
        "            docs_with_scores = docs_with_scores_all[:last_doc_index]\n",
        "            free_capacity = 0\n",
        "    \n",
        "            while True:\n",
        "                docs_with_scores, length_reduction = self.config.rag_helper.merge_documents(\n",
        "                    docs_with_scores, \n",
        "                    params.get('document_source_field', '')\n",
        "                )\n",
        "                free_capacity += length_reduction\n",
        "                quit_loop = True\n",
        "                while (last_doc_index < len(docs_with_scores_all) and \n",
        "                       free_capacity >= len(docs_with_scores_all[last_doc_index]['page_content'])):\n",
        "                    docs_with_scores.append(docs_with_scores_all[last_doc_index])\n",
        "                    free_capacity -= len(docs_with_scores_all[last_doc_index]['page_content'])\n",
        "                    last_doc_index += 1\n",
        "                    quit_loop = False\n",
        "                if quit_loop:\n",
        "                    break\n",
        "    \n",
        "            formatted_docs = \"\".join([f\"[Document]\\n{d['page_content']}[End]\\n\\n\" for d in docs_with_scores])\n",
        "            return docs_with_scores, formatted_docs\n",
        "        \n",
        "    \n",
        "    class InferenceModel:\n",
        "        \"\"\"Manages Watsonx.ai model inference\"\"\"\n",
        "        \n",
        "        def __init__(self, config: Config):\n",
        "            \"\"\"\n",
        "            Initialize with Config instance\n",
        "            \n",
        "            Args:\n",
        "                config: Config instance containing parameters and credentials\n",
        "            \"\"\"\n",
        "            self.config = config\n",
        "            self.model = self._initialize_model()\n",
        "    \n",
        "        def _initialize_model(self) -> ModelInference:\n",
        "            \"\"\"Initialize Watsonx.ai model\"\"\"\n",
        "            params = self.config.parameters\n",
        "            try:\n",
        "                if params.get('wx_ai_inference_space_id'):\n",
        "                    print(\"Running inference on watsonx.ai cloud from on-prem\")\n",
        "                    space_credentials = Credentials(\n",
        "                        api_key=params['watsonx_ai_api_key'],\n",
        "                        url=params['watsonx_ai_url']\n",
        "                    )\n",
        "                    return ModelInference(\n",
        "                        deployment_id=self.config.params['prompt_deployment_id'],\n",
        "                        credentials=space_credentials,\n",
        "                        space_id=params['wx_ai_inference_space_id']\n",
        "                    )\n",
        "                print(\"Running prompt on watsonx.ai\")\n",
        "\n",
        "                return ModelInference(\n",
        "                    deployment_id=self.config.params['prompt_deployment_id'],\n",
        "                    api_client=self.config.client\n",
        "                )\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Error initializing model: {str(e)}\")\n",
        "    \n",
        "    \n",
        "    class GuardrailChecker:\n",
        "        \"\"\"Manages PII and HAP detection\"\"\"\n",
        "        \n",
        "        def __init__(self, config: Config):\n",
        "            \"\"\"\n",
        "            Initialize with Config instance\n",
        "            \n",
        "            Args:\n",
        "                config: Config instance containing parameters\n",
        "            \"\"\"\n",
        "            self.config = config\n",
        "    \n",
        "        def check_for_pii_hap(self, llm_response: Dict) -> Tuple[bool, bool]:\n",
        "            \"\"\"Check for PII and HAP in LLM response\"\"\"\n",
        "            pii_flag = False\n",
        "            hap_flag = False\n",
        "            try:\n",
        "                result_dict = llm_response['results'][0]\n",
        "\n",
        "                if result_dict.get('moderations'):\n",
        "                    if result_dict.get('moderations').get('pii'):\n",
        "                        print(f\"PII detected.. Score: {result_dict['moderations']['pii'][0]['score']}\")\n",
        "                        pii_flag = True\n",
        "                    if result_dict.get('moderations').get('hap'):\n",
        "                        print(f\"HAP detected.. Score: {result_dict['moderations']['hap'][0]['score']}\")\n",
        "                        hap_flag = True\n",
        "            except Exception:\n",
        "                print(\"Moderations check failure. Please check the LLM response.\")\n",
        "            print(f\"PII Flag: {pii_flag}\")\n",
        "            print(f\"HAP Flag: {hap_flag}\")\n",
        "            return pii_flag, hap_flag\n",
        "    \n",
        "    class RAGPipeline:\n",
        "        \"\"\"Orchestrates RAG pipeline components\"\"\"\n",
        "        \n",
        "        def __init__(self, config: Config):\n",
        "            \"\"\"\n",
        "            Initialize with Config instance\n",
        "            \n",
        "            Args:\n",
        "                config: Config instance containing parameters and credentials\n",
        "            \"\"\"\n",
        "            self.config = config\n",
        "            self.vector_store = self._initialize_vector_store()\n",
        "            self.inference_model = InferenceModel(config)\n",
        "            self.retriever = Retriever(config, self.vector_store)\n",
        "            self.guardrail_checker = GuardrailChecker(config)\n",
        "    \n",
        "        def _initialize_vector_store(self) -> VectorStoreInterface:\n",
        "            \"\"\"Initialize appropriate vector store\"\"\"\n",
        "            try:\n",
        "                connection_details = self.config.client.connections.get_details(self.config.params['connection_id'])['entity']['properties']\n",
        "                \n",
        "                if self.config.connection_type == \"elasticsearch\":\n",
        "                    vector_store = ElasticsearchVectorStore(self.config)\n",
        "                    vector_store.connect(connection_details, self.config.rag_helper)\n",
        "                elif self.config.connection_type in [\"milvus\",\"milvuswxd\"]:\n",
        "                    embedding_model = EmbeddingModel(self.config)\n",
        "                    vector_store = MilvusVectorStore(self.config, embedding_model)\n",
        "                    vector_store.connect(connection_details, self.config.rag_helper)\n",
        "                elif self.config.connection_type == \"datastax\":\n",
        "                    embedding_model = EmbeddingModel(self.config)\n",
        "                    vector_store = DataStaxVectorStore(self.config, embedding_model)\n",
        "                    vector_store.connect()\n",
        "                return vector_store\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Error initializing vector store: {str(e)}\")\n",
        "    \n",
        "        def call_runnable_map(self, inputs: Dict, streaming: bool = False) -> Dict:\n",
        "            \"\"\"Execute the RAG pipeline\"\"\"\n",
        "            params = self.config.parameters\n",
        "            query = inputs[\"query\"]\n",
        "            filter = inputs.get(\"filter\")\n",
        "    \n",
        "            ai_guardrails = params.get('ai_guardrails', 'false').strip().lower() == 'true'\n",
        "            guardrails_params = {\n",
        "                \"guardrails\": ai_guardrails,\n",
        "                \"guardrails_pii_params\": {\"input\": {\"enabled\":params.get('enable_pii_detection', 'false').strip().lower() == 'true'}},\n",
        "                \"guardrails_hap_params\": {\"threshold\": params['guardrails_hap_threshold']}\n",
        "            } if ai_guardrails else {}\n",
        "    \n",
        "            print(\"LLM chain created\")\n",
        "            \n",
        "            # Step 1: Retrieve documents and format context\n",
        "            docs_with_scores, formatted_context = self.retriever.retrieve_with_scores(query, filter)\n",
        "            \n",
        "            # Step 2: Prepare input for the model\n",
        "            runnable_inputs = {\n",
        "                \"context\": formatted_context,\n",
        "                \"question\": query\n",
        "            }\n",
        "            \n",
        "            # Step 3: Directly call the model\n",
        "            #llm_response = self.inference_model.model.generate_text_stream(\n",
        "            #    params={\"prompt_variables\": runnable_inputs}\n",
        "            #)\n",
        "            #for chunk in llm_response:\n",
        "            #    print(chunk)\n",
        "\n",
        "            if streaming == True:\n",
        "                llm_response = self.inference_model.model.generate_text_stream(\n",
        "                    params={\"prompt_variables\": runnable_inputs})\n",
        "                return llm_response\n",
        "                \n",
        "            else:\n",
        "                llm_response = self.inference_model.model.generate_text(\n",
        "                        params={\"prompt_variables\": runnable_inputs}, \n",
        "                        raw_response=True, \n",
        "                        **guardrails_params\n",
        "                    )\n",
        "                \n",
        "                pii_flag, hap_flag = self.guardrail_checker.check_for_pii_hap(llm_response)\n",
        "                return {\n",
        "                    \"answer\": llm_response['results'][0]['generated_text'],\n",
        "                    \"context\": docs_with_scores,\n",
        "                    \"pii_flag\": pii_flag,\n",
        "                    \"hap_flag\": hap_flag\n",
        "                }\n",
        "        \n",
        "    \n",
        "    class HallucinationDetector:\n",
        "        \"\"\"Detects hallucinations in generated text using embeddings\"\"\"\n",
        "        \n",
        "        def __init__(self, config: Config, embedding_model: EmbeddingModel):\n",
        "            \"\"\"\n",
        "            Initialize with Config instance and EmbeddingModel\n",
        "            \n",
        "            Args:\n",
        "                config: Config instance containing parameters and thresholds\n",
        "                embedding_model: EmbeddingModel instance for creating embeddings\n",
        "            \"\"\"\n",
        "            self.config = config\n",
        "            self.embedding = embedding_model.create_embedding()\n",
        "    \n",
        "    \n",
        "        def validate_answer_against_sources(self,response_answer, source_documents, similarity_threshold=0.5):\n",
        "            source_texts = [doc for doc in source_documents]\n",
        "            \n",
        "            answer_embedding = self.embedding.embed_query(response_answer)\n",
        "            source_embeddings = self.embedding.embed_documents(source_texts)\n",
        "            answer_embedding = np.array(answer_embedding).reshape(1, -1)  # Reshape for cosine_similarity\n",
        "            source_embeddings = np.array(source_embeddings)\n",
        "            cosine_scores = cosine_similarity(answer_embedding, source_embeddings)[0]  # Get the first row since answer is 1D\n",
        "      \n",
        "            if any(score > similarity_threshold for score in cosine_scores):\n",
        "                matching_scores = [score for score in cosine_scores if score > similarity_threshold]\n",
        "                \n",
        "                confidence_score = max(matching_scores)\n",
        "                return {'isHallucination': False, 'confidence_score': confidence_score}\n",
        "            else:\n",
        "                confidence_score = np.mean(cosine_scores)\n",
        "                return {'isHallucination': True, 'confidence_score': float(confidence_score)}\n",
        "    \n",
        "        def is_hallucination(self, response_answer, source_documents, threshold_overlap_max=0.3,\n",
        "                             threshold_overlap_score_concat=0.4):\n",
        "            stop_words = {\n",
        "                'a', 'about', 'after', 'all', 'also', 'am', 'an', 'and', 'another', 'any', 'are', 'as', 'at',\n",
        "                'be', 'because', 'been', 'before', 'being', 'between', 'both', 'but', 'by',\n",
        "                'came', 'can', 'come', 'could', 'did', 'do', 'each', 'for', 'from', 'get', 'got',\n",
        "                'has', 'had', 'he', 'have', 'her', 'here', 'him', 'himself', 'his', 'how',\n",
        "                'i', 'if', 'in', 'into', 'is', 'it', 'like',\n",
        "                'make', 'many', 'me', 'might', 'more', 'most', 'much', 'must', 'my', 'never', 'now',\n",
        "                'of', 'on', 'only', 'or', 'other', 'our', 'out', 'over',\n",
        "                'said', 'same', 'should', 'since', 'some', 'still', 'such',\n",
        "                'take', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'these', 'they', 'this',\n",
        "                'those', 'through', 'to', 'too', 'under', 'up', 'very', 'was', 'way', 'we', 'well', 'were',\n",
        "                'what', 'where', 'which', 'while', 'who', 'with', 'would', 'you', 'your'\n",
        "            }\n",
        "    \n",
        "            regex = re.compile(r\"\\b\\w+(?:['-_]\\w+)?\\b\")\n",
        "    \n",
        "            def calculate_textual_overlap(text1, text2):\n",
        "                # tokenize\n",
        "                text1_tokens = regex.findall(text1.lower())\n",
        "                text2_tokens = regex.findall(text2.lower())\n",
        "    \n",
        "                # remove stop words\n",
        "                text1_tokens = [t for t in text1_tokens if t not in stop_words]\n",
        "                text2_tokens = [t for t in text2_tokens if t not in stop_words]\n",
        "    \n",
        "                # compute overlap\n",
        "                if len(text1_tokens) > 0:\n",
        "                    text1_tokens = set(text1_tokens)\n",
        "                    text2_tokens = set(text2_tokens)\n",
        "                    return len(text1_tokens.intersection(text2_tokens)) / len(text1_tokens)\n",
        "                else:\n",
        "                    return 0\n",
        "    \n",
        "            def overlap_score_concat(generated_text, passages):\n",
        "                passages_text = ' '.join(passages)\n",
        "                return calculate_textual_overlap(generated_text, passages_text)\n",
        "    \n",
        "            def overlap_score_max(generated_text, passages):\n",
        "                return max([calculate_textual_overlap(generated_text, passage_text) for passage_text in passages])\n",
        "    \n",
        "            # Get the LLM response along with source document text\n",
        "            llm_response_text = response_answer\n",
        "            source_docs_passages = [doc for doc in source_documents]\n",
        "    \n",
        "            result_overlap_score_max = 0.0\n",
        "            result_overlap_score_concat = 0.0\n",
        "    \n",
        "            if len(source_docs_passages):\n",
        "    \n",
        "                # Call overlap score calculations\n",
        "                result_overlap_score_max = overlap_score_max(llm_response_text, source_docs_passages)\n",
        "                result_overlap_score_concat = overlap_score_concat(llm_response_text, source_docs_passages)\n",
        "    \n",
        "                print('result_overlap_score_max: ' + str(result_overlap_score_max))\n",
        "                print('result_overlap_score_concat: ' + str(result_overlap_score_concat))\n",
        "    \n",
        "                # Define thresholds (these should be put in a parameter set or template)\n",
        "                # Need to experiment with what values to use\n",
        "    \n",
        "                response = {'isHallucination': False,\n",
        "                            'text': \"\",\n",
        "                            'maxOverlapScore': round(result_overlap_score_max, 2),\n",
        "                            'concatOverlapScore': round(result_overlap_score_concat, 2)}\n",
        "    \n",
        "                # Decide if hallucination or not\n",
        "                if result_overlap_score_max > threshold_overlap_max and result_overlap_score_concat > threshold_overlap_score_concat:\n",
        "    \n",
        "                    response['isHallucination'] = False\n",
        "                else:\n",
        "                    print(\"Hallucination\")\n",
        "                    response['isHallucination'] = True\n",
        "                    response['text'] = \"Sorry, I can not generate a valid answer to your question.\"\n",
        "            else:\n",
        "                response = {'isHallucination': True,\n",
        "                            'text': \"Sorry, I can not find an answer to your question in the available documents.\",\n",
        "                            'maxOverlapScore': round(result_overlap_score_max, 2),\n",
        "                            'concatOverlapScore': round(result_overlap_score_concat, 2)}\n",
        "    \n",
        "            return response\n",
        "\n",
        "    class rag_logger:\n",
        "        def __init__(self, config: Config, embedding_model: EmbeddingModel):\n",
        "            \"\"\"\n",
        "            Initialize logger with Config and EmbeddingModel instances\n",
        "            \n",
        "            Args:\n",
        "                config: Config instance containing parameters and client\n",
        "                embedding_model: EmbeddingModel instance for creating embeddings\n",
        "            \"\"\"\n",
        "            self.config = config\n",
        "            self.client = config.get_client()\n",
        "            self.params=config.params\n",
        "            self.rag_helper_functions = config.rag_helper\n",
        "            self.embedding = embedding_model.create_embedding()\n",
        "            self.parameters = config.get_parameters()\n",
        "            self.log_client = None\n",
        "            self.log_milvus_credentials = None\n",
        "            # Default embedding dimension (adjust based on your model)\n",
        "            self.embedding_dim = self.parameters.get('embedding_dim', 768)\n",
        "            self.log_connection_type = self.config.params['log_connection_type']\n",
        "            \n",
        "            self.dense_index_param = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\", \"params\": {\"nlist\": 1024}}\n",
        "            connection_details = self.config.client.connections.get_details(self.config.connection_id)['entity']['properties']\n",
        "            if self.config.params['connection_type'] in [\"milvus\" ,\"milvuswxd\"]:\n",
        "                \n",
        "                self.milvus_vector_store = MilvusVectorStore(self.config, embedding_model)\n",
        "                self.milvus_vector_store.connect(connection_details, self.config.rag_helper)\n",
        "            elif self.config.params['connection_type'] ==\"datastax\":\n",
        "                self.datastax_vector_store = DataStaxVectorStore(self.config, embedding_model)\n",
        "                self.datastax_vector_store.connect()\n",
        "            else:\n",
        "                self.es_store = ElasticsearchVectorStore(self.config)\n",
        "                self.es_store.connect(connection_details, self.config.rag_helper)\n",
        "                                \n",
        "            #milvus_vector_store.connect(connection_details, self.config.rag_helper)\n",
        "\n",
        "            \n",
        "            \n",
        "            print(\"logging\", self.parameters.get('log_index_name', ''))\n",
        "            if not 'log_index_name' in self.parameters or self.parameters['log_index_name'] == '':\n",
        "                self.logger_status = \"logging is deactivated\"\n",
        "            elif not 'log_connection_type' in self.config.params or self.log_connection_type == '':\n",
        "                self.logger_status = \"logging is deactivated\"\n",
        "            else:\n",
        "                self.log_index_name = self.parameters['log_index_name']\n",
        "                self.field_names = []\n",
        "                try:\n",
        "                    self.log_connection = self.client.connections.get_details(self.params['log_connection_id'])['entity']['properties']\n",
        "                    \n",
        "                except:\n",
        "                    self.log_connection = {key[4:]: self.parameters[key] for key in self.parameters.keys() if key.startswith('log_')}\n",
        "                try:\n",
        "                    \n",
        "                    print(\"Log Connection type is identified as:\", self.log_connection_type)\n",
        "                    if self.log_connection_type == \"elasticsearch\":\n",
        "                        print(\"rag_logger_create\")\n",
        "                        self.log_client = self.rag_helper_functions.create_and_check_elastic_client(self.log_connection, self.parameters['elastic_search_model_id'])\n",
        "                        idx_create_status = self.log_client.options(ignore_status=400).indices.create(index=self.log_index_name, body={'mappings': {'properties': {'question': {'type': 'completion'}}}})\n",
        "                        self.logger_status = f\"logger: connection established, log index {self.log_index_name} {'created' if not 'status' in idx_create_status else 'already exists' if idx_create_status['status'] == 400 else 'DEFECTIVE'}\"\n",
        "                    elif self.log_connection_type == \"milvus\" or self.log_connection_type == \"milvuswxd\":\n",
        "                        self.log_milvus_credentials = self.rag_helper_functions.connect_to_milvus_database(self.log_connection,self.parameters)\n",
        "                        \n",
        "                        self.log_client = self.log_connection_type\n",
        "                        collection_name = self.parameters['log_index_name']\n",
        "                        if collection_name not in utility.list_collections():\n",
        "                            fields = [\n",
        "                                FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=65535),\n",
        "                                FieldSchema(name=\"question\", dtype=DataType.VARCHAR, max_length=65535),\n",
        "                                FieldSchema(name=\"response\", dtype=DataType.VARCHAR, max_length=65535),\n",
        "                                FieldSchema(name=\"source_documents\", dtype=DataType.JSON),\n",
        "                                FieldSchema(name=\"Hallucination_Detection\", dtype=DataType.JSON),\n",
        "                                FieldSchema(name=\"log_timestamp\", dtype=DataType.VARCHAR, max_length=65535),\n",
        "                                FieldSchema(name=\"feedback\", dtype=DataType.JSON),\n",
        "                                FieldSchema(name=\"expert_details\", dtype=DataType.JSON),\n",
        "                                FieldSchema(name=\"elapsed_times\", dtype=DataType.JSON),\n",
        "                                FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim)\n",
        "                            ]\n",
        "                            self.field_names = [field.name for field in fields]\n",
        "                            schema = CollectionSchema(fields, description=\"Collection for document embeddings\", enable_dynamic_field=True)\n",
        "                            self.collection = Collection(name=collection_name, schema=schema)\n",
        "                            \n",
        "                            self.collection.create_index(field_name=\"vector\", index_params=self.dense_index_param)\n",
        "                            self.logger_status = f\"logger: connection established, log index {self.log_index_name} created.\"\n",
        "                        else:\n",
        "                            self.collection = Collection(name=collection_name)\n",
        "                            self.field_names = [field.name for field in self.collection.schema.fields]\n",
        "                            self.logger_status = f\"logger: connection established, log index {self.log_index_name} already exists.\"\n",
        "                        self.collection.load()\n",
        "                        print(\"Milvus log Connection Created\")\n",
        "                    elif self.log_connection_type == \"datastax\":\n",
        "                        self.datastax_session = cassio.config.resolve_session()\n",
        "                        self.datastax_keyspace = cassio.config.resolve_keyspace()\n",
        "                        self.datastax_session.execute((\n",
        "                            \"CREATE TABLE IF NOT EXISTS {keyspace}.{log_index_name} (id TEXT PRIMARY KEY, \"\n",
        "                            \"question TEXT, response TEXT, source_documents TEXT, hallucination_detection TEXT, \"\n",
        "                            \"log_timestamp timestamp , feedback TEXT, expert_details TEXT, elapsed_times TEXT, \"\n",
        "                            \"vector VECTOR<FLOAT,{v_dimension}>);\"\n",
        "                        ).format(keyspace=self.datastax_keyspace, log_index_name=self.log_index_name, v_dimension=self.embedding_dim))\n",
        "                        self.logger_status = \"logging is activated\"\n",
        "                        self.log_client=self.log_connection_type\n",
        "                except Exception as e:\n",
        "                    self.logger_status = f\"logger: ERROR: {str(e)}\"\n",
        "                    self.log_client = None\n",
        "                # PII detection\n",
        "                self.pii_detection_model = None\n",
        "                pii_status = 'inactive'\n",
        "                self.pii_model_syntax = None\n",
        "                self.pii_model_transformer = None\n",
        "                self.pii_model_rbr = None\n",
        "                watson_nlp_available = False  # Adjust based on your environment\n",
        "                if watson_nlp_available:\n",
        "                    gpu_available = False\n",
        "                    try:\n",
        "                        hw_spec = os.environ['RUNTIME_HARDWARE_SPEC']\n",
        "                        if 'num_gpu' in json.loads(hw_spec):\n",
        "                            gpu_available = True\n",
        "                    except:\n",
        "                        pass\n",
        "                    try:\n",
        "                        self.pii_model_syntax = watson_nlp.load('syntax_izumo_en_stock')\n",
        "                        self.pii_model_transformer = watson_nlp.load(f\"entity-mentions_transformer-workflow_multilingual_slate.153m.distilled{'-cpu' if not gpu_available else ''}\")\n",
        "                        self.pii_model_rbr = watson_nlp.load('entity-mentions_rbr_multi_pii')\n",
        "                        pii_status = 'active'\n",
        "                    except:\n",
        "                        pii_status = 'model load failed'\n",
        "                self.logger_status = f\"{self.logger_status}, PII suppression status: {pii_status}\"\n",
        "    \n",
        "        def status(self):\n",
        "            return self.logger_status if self.logger_status else 'unknown'\n",
        "    \n",
        "        def remove_pii(self, text):\n",
        "            if self.pii_model_transformer and self.pii_model_rbr:\n",
        "                for model_run in [lambda text: self.pii_model_transformer.run(text, language_code='en'), lambda text: self.pii_model_rbr.run(text, language_code='en')]:\n",
        "                    pii_detection = model_run(text)\n",
        "                    begin = 0\n",
        "                    parts = []\n",
        "                    for _pii in pii_detection.mentions:\n",
        "                        parts.append(text[begin:_pii.span.begin])\n",
        "                        begin = _pii.span.end\n",
        "                    parts.append(text[begin:len(text)])\n",
        "                    text = 'XXX'.join(parts)\n",
        "            return text\n",
        "    \n",
        "        def generate_hash(self, content):\n",
        "            return hashlib.sha256(content.encode()).hexdigest()\n",
        "    \n",
        "        def create_log_record(self, question, content, times, feedback, log_id=None):\n",
        "            if log_id is None:\n",
        "                log_id = self.generate_hash(str(datetime.now().timestamp()))\n",
        "\n",
        "            \n",
        "            content['source_documents']=content.get('source_documents',[])\n",
        "            if self.log_client:\n",
        "                log_record = {**content, 'log_timestamp': datetime.now().isoformat()}\n",
        "                if question:\n",
        "                    log_record['question'] = self.remove_pii(question)\n",
        "                \n",
        "                if feedback:\n",
        "                    log_record['feedback'] = feedback\n",
        "                \n",
        "                if content and 'response' in content:\n",
        "                    log_record['response'] = self.remove_pii(log_record['response'])\n",
        "                if times:\n",
        "                    times.append(('pii_removal', time.perf_counter()))\n",
        "                    log_record['elapsed_times'] = {times[i][0]: \"{:.3f}\".format(times[i][1] - times[i - 1][1]) for i in range(1, len(times))}\n",
        "                \n",
        "                res = None\n",
        "                if self.log_connection_type == 'elasticsearch':\n",
        "                    res = self.log_client.index(index=self.log_index_name,id=log_id, document=log_record)\n",
        "                    self.log_client.indices.refresh()\n",
        "                elif self.log_connection_type in ['milvus', 'milvuswxd']:\n",
        "                    \n",
        "                    log_record.update({\n",
        "                        'id': log_id,\n",
        "                        'vector': self.embedding.embed_documents([log_record['question']])[0],\n",
        "                        'Hallucination_Detection': log_record.get('Hallucination Detection',''),\n",
        "                        'feedback': {},\n",
        "                        'expert_details': []\n",
        "                    })\n",
        "                    #log_record.pop('Hallucination Detection')\n",
        "                    res = self.collection.insert([log_record])\n",
        "                    self.collection.flush()\n",
        "                    print(f'inserted data: {res.primary_keys[0]}')\n",
        "                elif self.log_connection_type==\"datastax\":\n",
        "                    log_record.update({\n",
        "                        'id': log_id,\n",
        "                        #'vector': self.embedding.embed_documents([log_record['question']])[0],\n",
        "                        'Hallucination_Detection': log_record.get('Hallucination Detection',''),\n",
        "                        'feedback': log_record.get('feedback',''),\n",
        "                        'expert_details': log_record.get('expert_details','')\n",
        "                    })\n",
        "                    time_data =log_record['log_timestamp']\n",
        "                    format_string = \"%Y-%m-%dT%H:%M:%S.%f\"\n",
        "                    # Convert the string to a datetime object\n",
        "                    log_record['log_timestamp'] = datetime.strptime(time_data, format_string)\n",
        "                    insert_log_query=self.datastax_session.prepare(f\"INSERT INTO {self.datastax_keyspace}.{self.log_index_name} (id, question, response, source_documents, vector, hallucination_detection, log_timestamp, feedback, expert_details, elapsed_times) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?) IF NOT EXISTS\")\n",
        "                    res=self.datastax_session.execute(insert_log_query, (log_id, log_record.get('question',''), str(log_record.get('response','')), str(log_record.get('source_documents','')), self.embedding.embed_documents([log_record.get('question', '')])[0], str(log_record.get('Hallucination Detection','')), log_record.get('log_timestamp',''), str(log_record.get('feedback','')), str(log_record.get('expert_details', '')), str(log_record.get('elapsed_times','')), ))\n",
        "                    print(\"insert successful\")\n",
        "                    status = 'log inserted'\n",
        "                if res is not None:\n",
        "                    log_id = res['_id'] if self.log_connection_type == 'elasticsearch' else res.primary_keys[0] if self.log_connection_type in ['milvus','milvuswxd'] else log_id\n",
        "                    status = 'ok'\n",
        "                else:\n",
        "                    status = 'log cannot be written'\n",
        "            else:\n",
        "                status = self.logger_status\n",
        "            return status, log_id\n",
        "        \n",
        "        def update_log_record(self, fields):\n",
        "            if self.log_client:\n",
        "                if 'log_id' not in fields:\n",
        "                    status = \"log id not provided\"\n",
        "                else:\n",
        "                    log_id = fields['log_id']\n",
        "                    #fields.pop(\"log_id\")\n",
        "                    if not fields:\n",
        "                        status = \"field to be updated not provided\"\n",
        "                    else:\n",
        "                        try:\n",
        "                            update_query = {'expert_details': fields['expert_details']} if 'expert_details' in fields else {'feedback': fields}\n",
        "                            if self.log_connection_type == 'elasticsearch':\n",
        "                                idx_upd_status = self.log_client.options(ignore_status=404).update(index=self.log_index_name, id=log_id, doc=update_query)\n",
        "                                status = \"ok\" if 'status' not in idx_upd_status else (f\"log id {str(log_id)} not found\" if idx_upd_status['status'] == 404 else f\"update failed, status {str(idx_upd_status['status'])}\")\n",
        "                            elif self.log_connection_type in ['milvus', 'milvuswxd']:\n",
        "                                existing_record = self.collection.query(expr=f\"id=='{log_id}'\", output_fields=self.field_names)\n",
        "                                if existing_record:\n",
        "                                    existing_record = existing_record[0]\n",
        "                                    existing_record.update(update_query)\n",
        "                                    res = self.collection.upsert([existing_record])\n",
        "                                    status = 'ok'\n",
        "                                else:\n",
        "                                    status = f\"log id {str(log_id)} not found\"\n",
        "                            elif self.log_connection_type==\"datastax\":\n",
        "                               \n",
        "                                #json_log_record = json.dumps(update_query,indent=4)\n",
        "                                #print(json_log_record)\n",
        "                                #insert_json_log_record=self.datastax_session.prepare(f\"\"\"\n",
        "                                #INSERT INTO  {self.datastax_keyspace}.{self.log_index_name} JSON ?\n",
        "                                #\"\"\")\n",
        "                                try:\n",
        "                                    if update_query.get(\"expert_details\",'') != '':\n",
        "                                        insert_expert_log_record=self.datastax_session.prepare(f\"\"\"\n",
        "                                    INSERT INTO {self.datastax_keyspace}.{self.log_index_name} (id , expert_details) VALUES ( ?, ?)\n",
        "                                    \"\"\")\n",
        "                                        self.datastax_session.execute(insert_expert_log_record, (log_id, str(update_query.get(\"expert_details\",'')),))\n",
        "                                    if update_query.get(\"feedback\",'') != '':\n",
        "                                        insert_feedback_log_record=self.datastax_session.prepare(f\"\"\"\n",
        "                                    INSERT INTO {self.datastax_keyspace}.{self.log_index_name} (id , feedback) VALUES ( ?, ?)\n",
        "                                    \"\"\")\n",
        "                                        self.datastax_session.execute(insert_feedback_log_record, (log_id, str(update_query.get(\"feedback\",'')),))\n",
        "                                    status = 'ok'\n",
        "                                    print(\"datastax updated successfully\")\n",
        "                                except Exception as e:\n",
        "                                    print(f\"Failed to update on datastax : {e}\")\n",
        "                                    status = \"update failed\"\n",
        "                                \n",
        "                        except Exception as e:\n",
        "                            status = f\"update of index {self.log_index_name} failed: {str(e)}\"\n",
        "            else:\n",
        "                status = self.logger_status\n",
        "            return status\n",
        "        \n",
        "        def retrieve_log_record(self, log_id):\n",
        "            expert_details = []\n",
        "            question = None\n",
        "            if self.log_client:\n",
        "                try:\n",
        "                    if self.log_connection_type == 'elasticsearch':\n",
        "                        response = self.log_client.search(index=self.log_index_name, query={'term': {'_id': log_id}})\n",
        "                        if response['hits']['hits']:\n",
        "                            question = response['hits']['hits'][0]['_source']['question']\n",
        "                            expert_details = response['hits']['hits'][0]['_source'].get('expert_details', [])\n",
        "                    elif self.log_connection_type in ['milvus', 'milvuswxd']:\n",
        "                        response = self.collection.query(expr=f\"id=='{log_id}'\", output_fields=['expert_details', 'question'])\n",
        "                        if response:\n",
        "                            response = response[0]\n",
        "                            question = response.get('question')\n",
        "                            expert_details = response.get('expert_details', [])\n",
        "                    elif self.log_connection_type==\"datastax\":\n",
        "                        select_log_query = self.datastax_session.prepare(f\"SELECT id, question,expert_details FROM {self.datastax_keyspace}.{self.log_index_name} WHERE id = ?\")\n",
        "                        results=self.datastax_session.execute(select_log_query, (log_id,))\n",
        "                        #results = datastax_session.execute(f\"SELECT * FROM {self.datastax_keyspace}.{self.log_index_name}\")\n",
        "                        for row in results:\n",
        "                            #print(f\"Retrieved row: ID={row.id}, Question={row.question}, Vector = {row.expert_details}\")\n",
        "                            question=row.question\n",
        "                            expert_details=row.expert_details\n",
        "                            #print (row.question, row.expert_details)\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    return None, f\"Error retrieving log records: {str(e)}\"\n",
        "            return question, expert_details\n",
        "    \n",
        "    \n",
        "        def remove_duplicate_questions(self, autocomplete_dict):\n",
        "            \"\"\"\n",
        "            Remove duplicate questions (case-insensitive) from autocomplete results, keeping first occurrence.\n",
        "            \n",
        "            Args:\n",
        "                autocomplete_dict: List of dictionaries containing 'question' and 'response'.\n",
        "            \n",
        "            Returns:\n",
        "                List: List of dictionaries with unique questions.\n",
        "            \"\"\"\n",
        "            seen = set()\n",
        "            result = []\n",
        "            for item in autocomplete_dict:\n",
        "                key = item[\"question\"].strip().lower()\n",
        "                if key not in seen:\n",
        "                    seen.add(key)\n",
        "                    result.append(item)\n",
        "            return result\n",
        "\n",
        "        def sort_questions_by_occurrence(self, autocomplete_dict):\n",
        "            counts = Counter(item[\"question\"].strip().lower() for item in autocomplete_dict)\n",
        "            return sorted(autocomplete_dict, key=lambda x: counts[x[\"question\"].strip().lower()], reverse=True)\n",
        "\n",
        "    \n",
        "        def get_completion_options(self, input_autocomplete_fields):\n",
        "            \"\"\"\n",
        "            Retrieve autocomplete suggestions for a given question prefix.\n",
        "            \n",
        "            Args:\n",
        "                input_autocomplete_fields: Dictionary with '_function', '_question_prefix', and optional 'limit'.\n",
        "            \n",
        "            Returns:\n",
        "                Tuple: (status, autocomplete_dict) where autocomplete_dict is a list of question-response pairs.\n",
        "            \"\"\"\n",
        "            autocomplete_dict = []\n",
        "            if self.log_client:\n",
        "                if input_autocomplete_fields.get('_function') != \"_auto_complete\":\n",
        "                    status = \"_autocomplete not provided\"\n",
        "                else:\n",
        "                    status = \"ok\"\n",
        "                    input_autocomplete_fields = input_autocomplete_fields.copy()\n",
        "                    input_autocomplete_fields.pop(\"_function\")\n",
        "                    if not input_autocomplete_fields:\n",
        "                        status = \"input_autocomplete_fields values not provided\"\n",
        "                    elif '_question_prefix' not in input_autocomplete_fields:\n",
        "                        status = \"_question_prefix not provided\"\n",
        "                    else:\n",
        "                        if 'limit' not in input_autocomplete_fields:\n",
        "                            input_autocomplete_fields['limit'] = 5\n",
        "                        try:\n",
        "                            question_prefix = input_autocomplete_fields[\"_question_prefix\"]\n",
        "                            if self.log_connection_type == 'elasticsearch':\n",
        "                                body = {\n",
        "                                    'suggest': {\n",
        "                                        'question': {\n",
        "                                            'prefix': question_prefix,\n",
        "                                            'completion': {'field': 'question', 'skip_duplicates': True, 'size': input_autocomplete_fields['limit']}\n",
        "                                        }\n",
        "                                    },\n",
        "                                    \"_source\": [\"response\", \"question\"]\n",
        "                                }\n",
        "                                res = self.log_client.search(index=self.log_index_name, body=body)\n",
        "                                if 'suggest' in res and 'question' in res['suggest'] and res['suggest']['question'] and 'options' in res['suggest']['question'][0]:\n",
        "                                    options = res['suggest']['question'][0]['options']\n",
        "                                    autocomplete_dict = [\n",
        "                                        {'question': opt['_source']['question'], 'response': opt['_source']['response']}\n",
        "                                        for opt in options if '_source' in opt and 'question' in opt['_source'] and 'response' in opt['_source']\n",
        "                                    ]\n",
        "                            elif self.log_connection_type  in ['milvus', 'milvuswxd']:\n",
        "                                raw_prefix = input_autocomplete_fields[\"_question_prefix\"].strip()\n",
        "                                question_prefix = raw_prefix.replace(\"'\", \"\\\\'\").lower()\n",
        "                                user_question_matching_records = self.collection.query(\n",
        "                                    expr=\"question != ''\",\n",
        "                                    output_fields=[\"question\", \"response\"]\n",
        "                                )\n",
        "                                filtered = [\n",
        "                                    item for item in user_question_matching_records\n",
        "                                    if item[\"question\"].strip().lower().startswith(question_prefix)\n",
        "                                ]\n",
        "                                autocomplete_dict = [\n",
        "                                    {'question': item['question'], 'response': item['response']}\n",
        "                                    for item in filtered\n",
        "                                ]\n",
        "                                if not autocomplete_dict:\n",
        "                                    return \"No log record found!\", autocomplete_dict\n",
        "\n",
        "                            elif self.log_connection_type == \"datastax\":\n",
        "                                #mode_type = \"CONTAINS\"\n",
        "                                raw_prefix = input_autocomplete_fields[\"_question_prefix\"].strip()\n",
        "                                question_prefix = raw_prefix.replace(\"'\", \"\\\\'\").lower()\n",
        "                                #question_prefix_like=f'%{question_prefix}%'\n",
        "                                # fix mode_type encoding issue to use below index later for better performance on search prefix like.\n",
        "                                #self.datastax_session.execute(f\"\"\"\n",
        "                                #    CREATE CUSTOM INDEX IF NOT EXISTS qna_question_sasi_idx ON {self.datastax_keyspace}.{self.log_index_name} (question)\n",
        "                                #    USING 'org.apache.cassandra.index.sasi.SASIIndex'\n",
        "                                #    WITH OPTIONS = {'mode': '{mode_type}'};\n",
        "                                #\"\"\")\n",
        "                                \n",
        "                                #print(\"qna_question_sasi_idx SASI index created (or already exists).\")\n",
        "\n",
        "                                \n",
        "                                select_log_query = self.datastax_session.prepare(f\"SELECT question,response FROM {self.datastax_keyspace}.{self.log_index_name} WHERE question >= ? LIMIT ? ALLOW FILTERING\")\n",
        "                                user_question_matching_records=self.datastax_session.execute(select_log_query, ( question_prefix , input_autocomplete_fields['limit']))\n",
        "                                autocomplete_dict=[]\n",
        "                                for item in user_question_matching_records:\n",
        "                                    row_item={'question': item.question, 'response': item.response}\n",
        "                                    autocomplete_dict.append(row_item)\n",
        "                                if not autocomplete_dict:\n",
        "                                    return \"No log record found!\", autocomplete_dict\n",
        "\n",
        "                            \n",
        "                            # Remove duplicates and sort by frequency\n",
        "                            autocomplete_dict = self.remove_duplicate_questions(autocomplete_dict)\n",
        "                            autocomplete_dict = self.sort_questions_by_occurrence(autocomplete_dict)\n",
        "                            # Limit to requested number of results\n",
        "                            autocomplete_dict = autocomplete_dict[:input_autocomplete_fields['limit']]\n",
        "                            status = \"ok\"\n",
        "                            \n",
        "                        except Exception as e:\n",
        "                            status = f\"Autocomplete failed: {str(e)}\"\n",
        "            else:\n",
        "                status = self.logger_status\n",
        "            return status, autocomplete_dict\n",
        "    \n",
        "        def get_top_experts(self, fields):\n",
        "            \"\"\"\n",
        "            Retrieve top experts based on a log record's query and update the log with expert details.\n",
        "            \n",
        "            Args:\n",
        "                fields: Dictionary containing at least 'log_id' and optionally other parameters.\n",
        "            \n",
        "            Returns:\n",
        "                Dict: Response with recommended experts and status.\n",
        "            \"\"\"\n",
        "            query = None\n",
        "            expert_details = []\n",
        "            status = \"invalid parameters\"\n",
        "    \n",
        "            if 'log_id' not in fields:\n",
        "                return {{\"function\": \"get_top_experts\", \"status\": status}}\n",
        "            \n",
        "            log_id = fields.pop('log_id')\n",
        "            \n",
        "            try:\n",
        "                query, expert_details = self.retrieve_log_record(log_id)\n",
        "                status = 'expert_details retrieved from log records'\n",
        "    \n",
        "                print('query: ', query)\n",
        "    \n",
        "                if query is None:\n",
        "                    status = f'log id {log_id} not found'\n",
        "                \n",
        "                if query is not None and (len(expert_details) == 0 or (len(expert_details) == 1 and expert_details[0] == '')):\n",
        "                    top_k_experts = self.parameters.get('top_k_experts', 1)\n",
        "                    es_search_kwargs = {\n",
        "                        \"size\": top_k_experts,\n",
        "                        \"min_score\": float(self.parameters.get('expert_profiles_es_min_score', 0.0)),\n",
        "                    }\n",
        "                    \n",
        "                    milvus_search_params = {\"metric_type\": \"L2\", \"params\": {\"radius\": 1.07}}\n",
        "                    \n",
        "                    if not self.parameters.get(\"expert_profiles_index\"):\n",
        "                        status = \"No expert profile index is provided\"\n",
        "                    else:\n",
        "                        if self.log_connection_type == \"elasticsearch\":\n",
        "                            es_query = self.es_store._create_query_template(query)\n",
        "                            query_temp_args = es_query\n",
        "                            if 'sub_searches' in es_query:\n",
        "                                query_temp_args = {'body': es_query}\n",
        "                                \n",
        "                            results = self.log_client.search(index=self.parameters[\"expert_profiles_index\"], **es_search_kwargs, **query_temp_args)\n",
        "                            expert_details = [\n",
        "                                {\n",
        "                                    'expert_id': doc['_source']['metadata']['document_id'],\n",
        "                                    'name': doc['_source']['metadata'].get('name', ''),\n",
        "                                    'email': doc['_source']['metadata'].get('email', ''),\n",
        "                                    'phone': doc['_source']['metadata'].get('phone', ''),\n",
        "                                    'domain': doc['_source']['metadata'].get('domain', ''),\n",
        "                                    'position': doc['_source']['metadata'].get('position', ''),\n",
        "                                    'source': doc['_source']['metadata'].get('source', ''),\n",
        "                                    'text': doc['_source']['text']\n",
        "                                } for doc in results['hits']['hits']\n",
        "                            ]\n",
        "                        elif self.log_connection_type in [\"milvus\", \"milvuswxd\"]:\n",
        "\n",
        "                            results = self.milvus_vector_store.expert_profile_vector_store.similarity_search_with_score_by_vector(\n",
        "                                    self.embedding.embed_query(query),\n",
        "                                    k=top_k_experts,\n",
        "                                    param=milvus_search_params\n",
        "                                )\n",
        "                                \n",
        "                            expert_details = [\n",
        "                                {\n",
        "                                    'expert_id': doc[0].metadata['document_id'],\n",
        "                                    'name': doc[0].metadata.get('name', ''),\n",
        "                                    'email': doc[0].metadata.get('email', ''),\n",
        "                                    'phone': doc[0].metadata.get('phone', ''),\n",
        "                                    'domain': doc[0].metadata.get('domain', ''),\n",
        "                                    'position': doc[0].metadata.get('position', ''),\n",
        "                                    'source': doc[0].metadata.get('source', ''),\n",
        "                                    'text': doc[0].page_content\n",
        "                                } for doc in results\n",
        "                            ]\n",
        "                        elif self.log_connection_type == \"datastax\":\n",
        "                            if self.datastax_vector_store:\n",
        "                                expert_details_record= self.datastax_vector_store.expert_profile_vector_store.similarity_search_with_score_by_vector(\n",
        "                                        self.embedding.embed_query(query),\n",
        "                                        k=top_k_experts\n",
        "                                    )\n",
        "\n",
        "                                expert_details = [\n",
        "                                    {\n",
        "                                      'expert_id': doc[0].metadata['document_id'],\n",
        "                                        'name': doc[0].metadata.get('name', ''),\n",
        "                                        'email': doc[0].metadata.get('email', ''),\n",
        "                                        'phone': doc[0].metadata.get('phone', ''),\n",
        "                                        'domain': doc[0].metadata.get('domain', ''),\n",
        "                                        'position': doc[0].metadata.get('position', ''),\n",
        "                                        'source': doc[0].metadata.get('source', ''),\n",
        "                                        'text': doc[0].page_content\n",
        "                                    } for doc in expert_details_record\n",
        "                                ]\n",
        "                            else: \n",
        "                                status=\"Datastax Connection failed\"\n",
        "                            \n",
        "                        else:\n",
        "                            status = \"Expert profile vector store not initialized\"\n",
        "\n",
        "                        if len(expert_details) > 0:\n",
        "                            expert_update_log = {'log_id': log_id, 'expert_details': expert_details}\n",
        "                            status = self.update_log_record(expert_update_log)\n",
        "                            status = \"log updated with expert_details\" if status == 'ok' else status\n",
        "                        else:\n",
        "                            status = \"No relevant expert profile is available\"\n",
        "            except Exception as e:\n",
        "                status = f'Error recommending top experts: {str(e)}'\n",
        "                \n",
        "            expert_response = {\"recommended_top_experts\":expert_details,\n",
        "                               \"expert_status\":status\n",
        "                              }\n",
        "            \n",
        "            return {'body': expert_response}\n",
        "\n",
        "    #Main\n",
        "    def initialize_rag(context: Any, params: dict) -> tuple:\n",
        "        \"\"\"\n",
        "        Initialize the RAG pipeline components.\n",
        "        \n",
        "        Args:\n",
        "            context: Object providing get_token and get_json methods.\n",
        "            params: Dictionary containing configuration parameters.\n",
        "        \n",
        "        Returns:\n",
        "            Tuple: (config, connector, log_connector, logging)\n",
        "        \"\"\"\n",
        "        embed_model=None\n",
        "        config=None\n",
        "        logger=None\n",
        "        rag_pipeline=None\n",
        "        hallucination_detector=None\n",
        "        try:\n",
        "            config = Config(context, params)\n",
        "            if config.parameters['log_connection_asset'] != \"\":\n",
        "                logging = True\n",
        "\n",
        "            embed_model=EmbeddingModel(config)\n",
        "            logger = rag_logger(config, embed_model)\n",
        "            hallucination_detector = HallucinationDetector(config, embed_model)\n",
        "            rag_pipeline = RAGPipeline(config)\n",
        "            print(\"RAG initialization completed\")\n",
        "                    \n",
        "            return config, logging, embed_model, logger, hallucination_detector, rag_pipeline\n",
        "            \n",
        "        except Exception as e:\n",
        "            \n",
        "            raise ValueError(f\"Initialization error: {str(e)}\")\n",
        "    \n",
        "    config, logging, embed_model, logger, hallucination_detector, rag_pipeline = initialize_rag(context,params)\n",
        "\n",
        "  \n",
        "    \n",
        "    \n",
        "\n",
        "    def generate(context: Any) -> Dict:\n",
        "            \"\"\"\n",
        "            Process input context to generate a response\n",
        "            \n",
        "            Args:\n",
        "                context: Object providing get_token and get_json methods\n",
        "            \n",
        "            Returns:\n",
        "                Dict: Response with query and result\n",
        "            \"\"\"\n",
        "        \n",
        "            try:\n",
        "\n",
        "                # Instantiate Config and get API client\n",
        "\n",
        "                \n",
        "                streaming = False\n",
        "                api_client = config.get_client()\n",
        "\n",
        "                \n",
        "                # Set token on API client\n",
        "                api_client.set_token(context.get_token())\n",
        "                \n",
        "                # Process payload\n",
        "                payload = context.get_json()\n",
        "                       \n",
        "                scoring_response_txt = {}\n",
        "                times = [('start', time.perf_counter())]\n",
        "\n",
        "        \n",
        "\n",
        "                \n",
        "                try:\n",
        "                    question = payload.get(\"question\")\n",
        "                    query_filter=payload.get(\"query_filter\", None)\n",
        "                    #fields = context.get_path_suffix()\n",
        "                    suffix = context.get_path_suffix()\n",
        "                    suffix = suffix.lstrip(\"/\").split('?')[0]\n",
        "                    if suffix not in [\"\",\"qna\",\"recommended_experts\", \"auto_complete\", \"log_feedback\"]:\n",
        "                        return {\"body\":{\"status\":f\"Invalid path_suffix {suffix}\"}}\n",
        "                        \n",
        "                    #if not isinstance(fields, dict):\n",
        "                        #fields = {}\n",
        "                    \n",
        "                except:\n",
        "                    return {\"body\":{\"path_suffix\": context.get_path_suffix(),\"status\":\"Invalid payload format\"}}\n",
        "                    \n",
        "        \n",
        "                if not question and not payload:\n",
        "                    return {\"body\":{ \"path_suffix\": context.get_path_suffix(), \"status\":\"No valid payload provided\"}}\n",
        "                \n",
        "        \n",
        "    \n",
        "                fields_dict = {}\n",
        "                if suffix=='log_feedback':\n",
        "                    print(\"Calling insert\")\n",
        "                    #status, log_id = logger.create_log_record(None, payload, None, None) if logger else (\"log not available\", \"\")\n",
        "                    status = logger.update_log_record(payload) if logger else \"log not available\"\n",
        "                    return {\"body\":{\"path_suffix\": context.get_path_suffix(),\"log_id\":payload.get('log_id'),\"status\":status}}\n",
        "                print('fields: ', fields_dict)\n",
        "\n",
        "\n",
        "\n",
        "                if suffix == 'recommended_experts':\n",
        "                    # return {'yes':'yes'}\n",
        "                    fields_dict ={'_function':'recommended_experts'}\n",
        "                    fields_dict.update(payload)\n",
        "                    return logger.get_top_experts(fields_dict)\n",
        "                elif suffix == 'auto_complete':\n",
        "                    \n",
        "                    fields_dict ={'_function':'_auto_complete'}\n",
        "                    fields_dict.update(payload)\n",
        "                    times.append(('start_auto_complete', time.perf_counter()))\n",
        "                    status, options = logger.get_completion_options(fields_dict) if logger else (\"feedback log not available\", [])\n",
        "                    times.append(('get_completion_options', time.perf_counter()))\n",
        "                    elapsed = {times[i][0]: \"{:.3f}\".format(times[i][1] - times[i-1][1]) for i in range(1, len(times))}\n",
        "                    return {\"body\":{\"path_suffix\": context.get_path_suffix(),\"options\":options,\"status\":status}}\n",
        "            \n",
        "                if 'log_id' in payload:\n",
        "                    #fields_dict = {\"log_id\":payload.get('log_id')}\n",
        "                    status = logger.update_log_record(payload) if logger else \"log not available\"\n",
        "                    return {\"body\":{\"status\":status, \"path\": context.get_path_suffix()}}\n",
        "        \n",
        "        \n",
        "        \n",
        "                try:\n",
        "                    inputs = {\"query\": question}\n",
        "                    if query_filter:\n",
        "                        inputs[\"filter\"] = query_filter\n",
        "            \n",
        "                    llm_response = rag_pipeline.call_runnable_map(inputs, streaming)\n",
        "                    times.append(('llm_chain', time.perf_counter()))\n",
        "            \n",
        "                    scoring_response_txt['response'] = llm_response['answer']\n",
        "                    scoring_response_txt['source_documents'] = llm_response['context']\n",
        "                    llm_response_documents = [i['page_content'] for i in llm_response['context']]\n",
        "                    \n",
        "                    # Handle PII/HAP flags\n",
        "                    if llm_response['pii_flag']:\n",
        "                        scoring_response_txt['response'] = \"There was Personally Identifiable Information detected in the input context.\"\n",
        "                        hallucination_dict = {\"Technique\": \"None\", \"isHallucination\": False}\n",
        "                    elif llm_response['hap_flag']:\n",
        "                        scoring_response_txt['response'] = \"There was Harmful, Abusive, or Profane (HAP) detected in the input context.\"\n",
        "                        hallucination_dict = {\"Technique\": \"None\", \"isHallucination\": False}\n",
        "                    else:\n",
        "                        # Check for hallucinations\n",
        "                        if llm_response['context']:\n",
        "                            if config.get_parameters()['default_hallucination_technique']==\"word_overlap\":\n",
        "                                hallucination_dict = hallucination_detector.is_hallucination(\n",
        "                                llm_response['answer'], \n",
        "                                llm_response_documents,config.get_parameters()['hallucination_threshold_max_text_overlap'],config.get_parameters()['hallucination_threshold_concatenated_text_overlap']\n",
        "                            )\n",
        "                            else:\n",
        "                                hallucination_dict = hallucination_detector.validate_answer_against_sources(\n",
        "                                    llm_response['answer'], \n",
        "                                    llm_response_documents\n",
        "                                )\n",
        "                            \n",
        "                            if hallucination_dict['isHallucination']:\n",
        "                                scoring_response_txt['response'] = \"Sorry, I cannot find an answer to your question in the available documents.\"\n",
        "                        else:\n",
        "                            scoring_response_txt['response'] = \"Sorry, I cannot find an answer to your question in the available documents.\"\n",
        "                            hallucination_dict = {\"Technique\": \"None\", \"isHallucination\": True}\n",
        "            \n",
        "                    scoring_response_txt['Hallucination Detection'] = hallucination_dict\n",
        "                    times.append(('hallucination_detection', time.perf_counter()))\n",
        "            \n",
        "                except Exception as e:\n",
        "                    return {\"body\":{\"path_suffix\": context.get_path_suffix(),\"status\":f\"LLM response error: {str(e)}\"}}\n",
        "            \n",
        "                # Extract unique source document references\n",
        "                try:\n",
        "                    document_title_field = 'metadata.title'.split('.')\n",
        "                    document_url_field = 'metadata.document_url'.split('.')\n",
        "                    urls = sorted([\n",
        "                        {\n",
        "                            'title': reduce(lambda a, i: a[i], document_title_field, doc) if document_title_field[0] in doc else '',\n",
        "                            'url': reduce(lambda a, i: a[i], document_url_field, doc) if document_url_field[0] in doc else '',\n",
        "                            'score': doc.get('score', 0.0)\n",
        "                        } for doc in scoring_response_txt['source_documents']\n",
        "                    ], key=lambda x: x['title'] + \"#\" + x['url'])\n",
        "                    \n",
        "                    scoring_response_txt['source_documents_references'] = sorted(\n",
        "                        [url for i, url in enumerate(urls) \n",
        "                         if not (url['title'] == '' and url['url'] == '') \n",
        "                         and (i == 0 or url['title'] != urls[i-1]['title'] or url['url'] != urls[i-1]['url'])],\n",
        "                        key=lambda x: x['score'], \n",
        "                        reverse=True\n",
        "                    )\n",
        "                except Exception:\n",
        "                    scoring_response_txt['source_documents_references'] = []\n",
        "            \n",
        "                # Log response if enabled\n",
        "                if logging and logger:\n",
        "                    try:\n",
        "                        status, log_id = logger.create_log_record(question, scoring_response_txt, times, fields_dict)\n",
        "                        scoring_response_txt['log_status'] = status\n",
        "                        scoring_response_txt['log_id'] = log_id\n",
        "                        # Update with expert recommendations if applicable\n",
        "                        if config.parameters.get('expert_profiles_index'):\n",
        "                            fields_dict['log_id'] = log_id\n",
        "                            expert_response = logger.get_top_experts(fields_dict)\n",
        "                            scoring_response_txt['expert_response']=expert_response\n",
        "                            # Assuming get_top_experts updates the log or returns relevant data\n",
        "                    except Exception as e:\n",
        "                        scoring_response_txt['log_status'] = f\"Logging error: {str(e)}\"\n",
        "            \n",
        "\n",
        "                elapsed_times = {\n",
        "                    step: f\"{curr - times[i - 1][1]:.3f}s\"\n",
        "                    for i, (step, curr) in enumerate(times[1:], start=1)\n",
        "                }\n",
        "                \n",
        "\n",
        "                print(\"Elapsed times (seconds):\", elapsed_times)\n",
        "    \n",
        "                \n",
        "                \n",
        "                response_body = {\n",
        "                    \"query\": question,\n",
        "                    \"path\": context.get_path_suffix(),\n",
        "                    \"result\": scoring_response_txt\n",
        "                }\n",
        "                \n",
        "                return {\"body\": response_body}\n",
        "                \n",
        "            except Exception as e:\n",
        "                return {\n",
        "                    \"body\": {\"path_suffix\": context.get_path_suffix(),\n",
        "                        \"error\": f\"Error processing request: {str(e)}\"\n",
        "                    }\n",
        "                }\n",
        "\n",
        "    def generate_stream(context: Any) -> Dict:\n",
        "        \"\"\"\n",
        "        Process input context to generate a response\n",
        "        \n",
        "        Args:\n",
        "            context: Object providing get_token and get_json methods\n",
        "        \n",
        "        Returns:\n",
        "            Dict: Response with query and result\n",
        "        \"\"\"\n",
        "\n",
        "        # Instantiate Config and get API client\n",
        "        streaming = True\n",
        "        api_client = config.get_client()\n",
        "        # Set token on API client\n",
        "        api_client.set_token(context.get_token())\n",
        "        \n",
        "        # Process payload\n",
        "        payload = context.get_json()\n",
        "               \n",
        "        scoring_response_txt = {}\n",
        "        times = [('start', time.perf_counter())]\n",
        "        try:\n",
        "            question = payload.get(\"question\")\n",
        "            \n",
        "            query_filter = payload.get(\"query_filter\")\n",
        "            log_id = payload.get(\"log_id\")\n",
        "\n",
        "            \n",
        "        except:\n",
        "            return {\"body\":{\"status\":\"Invalid payload format\"}}\n",
        "            \n",
        "\n",
        "\n",
        "        try:\n",
        "            inputs = {\"query\": question}\n",
        "            if query_filter:\n",
        "                print(query_filter)\n",
        "                inputs[\"filter\"] = query_filter\n",
        "    \n",
        "            llm_response = rag_pipeline.call_runnable_map(inputs, streaming)\n",
        "            times.append(('llm_chain', time.perf_counter()))\n",
        "            print(\"Streaming response will generate below:\")\n",
        "        except Exception as e:\n",
        "            return  {\"body\":{\"status\":f\"LLM Chain error: {str(e)}\"}}\n",
        "\n",
        "        response_chunk=[]\n",
        "        for chunk in llm_response:\n",
        "            response_chunk.append(chunk)\n",
        "            yield chunk\n",
        "\n",
        "        scoring_response_txt[\"response\"]=\"\".join(response_chunk)\n",
        "\n",
        "        if log_id:\n",
        "            print(\"\\nCreating a log record\")\n",
        "            status, log_id = logger.create_log_record(question, scoring_response_txt, times, {}, log_id)\n",
        "            log_dict ={\"log_id\":log_id}\n",
        "            print(\"log record created:\",log_dict)\n",
        "\n",
        "    return generate, generate_stream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd2ad7bc-1a04-4820-aca3-4e0316e9a52e"
      },
      "source": [
        "#### Test above AI Service Locally\n",
        "\n",
        "The `RuntimeContext` class is a lightweight context manager used to encapsulate request metadata and authentication details required for invoking the qna_with_rag_ai_service function. It stores the API client (which provides a token), the request payload (json), HTTP method, and an optional path suffix. Utility methods such as `get_json()`, `get_token()`, `get_method()`, and `get_path_suffix()` provide convenient access to these internal attributes. This setup is useful for simulating and testing the service locally before deployment, ensuring a structured interface for downstream functions to retrieve necessary request context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6b4a810-ec5a-403a-b9d2-8838fbc8a2b7"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import Any, Dict\n",
        "class RuntimeContext:\n",
        "    def __init__(self, api_client: client, json: dict | None = None, method: str = 'GET', path_suffix: str = ''):\n",
        "        self.api_client = api_client\n",
        "        self.request_payload_json = json\n",
        "        self._method = method\n",
        "        self._path_suffix = path_suffix\n",
        "    def get_json(self) -> dict[str, Any] | None:\n",
        "        return self.request_payload_json\n",
        "\n",
        "    def get_token(self)-> str:\n",
        "        return self.api_client.token \n",
        "\n",
        "    def get_method(self) -> str:\n",
        "        return self._method\n",
        "\n",
        "    def get_path_suffix(self) -> str:\n",
        "        return self._path_suffix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dd79d1b-9948-4262-8af7-67cd3351e39e"
      },
      "outputs": [],
      "source": [
        "context = RuntimeContext(api_client=client, json={}, method=\"\", path_suffix=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31b7a7dc-083d-424e-8d3e-28845a513321"
      },
      "source": [
        "#### Test QnA response\n",
        "The code cell below sets up a test to verify the default question-answering behavior of the qna_with_rag_ai_service function. It initializes the RuntimeContext with the HTTP method \"POST\" and sets a sample question to simulate a standard query. The format of the request payload is:\n",
        "\n",
        "```\n",
        "{\"question\":  \"how to run a project in watsonx.ai\"}\n",
        "\n",
        "```\n",
        "\n",
        "Optionally, users can add a query filter to restrict retrieval to specific documents. Example with a filter:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"question\":  \"how to run a project in watsonx.ai\",\n",
        "  \"query_filter\": {\n",
        "    \"metadata.source\": \"watson-docs/wsj/getting-started/projects.html\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "**Note**: Using incorrect  filters may result in no answer being returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "386e2fe5-fe9e-4e18-820b-8d3369138670"
      },
      "outputs": [],
      "source": [
        "context._method = \"POST\"\n",
        "context._path_suffix = \"\"\n",
        "question = \"how to perform decision optimization?\"\n",
        "context.request_payload_json = {\"question\": question}\n",
        "test_ai_service = qna_with_rag_ai_service(context=context)\n",
        "resp = test_ai_service[0](context)\n",
        "resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3f36bcb-6a81-4bc1-b256-e2363c763ef0"
      },
      "source": [
        "#### Test Streaming Response Locally\n",
        "\n",
        "\n",
        "This code cell tests the streaming response behavior of the `qna_with_rag_ai_service` function. A unique `log_id` is optionally generated using a SHA-256 hash of the current timestamp and included in the request payload for creating a log record with streaming response. The context is then passed to the streaming handler of the AI service, and the response is printed incrementally in real time.\n",
        "\n",
        "The format of the request payload is:\n",
        "\n",
        "```\n",
        "{\"question\": user_question, \"log_id\": optional_unique_id}\n",
        "```\n",
        "\n",
        "**Note:** Including `log_id` is optional.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e57a9cf4-b73c-4b22-b6ef-27ebb9ebbda8"
      },
      "outputs": [],
      "source": [
        "\n",
        "stream_log_id = hashlib.sha256(str(datetime.now().timestamp()).encode()).hexdigest()\n",
        "context.request_payload_json = {\"question\": question,\"log_id\":stream_log_id}\n",
        "response = test_ai_service[1](context)\n",
        "for chunk in response:\n",
        "    print(chunk, end=\"\", flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e99f6c24-898f-4058-89fb-b04969102e7d"
      },
      "source": [
        "#### Test Logging Feedback\n",
        "\n",
        "This code cell tests the feedback logging of the `qna_with_rag_ai_service` function. It extracts the `log_id` from a previous QnA response and sends a feedback payload using the `/log_feedback` endpoint. The request payload includes the `log_id`, a feedback `value` (e.g., `\"positive\"`), and an optional `comment`.\n",
        "\n",
        "The format of the feedback payload is:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"log_id\": \"<log_id_from_previous_response>\",\n",
        "  \"value\": \"positive\",\n",
        "  \"comment\": \"Nice log record!\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ef8a702-8ab0-42c3-ae4f-d7926dddf220"
      },
      "outputs": [],
      "source": [
        "\n",
        "log_id = resp['body']['result']['log_id']\n",
        "context._path_suffix = \"/log_feedback\"\n",
        "context.request_payload_json = {\"log_id\":log_id, \"value\":\"positive\", \"comment\":\"Nice log record!\"}\n",
        "\n",
        "test_ai_service_log = qna_with_rag_ai_service(context=context)\n",
        "test_ai_service_log[0](context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0160b58-3d8b-428a-bc48-30c70bd7163a"
      },
      "source": [
        "\n",
        "#### Test Top Experts Recommendation\n",
        "\n",
        "Below code cell tests the expert recommendation feature of the `qna_with_rag_ai_service` function via the `/recommended_experts` endpoint. It uses a previously generated `log_id` to fetch relevant expert recommendations based on the context of the original question.\n",
        "\n",
        "The request payload format is:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"log_id\": \"<log_id_from_previous_qna_response>\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fe17047-b918-458e-a122-35d7d90c73cc"
      },
      "outputs": [],
      "source": [
        "\n",
        "context._path_suffix = \"/recommended_experts\"\n",
        "context.request_payload_json = {\"log_id\":log_id}\n",
        "\n",
        "test_ai_service_log = qna_with_rag_ai_service(context=context)\n",
        "test_ai_service_log[0](context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e15fe828-9b82-4532-ac4c-1c18748ca706"
      },
      "source": [
        "#### Test Auto-Complete Suggestions\n",
        "\n",
        "This code cell tests the auto-complete feature of the `qna_with_rag_ai_service` function using the `/auto_complete` endpoint. It sends a partial question prefix along with a limit on the number of suggestions to retrieve.\n",
        "\n",
        "The request payload format is:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"_question_prefix\": \"how to deploy\",\n",
        "  \"limit\": 5\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e112690e-fb97-4b7d-889c-90144e1887c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "context._path_suffix = \"/auto_complete\"\n",
        "context.request_payload_json =  {\"_question_prefix\":\"how to perform\",\"limit\":5}\n",
        "test_ai_service_log = qna_with_rag_ai_service(context=context)\n",
        "test_ai_service_log[0](context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "483253e55428443e89fe3b06eff293a9"
      },
      "source": [
        "<a id=\"DeployScoringFunction\"></a>\n",
        "### Deployment of AI Service Function to Space\n",
        "\n",
        "The code first defines metadata for the `qna_with_rag_ai_service`, including its name, description, and software specification, and stores the function in the deployment space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e765430c-d84f-4bfb-9021-0725f31cf1d8"
      },
      "outputs": [],
      "source": [
        "meta_props = {\n",
        "    client.repository.AIServiceMetaNames.NAME: \"QnA with RAG AI service SDK with \"+connection_type,\n",
        "    client.repository.AIServiceMetaNames.DESCRIPTION: 'QnA with RAG using ' + connection_type,\n",
        "    client.repository.AIServiceMetaNames.SOFTWARE_SPEC_ID: sw_spec_id\n",
        "}\n",
        "stored_ai_service_details = client.repository.store_ai_service(qna_with_rag_ai_service, meta_props)\n",
        "ai_service_id = client.repository.get_ai_service_id(stored_ai_service_details)\n",
        "print(\"AI Service stored in the deployment space with id: \", ai_service_id) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88f9e327-02c1-4976-9d33-5c77f889d372"
      },
      "source": [
        "After storing the function in deployment space, the below code performs the following tasks:\n",
        "\n",
        "- It checks if the desired `deployment_serving_name` is available for use with the function.\n",
        "- If the serving name is available, a new deployment is created for the scoring function with the specified serving name and metadata (name, description, hardware spec).\n",
        "- If the serving name already exists, it retrieves the details of the existing deployment, extracts its ID, and updates the deployment's assets with the new scoring function.\n",
        "- The deployment ID is then retrieved after either creating or updating the deployment for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "966db7b7-db0d-48d9-8cfb-0a6abf0d4593"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    if(client.deployments.is_serving_name_available(parameters['deployment_serving_name'])):\n",
        "        print(f\"Serving name {parameters['deployment_serving_name']} available\")\n",
        "        \n",
        "        meta_props = {\n",
        "           client.deployments.ConfigurationMetaNames.NAME: \"rag_ai_service_with_\"+connection_type,\n",
        "           client.deployments.ConfigurationMetaNames.DESCRIPTION: \"QnA with RAG using\" + connection_type,\n",
        "           client.deployments.ConfigurationMetaNames.HARDWARE_SPEC: { 'name': 'S'},  \n",
        "           client.deployments.ConfigurationMetaNames.SERVING_NAME:  parameters['deployment_serving_name']\n",
        "        }\n",
        "\n",
        "        print(f\"Creating a new deployment with the serving name {parameters['deployment_serving_name']}\")\n",
        "        \n",
        "        watsonx_deployment_details = client.deployments.create(ai_service_id, meta_props=meta_props)\n",
        "        \n",
        "        watsonx_deployment_id = client.deployments.get_id(watsonx_deployment_details)\n",
        "\n",
        "    else:\n",
        "        print(f\"Serving name '{parameters['deployment_serving_name']}' already exists\")\n",
        "        existing_serving_name = client.deployments.get_details(serving_name=parameters['deployment_serving_name'])\n",
        "\n",
        "        if not existing_serving_name['resources']:\n",
        "            print(\"Serving name not accessible from the deployment space\")\n",
        "            raise RuntimeError(\"Error accessing the serving name from the deployment space. Please update the deployment serving name in parameter set\")\n",
        "        \n",
        "        print(\"Fetching the deployment details from the serving name..\")\n",
        "        \n",
        "        watsonx_deployment_id = existing_serving_name['resources'][0]['metadata']['id']\n",
        "        metadata = {client.deployments.ConfigurationMetaNames.ASSET: { \"id\": stored_ai_service_details['metadata']['id'] }}\n",
        "        print(\"Updating the assets of the deployment..\")\n",
        "\n",
        "        if client.deployments.get_details(watsonx_deployment_id)['entity']['status']['state'] == \"ready\":\n",
        "            watsonx_deployment_details = client.deployments.update(watsonx_deployment_id, changes=metadata)\n",
        "        \n",
        "\n",
        "        timeout = 900  \n",
        "        start_time = time.time()\n",
        "        \n",
        "        while True:\n",
        "            status = client.deployments.get_details(watsonx_deployment_id)['entity']['status']['state']\n",
        "            print(f\"Current status: {status}\")\n",
        "            \n",
        "            if status == 'ready':\n",
        "                print(\"The assets of the ai service deployment are successfully updated!\")\n",
        "                break\n",
        "            \n",
        "            elapsed_time = time.time() - start_time\n",
        "            if elapsed_time > timeout:\n",
        "                print(\"The update process timed out after waiting for 15 minutes.\")\n",
        "                break\n",
        "            print(\"Update in progress... Please wait.\")\n",
        "            time.sleep(20)  \n",
        "\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fed92610a4704ea28821165e5887d822"
      },
      "source": [
        "<a id=\"scoring\"></a>\n",
        "### Test the deployed AI Service\n",
        "\n",
        "Test the deployed deployed ai service function by passing in a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f98d09eee863470c8cef0391cc91886b"
      },
      "outputs": [],
      "source": [
        "deployments_results = client.deployments.run_ai_service(\n",
        "    watsonx_deployment_id, {\"question\": \"how to run a project in watsonx.ai\"}\n",
        ")\n",
        "deployments_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9718094-b1d7-41da-9160-f6e7365153d8"
      },
      "outputs": [],
      "source": [
        "deployments_results_stream = client.deployments.run_ai_service_stream(\n",
        "    watsonx_deployment_id, {\"question\": \"how to run decision optimization\"}\n",
        ")\n",
        "\n",
        "print(\"Streaming responses:\")\n",
        "for response in deployments_results_stream:\n",
        "    print(response.strip(), end=\" \", flush=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06d718d4-2bad-4808-aef3-e732e56b7fd4"
      },
      "source": [
        "<a id=\"expert-recommendation\"></a>\n",
        "### Test the expert recommendation function\n",
        "\n",
        "Test the deployed ai service function to recommend top experts for the question asked by passing the log_id. This cell will only work and give results if you have executed the **Ingest Expert Profile data to vector DB** notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d0e43ae-adc0-4f71-a7df-d57c2750efb7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "mltoken = context.get_token()\n",
        "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + context.get_token()}\n",
        "\n",
        "try:\n",
        "    log_id = deployments_results['result']['log_id']\n",
        "except NameError: \n",
        "    log_id = ''\n",
        "\n",
        "payload_scoring = {\"log_id\":log_id}\n",
        "\n",
        "\n",
        "deployment_url = client.deployments.get_details(watsonx_deployment_id)['entity']['status']['inference'][0]['url']\n",
        "path_suffix = \"/recommended_experts\"\n",
        "\n",
        "\n",
        "response_scoring = requests.post(deployment_url+path_suffix, json=payload_scoring, headers={'Authorization': 'Bearer ' + mltoken})\n",
        "\n",
        "print(\"Scoring response\")\n",
        "response_scoring.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c44baa8-c011-46ae-8807-091a4f5485e6"
      },
      "source": [
        "<a id=\"auto complete support\"></a>\n",
        "### Test Auto complete\n",
        "\n",
        "Test the deployed scoring function to auto-complete the given question prefix with existing user questions logged into log index. Update `<_add_question_prefix>` & limit values in the `values` parameter below to test. <br>\n",
        "**Note**: This functionality support requires the log index schema for the question field to have auto completion support in case of ES. \n",
        "Please update it with a new log index parameter. Existing/Old indexes will not directly support this functionality. Lastly, also ensure your log index has sufficient records to match the given question prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2929c644-19b3-4178-843b-3a12376797e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "payload_scoring = {\"_question_prefix\":\"how to deploy\",\"limit\":5}\n",
        "\n",
        "path_suffix = \"/auto_complete\"\n",
        "\n",
        "response_scoring = requests.post(deployment_url+path_suffix, json=payload_scoring, headers={'Authorization': 'Bearer ' + mltoken})\n",
        "\n",
        "print(\"Scoring response\")\n",
        "response_scoring.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b166ed1f-304d-489b-b252-56bd29d78748"
      },
      "source": [
        "\n",
        "<a id=\"feedback-logging\"></a>\n",
        "### Feedback Logging\n",
        "Updating feedback in log record. In case the answer returned by the LLM is relevant to your question provide the values below as postive along with a comment. Otherwise, provide the values below as negative. \\\n",
        "This feedback will be saved if an Elasticsearch index, Datastax or Milvus collection is provided for feedback logging in the parameter set. \n",
        "\n",
        "**Note**: In case of elasticsearch, you can view the indexed feedback logs via the elasticsearch **kibana** user interface under the **Index Management** section. \\\n",
        "Find the document for the log record that corresponds to the conversation above. It contains, the question and answers, the documents found and the user's feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c18f257-37e8-4338-b9b0-29ef3722b52f"
      },
      "outputs": [],
      "source": [
        "\n",
        "payload_scoring ={\"log_id\":log_id, \"value\":\"positive\", \"comment\":\"Nice log record!\"}\n",
        "\n",
        "path_suffix = \"/log_feedback\"\n",
        "\n",
        "response_scoring = requests.post(deployment_url+path_suffix, json=payload_scoring, headers={'Authorization': 'Bearer ' + mltoken})\n",
        "\n",
        "\n",
        "print(\"Scoring response\")\n",
        "response_scoring.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b16c5cfc-5490-4657-9f3b-a195f63b24bc"
      },
      "source": [
        "<a id=\"updateParameters\"></a>\n",
        "### Update parameter set in the project & deployment space\n",
        "\n",
        "Update the advanced parameter set in both project & space with the deployment id of the function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95ca5754-ae41-49b3-8da2-660ed263605c"
      },
      "outputs": [],
      "source": [
        "paramset_name = \"RAG_advanced_parameter_set\"\n",
        "parameter_to_be_updated = {\"name\":\"wml_rag_deployment_id\",\"value\":watsonx_deployment_id}\n",
        "rag_helper_functions.update_parameter_set(client,paramset_name,parameter_to_be_updated)\n",
        "\n",
        "client.set.default_project(project_id=project_id)\n",
        "if rag_helper_functions.update_parameter_set(client,paramset_name,parameter_to_be_updated) == True:\n",
        "    print(\"Parameter set in the project and deployment space has been updated with the deployment id of the function above.\")\n",
        "else:\n",
        "    print(\"Parameter set update failed.\")\n",
        "client.set.default_space(space_uid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80d93f7640ba47c88eed8a592f5bff59"
      },
      "source": [
        "**Sample Materials, provided under license.</a> <br>\n",
        "Licensed Materials - Property of IBM. <br>\n",
        "© Copyright IBM Corp. 2024, 2025. All Rights Reserved. <br>\n",
        "US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. <br>**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
