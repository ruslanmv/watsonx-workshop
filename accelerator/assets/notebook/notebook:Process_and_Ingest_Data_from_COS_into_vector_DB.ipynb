{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2c942354ffc4b7b97ba3c5039dcfce1"
      },
      "source": [
        "# Data Processing and Ingestion of data from Cloud Object Storage(COS) into Vector Databases\n",
        "\n",
        "### Supported Cloud Object Storage (COS) Providers\n",
        "* IBM Cloud Object Storage\n",
        "* Amazon S3\n",
        "\n",
        "### Supported Vector Databases\n",
        "* Elasticsearch\n",
        "* Milvus\n",
        "\n",
        "## Introduction\n",
        "\n",
        "<h4>Bulk Ingesting Data from AWS S3 to Elasticsearch Vector Database:</h4>\n",
        "\n",
        "To efficiently ingest data from AWS S3 into an Elasticsearch vector database, consider the following approaches:\n",
        "1. Using the Elastic S3 Connector:<br>\n",
        "   For large-scale data ingestion from AWS S3 to Elasticsearch, utilize the Elastic S3 Connector. This method offers efficient indexing by directly integrating with Amazon S3, minimizing the need for intermediary processing steps. Detailed setup instructions are available in the `Readme - Watsonx Discovery S3 Connector.pdf` document.<br>\n",
        "   **Note**: For substantial datasets, the Elastic S3 Connector is recommended to ensure optimal performance and efficiency during the indexing process.\n",
        "\n",
        "2. Using this Current Ingestion Notebook:<br>\n",
        "   Alternatively, you can utilize this current ingestion notebook, which provides step-by-step instructions to ingest documents from AWS S3 into Elasticsearch. This method offers flexibility and can be tailored to specific requirements.\n",
        "\n",
        "<h4>Bulk Ingestion of large datasets from Cloud Object Storage(COS) to Vector Databases:</h4>\n",
        "\n",
        "This notebook facilitates the bulk ingestion of data from Cloud Object Storage (IBM COS and AWS S3) into vector databases, specifically Elasticsearch and Milvus. The indexed documents are further used in the next notebook to create a Q&A AI Service function and deploy it on [watsonx.ai](https://www.ibm.com/products/watsonx-ai). \n",
        "\n",
        "The ingestion process uses vector embeddings to enhance data storage and retrieval within either Elasticsearch or Milvus vector database, ensuring both efficiency and effectiveness. \n",
        "\n",
        "The process encompasses the following steps:\n",
        "\n",
        "- *Establish COS Connection*:\n",
        "  * Connect to the COS bucket containing the `.tar` files.\n",
        "  * For `Milvus`: Also, connect to the COS bucket associated with Milvus vector database for storing intermediary prepared data files.\n",
        "- *Connect to Vector Database*: Establish a connection to the chosen vector database (Elasticsearch or Milvus).\n",
        "- *Extract and Process Data*: Extract `.pdf` documents from `.tar` files and convert them into a structured document format.\n",
        "- *Generate Document IDs and Chunk Data*: Assign unique IDs to documents and split them into manageable chunks for indexing.\n",
        "- *Generate Embeddings*:\n",
        "  * For `Milvus`: Create vector embeddings for each document chunk.\n",
        "- *Prepare Data for Insertion*:\n",
        "  * For `Milvus`: Prepare data files and store them in the associated COS bucket for bulk insertion into Milvus.\n",
        "- *Bulk Insert Data*:\n",
        "  * For `Elasticsearch`: Use async bulk indexing to index data into the Elasticsearch vector database.\n",
        "  * For `Milvus`: Utilize Milvus's bulk insert capabilities to index data efficiently.\n",
        "- *Configure Vector Index*:\n",
        "  * For `Elasticsearch`: Set up the vector index with appropriate index type (**sparse** or **dense**) based on the deployed embedding model.\n",
        "  * For `Milvus`: Set up the collection with appropriate embedding types (**dense** or **hybrid**) based on the search type.\n",
        "\n",
        "**Note**: It is recommended to run this notebook in a Python environment on Cloud Pak for Data software with a GPU-enabled or high vCPU and RAM hardware configuration, as generating embeddings may require significant memory. Please update the notebook runtime env `qna_bulk_template` accordingly.\n",
        "\n",
        "**Required Params**: \n",
        "- Make sure you have set `cos_data_input_connection_asset` , `cos_milvus_connection_asset` required `RAG_PARAMETER_SET` params with COS connection assets names\n",
        "- Please update `cos_data_path` param in `RAG_ADVANCED_PARAMETER_SET` to specify the relative data path to the directory containing input files in `cos_data_input_connection_asset` bucket.\n",
        "\n",
        "**Disclaimer**: \n",
        "\n",
        "This Notebook is recommendated to run via batch job if the input data size is large. Bulk ingestion may take several hours to complete depending on the input size. Running via this notebook for longer time may experience kernel disconnection related issues before completion. Please follow below steps how to run this as a job.\n",
        "\n",
        "- Check and Click on `Jobs` dropdown icon on this notebook which is next `Share` button on top right side.\n",
        "- Click on `Create a job` button \n",
        "- Under `Define details`, Enter `name` for job for this notebook and click `Next`\n",
        "- Under `Configure`, select `latest` version and `qna_bulk_template` environment and keep rest same and click `Next`\n",
        "- Under `Schedule`, you can select any checkbox either `Run after job creation` or `Run on a schedule` depending on your requirement. then Click `Next`\n",
        "- Under `Notify`, click `Next` \n",
        "- Under `Review and create` , click `Create` button. \n",
        "- Then you will find this notebook under `Jobs` sections to run.\n",
        "\n",
        "## Contents\n",
        "\n",
        "This notebook contains the following parts:\n",
        "- [Setup](#setup)\n",
        "- [Import Dependencies](#import)\n",
        "- [Process and Split Extracted Data](#split)\n",
        "- [Connect to Vector Database](#connect)\n",
        "- [Connect to COS Object Store](#connect_cos)\n",
        "- [Update parameter set in the project](#updateParameters)\n",
        "- [Extract Data from Input files](#input)\n",
        "- [Indexing Documents using Bulk Indexing](#bulk_insert_documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e09609f5baf4507a57aca85bd321ff9"
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "### Pre-Requisite Libraries and Dependencies\n",
        "Below cell downloads and installs specific mandatory libraries and dependencies required to run this notebook.\n",
        "\n",
        "**Note** : Some of the versions of the libraries may throw warnings after installation. These library versions are crucial for execution of the accelerator. Please ignore the warning/error and proceed with your execution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc36dbb7f1a446988816cff0db8aa113",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community | tail -n 1\n",
        "!pip install ibm_watsonx_ai==1.3.26| tail -n 1\n",
        "!pip install pypdf | tail -n 1\n",
        "!pip install -U pymilvus[bulk_writer]==2.5.11 | tail -n 1\n",
        "!pip install elasticsearch==8.18.1 | tail -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90d5355-66ce-44d7-b6b3-17e29d51c30e"
      },
      "source": [
        "Restart the kernel after performing the pip install if the below cell fails to import all the libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe434599-967f-4f87-98e0-00d84e50a966"
      },
      "outputs": [],
      "source": [
        "from ibm_watsonx_ai import Credentials\n",
        "from ibm_watsonx_ai.foundation_models import Embeddings\n",
        "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
        "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai import APIClient\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "import hashlib\n",
        "import multiprocessing\n",
        "import json\n",
        "import os\n",
        "import shutil \n",
        "import warnings\n",
        "import time\n",
        "import tempfile\n",
        "import boto3\n",
        "import tarfile\n",
        "import datetime\n",
        "import ibm_boto3\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "from ibm_botocore.client import Config, ClientError\n",
        "\n",
        "from elasticsearch import helpers, AsyncElasticsearch\n",
        "from pymilvus.bulk_writer import RemoteBulkWriter, BulkFileType\n",
        "from pymilvus import(IndexType,Status,connections,FieldSchema,DataType,Collection,CollectionSchema,utility,BulkInsertState,Function,FunctionType)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a95f6b5b-427e-4a1b-93c0-24b98e6f513d"
      },
      "outputs": [],
      "source": [
        "project_id=os.environ['PROJECT_ID']\n",
        "# Environment and host url\n",
        "hostname = os.environ['RUNTIME_ENV_APSX_URL']\n",
        "\n",
        "if hostname.endswith(\"cloud.ibm.com\") == True:\n",
        "    environment = \"cloud\"\n",
        "    project_id = os.environ['PROJECT_ID']\n",
        "    runtime_region = os.environ[\"RUNTIME_ENV_REGION\"] \n",
        "else:\n",
        "    environment = \"on-prem\"\n",
        "    from ibm_watson_studio_lib import access_project_or_space\n",
        "    wslib = access_project_or_space()   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8cc005-454e-4646-901c-4cb3825d9bc5"
      },
      "source": [
        "<a id=\"import\"></a>\n",
        "### Import Parameter Sets, Credentials and Helper functions script.\n",
        "\n",
        "Below cells imports parameter sets values, sets the watsonx.ai credentials and imports the helper functions script. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89320d03-8254-4195-b127-cd5503e636bd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    filename = 'rag_helper_functions.py'\n",
        "    wslib.download_file(filename)\n",
        "    import rag_helper_functions\n",
        "    print(\"rag_helper_functions imported from the project assets\")\n",
        "except NameError as e:\n",
        "    print(str(e))\n",
        "    print(\"If running watsonx.ai aaS on IBM Cloud, check that the first cell in the notebook contains a project token. If not, select the vertical ellipsis button from the notebook toolbar and `insert project token`. Also check that you have specified your ibm_api_key in the second code cell of the notebook\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7937a1e9-2cd6-4e8d-9260-633f8bed4cc3"
      },
      "outputs": [],
      "source": [
        "parameter_sets = [\"RAG_parameter_set\",\"RAG_advanced_parameter_set\"]\n",
        "\n",
        "parameters=rag_helper_functions.get_parameter_sets(wslib, parameter_sets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2cd9353-c2a8-454a-b6eb-687e18ed79d4"
      },
      "outputs": [],
      "source": [
        "ibm_api_key=parameters['watsonx_ai_api_key']\n",
        "if environment == \"cloud\":\n",
        "    WML_SERVICE_URL=f\"https://{runtime_region}.ml.cloud.ibm.com\" \n",
        "    wml_credentials = Credentials(api_key=parameters['watsonx_ai_api_key'], url=WML_SERVICE_URL)\n",
        "else:\n",
        "    token = os.environ['USER_ACCESS_TOKEN']\n",
        "    wml_credentials=Credentials(token=os.environ['USER_ACCESS_TOKEN'],url=hostname,instance_id='openshift')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62a9b987-bf46-4e64-9cca-d6cdb8b7fc7d"
      },
      "source": [
        "### Set Watsonx.ai client\n",
        "Below cell uses the watson machine learning credentials to create an API client to interact with the project and deployment space. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72cd218c865b4e70a6da8f0406ed4398"
      },
      "outputs": [],
      "source": [
        "client = APIClient(wml_credentials)\n",
        "client.set.default_project(project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0a3b078433244b3b41c0ba405a84043"
      },
      "source": [
        "<a id=\"split\"></a>\n",
        "### Load all the Documents \n",
        "\n",
        "In the following cell, documents are processed to be inserted into a vector database. This involves splitting the documents using langchain's `RecursiveCharacterTextSplitter` and incorporating both content and metadata into the documents. The term \"recursive\" suggests that this division process happens in multiple stages or levels, breaking down the text into increasingly smaller segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "597bc5e52d4c4a678fcd061681c0e3e1"
      },
      "outputs": [],
      "source": [
        "def get_split_documents(documents, doc_name):\n",
        "    content=[]\n",
        "    metadata = []\n",
        "    for doc in documents:\n",
        "        \n",
        "        document_url = \"\"\n",
        "        document_title = doc.metadata[\"title\"] if \"title\" in doc.metadata else doc.metadata['source'].split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        metadata.append({\n",
        "                \"title\":document_title ,\n",
        "                \"source\": doc.metadata['source'],\n",
        "                \"document_url\":document_url,\n",
        "                \"page_number\":str(doc.metadata['page']) if 'page' in doc.metadata else ''\n",
        "                \n",
        "            })\n",
        "        \n",
        "        content.append(doc.page_content)\n",
        "    \n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=parameters['ingestion_chunk_size'],\n",
        "        chunk_overlap=parameters['ingestion_chunk_overlap'],\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    \n",
        "    split_documents = text_splitter.create_documents(content, metadatas=metadata)\n",
        "    print(f\"In {doc_name}: {len(documents)} Documents are split into {len(split_documents)} documents with a chunk size {parameters['ingestion_chunk_size']}.\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    for chunk in split_documents:\n",
        "        chunk.metadata[\"title\"] = chunk.metadata.get(\"title\", \"Unknown Title\")\n",
        "        chunk.page_content = f\"Document Title: {chunk.metadata['title']}\\n Document Content: {chunk.page_content}\"\n",
        "    \n",
        "    split_docs = rag_helper_functions.remove_duplicate_records(split_documents)\n",
        "    print(f'In {doc_name}: After de-duplication, there are {len(split_docs)} documents present.')\n",
        "\n",
        "    return split_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c3a18cdf0c34d0999ee68bf6577bcd9"
      },
      "source": [
        "<a id=\"connect\"></a>\n",
        "### Connecting to a vector database\n",
        "\n",
        "#### Connecting using Project Connection Asset (default)\n",
        "\n",
        "The notebook, by default, will look for a connection asset in the project named `milvus_connect` or `elasticsearch_connect`.  You can set this up by following the instructions in the project readme. \n",
        "This code checks if a specified connection exists in the project. If found, it retrieves the connection details and identifies the connection type. Depending on the connection type, it establishes a connection to the appropriate database. If the connection is not found, it raises an error indicating the absence of the specified connection in the project.\n",
        "\n",
        "Additionally, If the connection type is **Elastic Search**, the below cell creates an AsyncElasticsearch client and gets the status of the current Elasticsearch model (e.g. ELSER 2 or E5 multilingual). For this ensure that you have the model downloaded and deployed on Elasticsearch. This can be done under **Machine Learning >> Trained Models** section on Elasticsearch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e14441e06d140ff87b3b687c80d606c"
      },
      "outputs": [],
      "source": [
        "connection_name = parameters[\"connection_asset\"]\n",
        "if(next((conn for conn in wslib.list_connections() if conn['name'] == connection_name), None)):\n",
        "    print(connection_name, \"Connection found in the project\")\n",
        "    db_connection = wslib.get_connection(connection_name)\n",
        "    \n",
        "    connection_datatypesource_id=client.connections.get_details(db_connection['.']['asset_id'])['entity']['datasource_type']\n",
        "    connection_type = client.connections.get_datasource_type_details_by_id(connection_datatypesource_id)['entity']['name']\n",
        "    \n",
        "    print(\"Successfully retrieved the connection details\")\n",
        "    print(\"Connection type is identified as:\",connection_type)\n",
        "\n",
        "    if connection_type==\"elasticsearch\":\n",
        "        es_client= await rag_helper_functions.create_and_check_async_elastic_client(db_connection, parameters['elastic_search_model_id'])\n",
        "    elif connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "        milvus_credentials = rag_helper_functions.connect_to_milvus_database(db_connection, parameters)\n",
        "else:\n",
        "    db_connection=\"\"\n",
        "    raise ValueError(f\"No connection named {connection_name} found in the project.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffff983b-d60f-400d-9287-c690667de55b"
      },
      "source": [
        "<a id=\"connect_cos\"></a>\n",
        "### Connecting to the Cloud Object Store\n",
        "\n",
        "This code checks if a specified Cloud Object Store (`Amazon S3` or `IBM COS`) connection exists in the project. If found, it retrieves the connection details and identifies the connection type. Depending on the connection type, it establishes a connection to the COS object store. If the connection is not found, it raises an error indicating the absence of the specified connection in the project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c49f501d-e404-42ad-bf65-2858068291bb",
        "msg_id": "04347a3b-90d0-484d-b781-7f4c1b78507e"
      },
      "outputs": [],
      "source": [
        "def create_and_get_cos_connection(connection_name):\n",
        "    if(next((conn for conn in wslib.list_connections() if conn['name'] == connection_name), None)):\n",
        "        print(connection_name, \"Connection found in the project\")\n",
        "        cos_connection = wslib.get_connection(connection_name)\n",
        "        \n",
        "        connection_datatypesource_id=client.connections.get_details(cos_connection['.']['asset_id'])['entity']['datasource_type']\n",
        "        connection_type = client.connections.get_datasource_type_details_by_id(connection_datatypesource_id)['entity']['name']\n",
        "        \n",
        "        print(\"Successfully retrieved the connection details\")\n",
        "        print(\"Connection type is identified as:\",connection_type)\n",
        "    \n",
        "        if connection_type==\"amazons3\": # For Amazon S3\n",
        "            conn = RemoteBulkWriter.ConnectParam(\n",
        "                endpoint=\"s3.amazonaws.com\",\n",
        "                access_key=cos_connection['access_key'],\n",
        "                secret_key=cos_connection['secret_key'],\n",
        "                bucket_name=cos_connection['bucket'],\n",
        "                region = cos_connection['region'],\n",
        "                secure=True\n",
        "            )\n",
        "            print(\"Successfully created connection to the AWS S3 instance\")\n",
        "            \n",
        "        elif connection_type==\"bluemixcloudobjectstorage\": # For IBM Cloud Object Storage\n",
        "            conn = RemoteBulkWriter.ConnectParam(\n",
        "                endpoint=cos_connection['url'],\n",
        "                access_key=cos_connection['access_key'],\n",
        "                secret_key=cos_connection['secret_key'],\n",
        "                bucket_name=cos_connection['bucket'],\n",
        "                secure=True\n",
        "            )\n",
        "            print(\"Successfully created connection to the IBM COS instance\")\n",
        "    else:\n",
        "        conn = \"\"\n",
        "        raise ValueError(f\"No connection named {connection_name} found in the project.\")\n",
        "\n",
        "    return cos_connection, connection_type, conn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06c4fe8-19c3-4a37-8342-8e7bd55a9096"
      },
      "source": [
        "<a id=\"input\"></a>\n",
        "### Input File for extracting, loading, processing and indexing into a vector database\n",
        "\n",
        "This section of the code is responsible for establishing a connection with the specified Cloud Object Storage (COS) and retrieving all `.tar` files located within the specified data path. It performs the following key operations:\n",
        "- Connects to the COS (either `Amazon S3` or `IBM COS`) client.\n",
        "- Accesses the specified COS bucket object.\n",
        "- Searches the defined cos_data_path within the bucket.\n",
        "- Filters and retrieves all files with the .tar extension in the specified cos_data_path for further processing.\n",
        "\n",
        "This ensures that only relevant tar files are fetched from COS, streamlining the data ingestion or processing workflow.\n",
        "\n",
        "The paramter `cos_data_path` can be updated in the **RAG advanced parameter set** as required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5848f156-6b93-462c-a021-574fa4899caa",
        "msg_id": "04c8d660-1c19-4b19-b835-1b96a7f7e211"
      },
      "outputs": [],
      "source": [
        "bucket_data_path = parameters['cos_data_path']\n",
        "\n",
        "cos_data_connection, cos_connection_type, _ = create_and_get_cos_connection(parameters['cos_data_input_connection_asset'])\n",
        "\n",
        "cos_client = None\n",
        "\n",
        "if cos_connection_type==\"amazons3\":\n",
        "    cos_client = boto3.client(\n",
        "        \"s3\",\n",
        "        aws_access_key_id=cos_data_connection[\"access_key\"],\n",
        "        aws_secret_access_key=cos_data_connection[\"secret_key\"]\n",
        "    )\n",
        "elif cos_connection_type==\"bluemixcloudobjectstorage\":\n",
        "    cos_endpoint_url = cos_data_connection[\"url\"] if cos_data_connection[\"url\"].startswith('https://') else 'https://'+cos_data_connection[\"url\"]\n",
        "    cos_client = ibm_boto3.client(\n",
        "        \"s3\",\n",
        "        ibm_api_key_id=cos_data_connection['api_key'],\n",
        "        ibm_service_instance_id=cos_data_connection['resource_instance_id'],\n",
        "        config=Config(signature_version=\"oauth\"),\n",
        "        endpoint_url=cos_endpoint_url\n",
        "    )\n",
        "\n",
        "def get_cos_objects(bucket_name, prefix):\n",
        "    objects = []\n",
        "    paginator = cos_client.get_paginator(\"list_objects_v2\")\n",
        "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
        "        if \"Contents\" in page:\n",
        "            for obj in page[\"Contents\"]:\n",
        "                objects.append(obj[\"Key\"])\n",
        "    return objects\n",
        "\n",
        "cos_objects = get_cos_objects(cos_data_connection[\"bucket\"], bucket_data_path)\n",
        "tar_files = [key for key in cos_objects if key.endswith(\".tar\")]\n",
        "print(f\"Number of tar files found: {len(tar_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b87285a-01f6-4515-ac83-19b1fd5dcb83"
      },
      "source": [
        "<a id=\"bulk_insert_documents\"></a>\n",
        "### Inserting Documents using Bulk Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24e87f69-8692-48dd-8574-be4067da5143"
      },
      "source": [
        "This section describes the process of ingesting large documents (terabytes or more) into a vector database from a Cloud Object Storage (COS) bucket for efficient and rapid indexing.\n",
        "\n",
        "It contains the following key parts:\n",
        "- Vector Index Creation:\n",
        "  * For `Elasticsearch`: It initializes a vector index with either **sparse** or **dense** embeddings, depending on the selected Elasticsearch embedding model.\n",
        "  * For `Milvus`:\n",
        "     - Initializes a vector index with either **dense embeddings** or **hybrid search** (dense + BM25 sparse embeddings), based on the `milvus_hybrid_search` parameter. If hybrid search is enabled, it creates a new collection with **IVF_FLAT for dense** and **BM25 for sparse** indexing. Otherwise, it only adds **dense embeddings**.\n",
        "     - Initializes the selected Milvus embedding model.\n",
        "     - Initializes a RemoteBulkWriter object linked to the Milvus-associated Cloud Object Store bucket for bulk indexing.\n",
        "- Document Processing:\n",
        "  * Reads the input `.tar` files from the input COS bucket for document ingestion.\n",
        "  * Extracts and processes all pdf contents from the tar archieves to create documents.\n",
        "- Indexing Process:\n",
        "  * For `Elasticsearch`: Bulk indexes the processed documents into the Elasticsearch vector index.\n",
        "  * For `Milvus`:\n",
        "    - Adds processed documents to the RemoteBulkWriter for storage in the Milvus-associated COS bucket.\n",
        "    - Bulk indexes the processed data files from the Milvus-associated COS bucket into the Milvus vector index. This can be tracked using the generated task_id.\n",
        "\n",
        "**Note**: It is recommended to run this section in a Python environment on Cloud Pak for Data software with a GPU-enabled or higher vCPU and RAM hardware configuration, as generating embeddings and processing such large datasets requires significant memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "249758e9-ade6-428c-bba0-79650a531d4c"
      },
      "outputs": [],
      "source": [
        "def create_collection(index_name):\n",
        "    # Initializes embedding model\n",
        "    if environment==\"cloud\":\n",
        "        credentials=Credentials(\n",
        "            api_key = parameters['watsonx_ai_api_key'],\n",
        "            url =WML_SERVICE_URL)\n",
        "        embedding = Embeddings(\n",
        "          model_id=parameters['embedding_model_id'],\n",
        "          credentials=credentials,\n",
        "          project_id=project_id,\n",
        "          verify=True\n",
        "        )\n",
        "    elif environment==\"on-prem\":\n",
        "        try:\n",
        "            if client.foundation_models.EmbeddingModels.__members__:\n",
        "                if client.foundation_models.EmbeddingModels(parameters[\"embedding_model_id\"]).name:\n",
        "                    embedding = Embeddings(\n",
        "                      model_id=parameters['embedding_model_id'],\n",
        "                      credentials=wml_credentials,\n",
        "                      project_id=project_id,\n",
        "                      verify=True\n",
        "                    )\n",
        "            else:\n",
        "                print(\"local on prem embeddng models are not found, using models from IBM Cloud API\")\n",
        "                credentials=Credentials(\n",
        "                    api_key = parameters['watsonx_ai_api_key'],\n",
        "                    url =parameters['watsonx_ai_url'])\n",
        "                embedding = Embeddings(\n",
        "                  model_id=parameters['embedding_model_id'],\n",
        "                  credentials=credentials,\n",
        "                  space_id=parameters[\"wx_ai_inference_space_id\"],\n",
        "                  verify=True\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(\"Exception in loading Embedding Models:\" + str(e))\n",
        "\n",
        "    embedding_dim = embedding.embed_documents(['a'])[0]\n",
        "\n",
        "    # Creates/ retrieves collection\n",
        "    \n",
        "    \n",
        "    if index_name not in utility.list_collections():\n",
        "        dense_index_params = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\",\"params\": {\"nlist\": 1024},}\n",
        "        sparse_index_params = {\"metric_type\": \"BM25\",\"index_type\": \"SPARSE_INVERTED_INDEX\", \"params\": {\"drop_ratio_build\": 0.2}}\n",
        "\n",
        "        hybrid_search = True if parameters['milvus_hybrid_search'].lower()==\"true\" else False\n",
        "        if hybrid_search:\n",
        "            fields = [\n",
        "                FieldSchema(\"id\", DataType.VARCHAR, is_primary=True, max_length=65535, auto_id=False),\n",
        "                FieldSchema(\"dense\", DataType.FLOAT_VECTOR, dim=len(embedding_dim)),\n",
        "                FieldSchema(\"sparse\", DataType.SPARSE_FLOAT_VECTOR),\n",
        "                FieldSchema(\"title\", DataType.VARCHAR, max_length=65535),\n",
        "                FieldSchema(\"source\", DataType.VARCHAR, max_length=65535),\n",
        "                FieldSchema(\"document_url\", DataType.VARCHAR, max_length=65535),\n",
        "                FieldSchema(\"page_number\", DataType.VARCHAR, max_length=65535),\n",
        "                FieldSchema(\"text\", DataType.VARCHAR, max_length=65535, enable_analyzer=True)\n",
        "            ]\n",
        "            bm25_func = Function(\n",
        "                name=f\"bm25_text\",\n",
        "                function_type=FunctionType.BM25,\n",
        "                input_field_names=['text'],\n",
        "                output_field_names=['sparse'],\n",
        "                )\n",
        "            coll_schema = CollectionSchema(fields)\n",
        "            coll_schema.add_function(bm25_func)\n",
        "            coll = Collection(name=index_name, schema=coll_schema)\n",
        "\n",
        "            coll.create_index(field_name=\"dense\", index_params=dense_index_params)\n",
        "            coll.create_index(field_name=\"sparse\", index_params=sparse_index_params)\n",
        "        else:\n",
        "            fields = [\n",
        "                FieldSchema(\"id\", DataType.VARCHAR, is_primary=True, max_length=65535, auto_id=False),\n",
        "                FieldSchema(\"vector\", DataType.FLOAT_VECTOR, dim=len(embedding_dim)),\n",
        "                FieldSchema(\"title\", DataType.VARCHAR, max_length=65535),\n",
        "                FieldSchema(\"source\", DataType.VARCHAR, max_length=65535),\n",
        "                FieldSchema(\"document_url\", DataType.VARCHAR, max_length=65535),\n",
        "                FieldSchema(\"page_number\", DataType.VARCHAR, max_length=65535),\n",
        "                FieldSchema(\"text\", DataType.VARCHAR, max_length=65535)\n",
        "            ]\n",
        "            coll_schema = CollectionSchema(fields)\n",
        "            coll = Collection(name=index_name, schema=coll_schema)\n",
        "        \n",
        "            coll.create_index(field_name=\"vector\", index_params=dense_index_params)\n",
        "            \n",
        "        print('Milvus collection is created!')\n",
        "    else:\n",
        "        coll = Collection(name=index_name)\n",
        "        print('Milvus collection is retrieved!')\n",
        "\n",
        "    return coll, embedding\n",
        "\n",
        "\n",
        "def create_remote_writer(collection_obj):\n",
        "    # Create connection object to the remote COS object store\n",
        "    _, _, cos_conn = create_and_get_cos_connection(parameters['cos_milvus_connection_asset'])\n",
        "    \n",
        "    # Create writer object of the Remote object store for storing data\n",
        "    bulk_writer_remote_data_path = parameters['bulk_writer_remote_data_path']\n",
        "    writer = RemoteBulkWriter(\n",
        "        schema=collection_obj.schema,\n",
        "        remote_path=bulk_writer_remote_data_path,\n",
        "        connect_param=cos_conn,\n",
        "        file_type=BulkFileType.NUMPY\n",
        "    )\n",
        "    print(\"Bulk writer object to the Cloud object store is created!\")\n",
        "    \n",
        "    return writer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c0a06d1-c8c9-408b-9745-71e8306f4bee"
      },
      "source": [
        "Below code initializes the Milvus collection and embedding model for indexing of documents. It also creates remote bulk writer object which will be used to store input data files for Milvus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9b42318-db17-41a1-a0dc-c2b326d8102a"
      },
      "outputs": [],
      "source": [
        "if connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "    coll, embedding = create_collection(parameters['vector_store_index_name'])\n",
        "    writer = create_remote_writer(coll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071b7c7d-4d79-417b-8685-c29d915f12a5"
      },
      "source": [
        "Below code initializes an Elasticsearch index and configures an ingest pipeline for document indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcc75475-a4b7-4491-bf9d-1931e3d2b8d9"
      },
      "outputs": [],
      "source": [
        "if connection_type==\"elasticsearch\":\n",
        "    try:\n",
        "        es_client.options(ignore_status=400).indices.create(\n",
        "                index=parameters['vector_store_index_name'],\n",
        "                mappings={\n",
        "                    'properties': {\n",
        "                        'vector.tokens': {\n",
        "                            'type': 'sparse_vector' if 'sparse' in parameters['elastic_search_vector_type'] else 'dense_vector',\n",
        "                        },\n",
        "                    }\n",
        "                },\n",
        "                settings={\n",
        "                    'index': {\n",
        "                        'default_pipeline': 'ingest-pipeline',\n",
        "                        \"mapping.total_fields.limit\": 1000000\n",
        "                    },\n",
        "                    \"number_of_shards\": parameters[\"es_number_of_shards\"],\n",
        "                }\n",
        "            )\n",
        "        \n",
        "        es_client.ingest.put_pipeline(\n",
        "                id='ingest-pipeline',\n",
        "                processors=[\n",
        "                    {\n",
        "                        'inference': {\n",
        "                            'model_id': parameters['elastic_search_model_id'],\n",
        "                            'input_output': [\n",
        "                                {\n",
        "                                    'input_field': 'text',\n",
        "                                    'output_field': 'vector.tokens',\n",
        "                                }\n",
        "                            ]\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "        print(f'Elastic search index created with {parameters[\"elastic_search_model_id\"]}!')\n",
        "    except Exception as e:\n",
        "        print(f'Error creating index: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Dataset Preparation and Processing</h4>\n",
        "\n",
        "This section handles the extraction and preparation of dataset contents for ingestion into vector database. The process includes the following steps:\n",
        "- Extracts all PDF files from the retrieved TAR archives.\n",
        "- Processes the content of each PDF to structure the data appropriately.\n",
        "- For `Elasticsearch`:\n",
        "    - Stores the processed data into a data structure for further processing.\n",
        "- For `Milvus`:\n",
        "    - Transforms the processed data into a format compatible with the Milvus bulk writer.\n",
        "    - Writes the prepared data to a Milvus-associated COS bucket.\n",
        "\n",
        "**Optional**: You can uncomment the pdf_count parameter to limit the number of PDF files processed per tar file during execution — useful for testing or resource management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1928487e-5fbe-4c9b-9749-aee7600b1420"
      },
      "outputs": [],
      "source": [
        "def generate_hash(content):\n",
        "    return hashlib.sha256(content.encode()).hexdigest()\n",
        "    \n",
        "def prepare_and_write_data(documents, hybrid_search, doc_list, pdf_name):\n",
        "    try:\n",
        "        global embedding\n",
        "        # Generating batch embedding for the document list\n",
        "        doc_embeddings = embedding.embed_documents([doc.page_content for doc in documents])\n",
        "        for index in range(len(documents)):\n",
        "            doc = documents[index]\n",
        "            doc._id=generate_hash(doc.page_content+'\\nTitle: '+doc.metadata['title']+'\\nUrl: '+doc.metadata['document_url']+'\\nPage: '+doc.metadata['page_number'])\n",
        "            doc.id=doc._id\n",
        "            doc = json.loads(doc.json())\n",
        "            doc['title']=doc['metadata']['title']\n",
        "            doc['document_url']=doc['metadata']['document_url']\n",
        "            doc['page_number']=doc['metadata']['page_number']\n",
        "            doc['source']=doc['metadata']['source']\n",
        "            doc['text'] = doc['page_content']\n",
        "            if hybrid_search:\n",
        "                doc['dense'] = doc_embeddings[index]\n",
        "            else:\n",
        "                doc['vector'] = doc_embeddings[index]\n",
        "            del doc['page_content']\n",
        "            del doc['type']\n",
        "            del doc['metadata']\n",
        "            doc_list.append(doc)\n",
        "    except Exception as e:\n",
        "        print(f'Error in preparing and writing data for PDF : {pdf_name}. Error : {e}.')\n",
        "        \n",
        "def process_pdf(pdf, doc_list, all_docs_split):\n",
        "    pdf_name, pdf_content = pdf\n",
        "    try:\n",
        "        print(f'Started Processing pdf content: {pdf_name}.')\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".pdf\", delete=True) as temp_pdf:\n",
        "            temp_pdf.write(pdf_content)\n",
        "            temp_pdf.flush()  # Ensure all data is written to the file\n",
        "            loader = PyPDFLoader(temp_pdf.name)\n",
        "            documents = loader.load()\n",
        "            split_docs = get_split_documents(documents, pdf_name)\n",
        "            \n",
        "            if connection_type==\"elasticsearch\":\n",
        "                all_docs_split.extend(split_docs)\n",
        "\n",
        "            elif connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "                hybrid_search = True if parameters['milvus_hybrid_search'].lower()==\"true\" else False\n",
        "                prepare_and_write_data(split_docs, hybrid_search, doc_list, pdf_name)\n",
        "                \n",
        "        \n",
        "        print(f'Processing completed for PDF: {pdf_name}.')\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f'Error processing PDF: {pdf_name}. Error : {e}')\n",
        "        global processing_error\n",
        "        processing_error.append(f'Error processing PDF: {pdf_name}. Error : {e}.')\n",
        "            \n",
        "def process_tar_file(bucket_name, tar_key,):\n",
        "    map_pdf_content = {}\n",
        "    try:\n",
        "        response = cos_client.get_object(Bucket=bucket_name, Key=tar_key)\n",
        "        tar_bytes = BytesIO(response[\"Body\"].read())\n",
        "        if tar_bytes.getbuffer().nbytes > 1:\n",
        "            print('--------Reading tar file---------')\n",
        "            with tarfile.open(fileobj=tar_bytes, mode=\"r:\") as tar:\n",
        "                global pdf_count\n",
        "                for member in tar.getmembers():\n",
        "                    if member.isfile() and member.name.endswith(\".pdf\"):\n",
        "                        print(f'Reading PDF content: {member.name}')\n",
        "                        pdf_count += 1\n",
        "\n",
        "                        # limit pdf count to be processed\n",
        "                        # if pdf_count > 5: \n",
        "                        #     break\n",
        "\n",
        "                        pdf_file = tar.extractfile(member)\n",
        "                        if pdf_file:\n",
        "                            pdf_content = pdf_file.read()\n",
        "                            map_pdf_content.update({member.name: pdf_content})\n",
        "    except Exception as e:\n",
        "        print(f'Error processing tar files. Error : {e}.')\n",
        "    return map_pdf_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebce008-ae19-4f86-8660-f186da4838cc"
      },
      "source": [
        "<h4>PDF Document Chunking and Writing to Compatible Format</h4>\n",
        "\n",
        "This section of the code iterates through all PDF documents extracted from the list of TAR files. It utilizes multiprocessing and spawn processes based on the `num_workers` parameter. A progress bar is employed to track the number of TAR files and PDFs processed.\n",
        "\n",
        "It performs the following key operations:<br>\n",
        "- Each PDF document is processed individually.\n",
        "- The content of each PDF is divided into smaller, manageable document chunks optimized for vector indexing.\n",
        "- For `Elasticsearch`:\n",
        "  * The processed data is organized into a data structure, preparing them for subsequent processing and ingestion into the vector database.\n",
        "- For `Milvus`:\n",
        "  * The processed data is transformed into a format compatible with the Milvus bulk writer.\n",
        "  * Employs the RemoteBulkWriter to write the processed data to a Milvus-associated Cloud Object Store (COS) bucket.\n",
        "\n",
        "This approach ensures efficient handling and indexing of large PDF datasets.\n",
        "\n",
        "**Optional**: You can uncomment the tar_count parameter to limit the number of TAR files processed during execution — useful for testing or resource management.\n",
        "\n",
        "**Note**: The execution time of this operation can vary significantly and may take several hours, depending on the size and number of input documents. <br>\n",
        " To optimize the performance, adjust the `num_workers` parameter based on your system's CPU core count. Be mindful of the increased CPU memory consumption and ensure your system can handle the load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6330e517-e762-40bd-8b87-99672093e2b4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "tar_count = 0\n",
        "pdf_count = 0\n",
        "processing_error = []\n",
        "data_batch_files = []\n",
        "\n",
        "if connection_type==\"elasticsearch\":\n",
        "    writer=None\n",
        "    embedding=None\n",
        "\n",
        "# Creating shared list to be shared across processes\n",
        "m = multiprocessing.Manager()\n",
        "shared_data_list = m.list()\n",
        "shared_all_split_docs = m.list()\n",
        "\n",
        "num_workers = parameters['num_workers']\n",
        "\n",
        "def update_progress(result):\n",
        "    pbar.update(1)\n",
        "\n",
        "# Loop through and process each tar file\n",
        "print('Execution Started!')\n",
        "start_time = datetime.datetime.now()\n",
        "try:\n",
        "    with tqdm(total=len(tar_files), desc=\"Processing Tar Files\", unit=\"tars\") as pbar_tar:\n",
        "        for tar_file in tar_files:\n",
        "            tar_count += 1\n",
        "            \n",
        "            # limit tar count to be processed\n",
        "            # if tar_count > 5: \n",
        "            #     break\n",
        "            \n",
        "            print(f\"Tar file: {tar_count}\")    \n",
        "            print(f\"Processing tar file: {tar_file}\\n------------------\\n\")\n",
        "            \n",
        "            start_tar_execution = datetime.datetime.now()\n",
        "            map_pdf_content = process_tar_file(cos_data_connection[\"bucket\"], tar_file)\n",
        "            print(f\"\\n----------\\nTotal PDF count: {len(map_pdf_content)}\\n\")\n",
        "    \n",
        "            with multiprocessing.Pool(processes=num_workers) as pool:\n",
        "                with tqdm(total=len(shared_data_list), desc=f\"Tar: {tar_file}, Total PDFs: {len(map_pdf_content)}, Processing PDF\", unit=\" \") as pbar:\n",
        "                    results = [pool.apply_async(process_pdf, (p ,shared_data_list, shared_all_split_docs), callback=update_progress) for p in map_pdf_content.items()]\n",
        "    \n",
        "                    for r in results:\n",
        "                        r.get()\n",
        "                        # print('\\t', r.get())\n",
        "            \n",
        "            end_tar_execution = datetime.datetime.now()\n",
        "            \n",
        "            if connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "                for doc in shared_data_list:\n",
        "                    writer.append_row(doc)\n",
        "                writer.commit()\n",
        "                print('Successfully written documents to the COS Milvus object store!')\n",
        "            \n",
        "                # Data files path\n",
        "                data_batch_files = writer.batch_files\n",
        "                print(f'Data batch files created in COS: {data_batch_files}')\n",
        "    \n",
        "                shared_data_list[:] = []\n",
        "        \n",
        "            print(f\"\\n----------\\nProcessing of tar {tar_file} completed. Total PDF count: {pdf_count}\\n Time elapsed: {end_tar_execution-start_tar_execution}\\n-----------\\n\")\n",
        "            pbar_tar.update(1)\n",
        "            pdf_count = 0\n",
        "except Exception as e:\n",
        "    print(f'Failed to process data files from cos bucket. Error: {e}.')\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print('Execution Complete! \\nTotal Execution complete: ',(end_time-start_time))\n",
        "print('start time: ', start_time)\n",
        "print('end time: ', end_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bd76ab3-2970-4bea-a4cc-b4f1b7f9d77d"
      },
      "source": [
        "<h4>Bulk ingestion of generated data files to Milvus vector store. </h4>\n",
        "\n",
        "Batch files containing set of documents can be directly ingested into Milvus vector store.<br>\n",
        "This `asynchronous process` generates a unique task_id for each batch file, allowing for progress tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a003abf6-17df-40d8-91a7-7fa3ec6d323a"
      },
      "outputs": [],
      "source": [
        "if connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "    task_id_list = []\n",
        "    try:\n",
        "        for batch in data_batch_files:\n",
        "            batch_data_path = '/'.join(batch[0].split('/')[:-1])\n",
        "            print('Batch:',batch_data_path)\n",
        "            task_id = utility.do_bulk_insert(\n",
        "                collection_name=parameters['vector_store_index_name'],\n",
        "                files=batch)\n",
        "            task_id_list.append(task_id)\n",
        "            print(f'task_id: {task_id}')\n",
        "        print('Bulk indexing of the documents has started')\n",
        "    except Exception as e:\n",
        "        print('Bulk Insert failed. Error: ', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53746a2f-5cf2-47af-aad2-b01d30a94b97"
      },
      "source": [
        "The document ingestion progress can be tracked using the task IDs generated for each batch file in the previous step.\n",
        "Each `task_id` represents an asynchronous bulk-insert operation, and its state can be checked to determine the current status of the ingestion process. Different task states are as follows:\n",
        "* Pending\n",
        "* Started / In Progress\n",
        "* Persisted\n",
        "* Completed\n",
        "* Failed\n",
        "  \n",
        "**Note** The bulk ingestion is an intensive operation and may take some time to complete. It's advisable to monitor the progress periodically to ensure successful completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab78e8cb-c10c-41d3-91bd-a63b1e2b9822"
      },
      "outputs": [],
      "source": [
        "if connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "    print('Bulk indexing Status:')\n",
        "    for task_id in task_id_list:\n",
        "        print(f'\\n----------\\ntask_id: {task_id}')\n",
        "        task = utility.get_bulk_insert_state(task_id=task_id)\n",
        "        state_2_name = task.state_2_name\n",
        "        state_2_name.update({2:'In Progress'})\n",
        "        \n",
        "        print(\"UTC Start time:\", task.create_time_str)\n",
        "        print(\"Ingestion Status:\", state_2_name[task.state])\n",
        "        print(\"Ingestion Progress: {}%\".format(task.progress))\n",
        "\n",
        "        if task.state == BulkInsertState.ImportCompleted:\n",
        "            print(\"Imported row count:\", task.row_count)\n",
        "            print('Document Ingestion Completed!')\n",
        "            \n",
        "        if task.state == BulkInsertState.ImportFailed:\n",
        "            print('Document Ingestion Failed!')\n",
        "            print('-------------------------')\n",
        "            print(\"Failed reason:\", task.failed_reason)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After completing the bulk ingestion process, execute the following cell to load all data into the Milvus collection. This step is essential for making the data available for querying.\n",
        "\n",
        "Please note that executing this cell will fail if any of the previous bulk insertion steps encountered errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8a0a61f-dce9-4a9a-b2aa-cd45d0ae226e"
      },
      "outputs": [],
      "source": [
        "if connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "    try:\n",
        "        coll.load()\n",
        "        print('Milvus collection is loaded with documents and ready for querying!')\n",
        "    except Exception as e:\n",
        "        print('Failed loading Milvus collection. Error in bulk ingestion : ', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d0450cc-bb2a-4662-a327-bf2faaf8d89d"
      },
      "source": [
        "<h4>Bulk ingestion of documents to Elasticsearch vector store. </h4>\n",
        "\n",
        "- Transforms the processed data into a format compatible with the Elasticsearch bulk writer.\n",
        "- Performs asynchronous bulk insertion into the Elasticsearch index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "450ab573-0825-4689-9206-4c765670b94f"
      },
      "outputs": [],
      "source": [
        "if connection_type==\"elasticsearch\":\n",
        "    all_docs_split_upd=[]\n",
        "    try:\n",
        "        for doc in shared_all_split_docs:\n",
        "            doc = json.loads(doc.json())\n",
        "            doc['text'] = doc['page_content']\n",
        "            del doc['page_content']\n",
        "            all_docs_split_upd.append(doc)\n",
        "        print('Documents are ready for ingestion in Elasticsearch!')\n",
        "    except Exception as e:\n",
        "        print(f'Error: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cell asynchronously indexes data into an Elasticsearch index by processing documents in chunks and streaming the progress of their insertion.\n",
        "\n",
        "**Note**: Executing this operation may take some time if the dataset contains a large number of documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d461ddb-9f56-47b8-85d5-66e8420cf1c1"
      },
      "outputs": [],
      "source": [
        "if connection_type==\"elasticsearch\":\n",
        "    start_time = datetime.datetime.now()\n",
        "    try:\n",
        "        total_docs = 0\n",
        "        successful_docs = 0\n",
        "        print('Starting Bulk indexing of the documents.')\n",
        "        async for success, info in helpers.async_streaming_bulk(es_client, all_docs_split_upd, index = parameters['vector_store_index_name'], chunk_size=500, raise_on_error=True):\n",
        "            total_docs += 1\n",
        "            if success:\n",
        "                successful_docs += 1\n",
        "            print(f\"Indexed {total_docs} documents: {successful_docs} successful\")\n",
        "    except helpers.BulkIndexError as e:\n",
        "        print('Bulk Insert failed. Errors: ')\n",
        "        for error in e.errors:\n",
        "            print(error)\n",
        "    end_time = datetime.datetime.now()\n",
        "    print(f\"Indexing of document completed. \\n Time elapsed: {end_time-start_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30e726fb-61a4-4af9-abb5-3091ec0229c9"
      },
      "source": [
        "Above cell may take significant amount of time to complete based on the size of the documents. \n",
        "\n",
        "Optionally you can also proceed to **`Create and Deploy QnA AI Service`** notebook to create and deploy the RAG AI service.<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "336cc73aa6c3495a85dc167312b99079"
      },
      "source": [
        "\n",
        "\n",
        "**Sample Materials, provided under license.</a> <br>\n",
        "Licensed Materials - Property of IBM. <br>\n",
        "© Copyright IBM Corp. 2025. All Rights Reserved. <br>\n",
        "US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. <br>**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
