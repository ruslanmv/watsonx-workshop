{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"watsonx Workshop Series","text":""},{"location":"#welcome","title":"Welcome","text":"<p>The watsonx Workshop Series is a hands-on set of tracks that show how to build with IBM watsonx:</p> <ul> <li>Day 0 \u2013 Environment \u2014 local &amp; cloud setup for the whole week  </li> <li>Day 1 \u2013 LLMs &amp; Prompting \u2014 Granite concepts, patterns, and safe prompting  </li> <li>Day 2 \u2013 RAG \u2014 retrieval-augmented generation from zero to API &amp; UI  </li> <li>Day 3 \u2013 Orchestrate &amp; Agents \u2014 tool-using agents and governance, end-to-end</li> </ul> <p>Markdown-First Approach</p> <p>Everything is Markdown-first for clean builds. Day-specific notebooks live under:</p> <ul> <li><code>labs-src/</code> (governance &amp; RAG examples)</li> <li><code>docs/assets/notebooks/day3/</code> (agent notebooks reference)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"Browse the DocsRun the Accelerator (RAG) Locally <ol> <li>Use the left sidebar to pick a Day.</li> <li>Follow the pages top-to-bottom; theory first, then labs with solutions.</li> <li>Start at Day 0 to prepare both environments.</li> </ol> <pre><code># Minimal end-to-end run\ncd accelerator\npython -m venv .venv\nsource .venv/bin/activate         # Windows: .venv\\Scripts\\activate\npip install -U pip &amp;&amp; pip install -e .\ncp .env.sample .env               # fill watsonx + vector backend settings\nmake all                          # extract \u2192 chunk \u2192 index\nmake api                          # FastAPI at http://localhost:8001/health\n# in a second terminal:\nmake ui                           # Streamlit at http://localhost:8501\n</code></pre>"},{"location":"#choose-a-track","title":"Choose a Track","text":"<ul> <li> <p> Day 0 \u2013 Environment Setup</p> <p>Prepare simple-ollama and simple-watsonx envs; verify both.</p> <p> Open</p> </li> <li> <p> Day 1 \u2013 LLMs &amp; Prompting</p> <p>LLM concepts, prompt patterns &amp; templates, lightweight eval &amp; safety.</p> <p> Open</p> </li> <li> <p> Day 2 \u2013 RAG</p> <p>Build a grounded Q&amp;A app (Elasticsearch/Chroma + watsonx.ai), package and evaluate.</p> <p> START HERE</p> </li> <li> <p> Day 3 \u2013 Orchestrate &amp; Agents</p> <p>Agentic AI: CrewAI/LangGraph patterns \u2192 watsonx Orchestrate with governance.</p> <p> Open</p> </li> <li> <p> Capstone (Optional)</p> <p>Team up to ship a mini project using the accelerator &amp; governance workflow.</p> <p> Open</p> </li> </ul>"},{"location":"#presentation-slides","title":"Presentation Slides","text":"<p>Use the docs for step-by-step labs, and the slides for live delivery or sharing with participants.</p>"},{"location":"#html-decks-interactive","title":"HTML Decks (Interactive)","text":"<ul> <li> <p> Day 0 \u2013 Environment Setup</p> <p>Environment overview, accounts, and validation of both stacks.</p> <p> View Slides</p> </li> <li> <p> Day 1 \u2013 LLMs &amp; Prompting</p> <p>LLM concepts, prompt patterns &amp; templates, eval &amp; safety.</p> <p> View Slides</p> </li> <li> <p> Day 2 \u2013 RAG</p> <p>RAG architecture, retrieval patterns, and accelerator alignment.</p> <p> View Slides</p> </li> <li> <p> Day 3 \u2013 Orchestrate &amp; Agents</p> <p>Agentic AI, orchestration, and governance recap.</p> <p> View Slides</p> </li> <li> <p> Capstone</p> <p>Project overview, ideas, and framing for the final exercise.</p> <p> View Slides</p> </li> </ul> <p>PDF Export</p> <p>To export slides as PDF, open any HTML deck and use your browser's print function (Ctrl+P / Cmd+P) with \"Save as PDF\" option.</p>"},{"location":"#rag-track-at-a-glance","title":"RAG Track at a Glance","text":"<ul> <li>Pre-work: environment, credentials, and sample data  </li> <li>Lab 1: end-to-end accelerator (HTML/PDF \u2192 vectors \u2192 API + Streamlit)  </li> <li>Lab 2 (choose or compare):</li> <li>2A: Elasticsearch + LangChain</li> <li>2B: Elasticsearch Python SDK (no LangChain)</li> <li>2C: Chroma + LangChain (local/dev)</li> <li>Lab 3: Packaging &amp; Evaluation with watsonx.governance</li> </ul> <pre><code>flowchart TD\n  subgraph ING[Ingestion]\n    direction TB\n    A[Docs: HTML \u2022 PDF] --&gt; B[Extractor &amp; Cleaner]\n    B --&gt; C[Chunker]\n  end\n  subgraph RET[Retrieval]\n    direction TB\n    C --&gt; D[Embeddings]\n    D --&gt;|dense vectors| E[(Vector Store ES/Chroma)]\n    E --&gt;|top-k| F[Reranker optional]\n  end\n  subgraph GEN[Generation]\n    direction TB\n    F --&gt; G[Prompt Composer]\n    G --&gt; H[LLM on watsonx.ai]\n    H --&gt; I[Answer + Citations]\n  end\n  subgraph SRV[Serving]\n    direction TB\n    I --&gt; J[FastAPI /ask]\n    J --&gt; K[Streamlit Chat UI]\n  end\n  classDef source fill:#E3F2FD,stroke:#1E88E5,color:#0D47A1,stroke-width:1px;\n  classDef process fill:#F1F8E9,stroke:#7CB342,color:#2E7D32,stroke-width:1px;\n  classDef store fill:#FFF3E0,stroke:#FB8C00,color:#E65100,stroke-width:1px;\n  classDef model fill:#F3E5F5,stroke:#8E24AA,color:#4A148C,stroke-width:1px;\n  classDef output fill:#E0F7FA,stroke:#00ACC1,color:#006064,stroke-width:1px;\n  class A source; class B,C,D,F,G,J,K process; class E store; class H model; class I output;</code></pre>"},{"location":"#day-3-agent-notebooks-reference","title":"Day 3 Agent Notebooks (Reference)","text":"<p>Agent implementation examples for different frameworks:</p> <ul> <li><code>docs/assets/notebooks/day3/agent_crewai.ipynb</code> \u2014 Multi-agent collaboration</li> <li><code>docs/assets/notebooks/day3/agent_langgraph.ipynb</code> \u2014 Stateful workflows</li> <li><code>docs/assets/notebooks/day3/agent_watsonx.ipynb</code> \u2014 watsonx Orchestrate integration</li> </ul> <p>Lab Integration</p> <p>Use these alongside Lab 3.1: Agent + Accelerator API.</p>"},{"location":"#whats-included","title":"What's Included","text":"<ul> <li>Production-ready samples \u2014 FastAPI + Streamlit, CLI, Dockerfiles</li> <li>Reproducible configs \u2014 <code>.env.sample</code>, <code>requirements.txt</code> / <code>pyproject.toml</code></li> <li>Evaluation workflows \u2014 watsonx.governance for model comparison</li> </ul> <p>Everything is Markdown</p> <p>All labs are follow-along pages. Copy-paste commands and code blocks; notebooks are optional helpers.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Begin Day 0</p> <p>Set up and verify both environments</p> <p> Start</p> </li> <li> <p> Jump to RAG</p> <p>Start building immediately if ready</p> <p> Go</p> </li> <li> <p> Resources</p> <p>Additional learning materials</p> <p> Browse</p> </li> </ul> <p>Built with \u2764\ufe0f for the watsonx Community</p> <p> \u2b50 Star on GitHub \u2022     Report Issue \u2022     Discussions </p>"},{"location":"resources/","title":"docs/resources.md","text":""},{"location":"resources/#resources","title":"Resources","text":"<ul> <li>IBM Cloud: https://cloud.ibm.com</li> <li>watsonx.ai docs: https://www.ibm.com/products/watsonx-ai</li> <li>Watsonx.governance (Evaluation Studio): https://www.ibm.com/products/watsonx-governance</li> <li>Elasticsearch documentation: https://www.elastic.co/guide/index.html</li> <li>Chroma vector DB documentation: https://docs.trychroma.com</li> <li>LangChain docs: https://python.langchain.com</li> <li>Hugging Face Transformers: https://huggingface.co/docs/transformers/</li> <li>Ollama: https://ollama.com/</li> </ul>"},{"location":"blog/day3-agentic-ai-orchestrate/","title":"Day 3 \u2013 From RAG to Agents and Orchestrate","text":"<p>On Day 2 we made LLMs know things by adding Retrieval-Augmented Generation (RAG). On Day 3 we made those LLMs do things by wrapping them as agents and orchestration flows.</p> <p>This short recap is meant as an internal blog or summary you can share with your team.</p>"},{"location":"blog/day3-agentic-ai-orchestrate/#what-we-set-out-to-do","title":"What We Set Out to Do","text":"<p>By the end of Day 3, the goals were to:</p> <ul> <li>Put an agent on top of the RAG accelerator service.</li> <li>Explore common agent frameworks (CrewAI, Langflow, LangGraph).</li> <li>Understand how to move from notebooks to watsonx Orchestrate for production.</li> <li>Connect the dots with governance and evaluation.</li> </ul> <p>In other words:</p> <p>Day 1: prompts \u2192 Day 2: RAG \u2192 Day 3: agents on top of RAG \u2192 Orchestrate &amp; governance.</p>"},{"location":"blog/day3-agentic-ai-orchestrate/#morning-agentic-ai-frameworks","title":"Morning \u2013 Agentic AI &amp; Frameworks","text":"<p>We started with an overview of Agentic AI:</p> <ul> <li>Agents plan, choose tools, and act, instead of just answering once.</li> <li>Tools can be anything:</li> <li>RAG APIs (our accelerator <code>/ask</code> endpoint).</li> <li>Calculators and search APIs.</li> <li>Internal systems (tickets, HR, CRM\u2026).</li> </ul> <p>We looked at four complementary frameworks:</p> <ol> <li>CrewAI \u2013 multi-agent \u201ccrews\u201d in Python.  </li> <li>Langflow \u2013 drag-and-drop builder for LangChain flows.  </li> <li>LangGraph \u2013 state-machine style orchestration with nodes and edges.  </li> <li>watsonx Orchestrate \u2013 IBM platform for building, governing, and deploying agents.</li> </ol> <p>Key takeaway: the patterns (tool-calling, ReAct, plan\u2013act, multi-agent) are more important than the specific library. Once you understand the patterns, you can switch frameworks.</p>"},{"location":"blog/day3-agentic-ai-orchestrate/#afternoon-labs","title":"Afternoon \u2013 Labs","text":""},{"location":"blog/day3-agentic-ai-orchestrate/#lab-31-local-agent-accelerator-api","title":"Lab 3.1: Local Agent + Accelerator API","text":"<p>We built a small agent in the <code>simple-watsonx-environment</code> that:</p> <ul> <li>Exposes two tools to a Granite LLM:</li> <li><code>rag_service_tool(question)</code> \u2013 calls the accelerator <code>/ask</code> endpoint.</li> <li><code>calculator_tool(expression)</code> \u2013 does safe arithmetic.</li> <li>Uses Granite as a planner to:</li> <li>Decide which tool to use.</li> <li>Call it.</li> <li>Combine the tool result into a final answer.</li> </ul> <p>We also introduced logging for governance:</p> <ul> <li>Question, chosen tool, tool arguments.</li> <li>Tool output summary.</li> <li>Final answer, model id, latency.</li> </ul> <p>These logs can feed into:</p> <ul> <li><code>notebook:Analyze_Log_and_Feedback.ipynb</code> in the accelerator.  </li> <li>watsonx.governance Evaluation Studio.</li> </ul>"},{"location":"blog/day3-agentic-ai-orchestrate/#framework-demos-crewai-langgraph-orchestrate","title":"Framework Demos (CrewAI / LangGraph / Orchestrate)","text":"<p>Depending on time and interest, we also showed how the same idea looks in:</p> <ul> <li>CrewAI: an agent inside a crew uses the RAG service tool and calculator.  </li> <li>LangGraph: a small state graph with nodes for retrieval, generation, and evaluation.  </li> <li>watsonx Orchestrate: agents, tools, connections, and knowledge bases defined via YAML and ADK.</li> </ul> <p>You should now feel comfortable reading agent definitions in any of these styles.</p>"},{"location":"blog/day3-agentic-ai-orchestrate/#bridge-to-orchestrate-governance","title":"Bridge to Orchestrate &amp; Governance","text":"<p>We then zoomed out to see how this looks in a governed production setting:</p> <ul> <li>The accelerator <code>/ask</code> endpoint becomes a tool in Orchestrate.  </li> <li>Your Lab 3.1 planner prompt maps to an Orchestrate agent with instructions and tools.  </li> <li>Logs feed into Evaluation Studio and custom notebooks.  </li> <li>Policies in watsonx.governance restrict which models/tools can be used and how.</li> </ul> <p>Think of Orchestrate as the place where your Day 3 patterns grow up:</p> <ul> <li>Multiple agents collaborating.</li> <li>Secure connections to enterprise systems.</li> <li>Evaluations and monitoring over time.</li> </ul>"},{"location":"blog/day3-agentic-ai-orchestrate/#where-to-go-next","title":"Where to Go Next","text":"<p>Some natural next steps:</p> <ul> <li>Turn your agent notebook into:</li> <li>A reusable Python module.</li> <li>A more robust FastAPI service.</li> <li>Extend the accelerator with:</li> <li>Additional endpoints (e.g. <code>/batch-ask</code>).</li> <li>More tools (HR APIs, ticketing, monitoring).</li> <li>Experiment with:</li> <li>CrewAI and LangGraph implementations of the same RAG agent.</li> <li>watsonx Orchestrate Developer Edition to host agents locally.</li> </ul> <p>Finally, consider using the Capstone Day to:</p> <ul> <li>Pick a concrete use case (HR helper, internal FAQ bot, RAG debugger\u2026).  </li> <li>Build a thin but real data \u2192 RAG \u2192 agent \u2192 API/UI \u2192 evaluation story.  </li> <li>Share it with your wider team as a blueprint.</li> </ul> <p>Day 3 is the moment where all the pieces connect \u2014 from raw models, to RAG, to agents, to governed orchestration. Everything you build next can reuse these foundations.</p>"},{"location":"tracks/capstone/capstone-overview/","title":"Capstone Day \u2013 Overview","text":""},{"location":"tracks/capstone/capstone-overview/#purpose-of-the-capstone","title":"Purpose of the Capstone","text":"<ul> <li>Consolidate learning by building a small project.</li> <li>Encourage teamwork and creativity.</li> <li>Push the accelerator closer to production for at least one use case.</li> </ul>"},{"location":"tracks/capstone/capstone-overview/#format-schedule","title":"Format &amp; Schedule","text":"<p>Suggested 3\u20134 hour structure:</p> <ol> <li>Intro &amp; recap (20\u201330 min)</li> <li>Remind everyone of Days 1\u20133.</li> <li> <p>Clarify expectations and scoring (if any).</p> </li> <li> <p>Team formation and project selection (20\u201330 min)</p> </li> <li>2\u20134 people per team.</li> <li> <p>Pick or adapt a project idea (see project ideas file).</p> </li> <li> <p>Build time (2\u20132.5 h)</p> </li> <li> <p>Implement a small but end-to-end solution.</p> </li> <li> <p>Demos and feedback (30\u201360 min)</p> </li> <li>5\u201310 min per team.</li> <li>Explain goal, architecture, demo, lessons learned.</li> </ol>"},{"location":"tracks/capstone/capstone-overview/#expectations-for-teams","title":"Expectations for Teams","text":"<p>Teams should:</p> <ul> <li>Use both environments where possible (Ollama + watsonx).</li> <li>For a \u201cproduction-ready\u201d flavour, build on top of:</li> <li>The <code>accelerator</code> service (API + tools + UI), and/or</li> <li>RAG &amp; governance notebooks from <code>labs-src</code>.</li> </ul> <p>Produce:</p> <ul> <li>A working notebook or small service.</li> <li>A short explanation (slides or markdown) that covers:</li> <li>Problem statement.</li> <li>Architecture (how they used RAG, agents, orchestration).</li> <li>Limitations and next steps.</li> </ul>"},{"location":"tracks/capstone/capstone-overview/#assessment-criteria-optional","title":"Assessment Criteria (Optional)","text":"<p>If you choose to \u201cscore\u201d or rank projects, consider:</p> <ul> <li>Clarity of problem statement</li> <li> <p>Is it clear what the assistant or system is trying to solve?</p> </li> <li> <p>Technical implementation</p> </li> <li>Does it run end-to-end?</li> <li> <p>Does it use RAG and/or agents in a meaningful way?</p> </li> <li> <p>Use of workshop topics</p> </li> <li>RAG (Day 2).</li> <li>Orchestration / agents (Day 3).</li> <li> <p>Evaluation / governance (optional but encouraged).</p> </li> <li> <p>Presentation &amp; storytelling</p> </li> <li>Is the demo easy to follow?</li> <li>Do they reflect on what worked / didn\u2019t?</li> </ul>"},{"location":"tracks/capstone/capstone-overview/#deliverables","title":"Deliverables","text":"<p>Each team should provide:</p> <ul> <li>A repo or folder (or branch) with:</li> <li>Code / notebooks.</li> <li> <p>Instructions to run (README).</p> </li> <li> <p>A demo plan:</p> </li> <li>2\u20133 main use cases to show.</li> <li>Known limitations.</li> </ul> <p>Optional:</p> <ul> <li>PRs or patches against <code>accelerator</code> (if your org uses a shared repo).</li> </ul>"},{"location":"tracks/capstone/capstone-overview/#linking-to-project-ideas","title":"Linking to Project Ideas","text":"<p>You can reference the Capstone Project Ideas page for inspiration.</p> <p>Highlight projects that explicitly:</p> <ul> <li>Extend <code>retriever.py</code>, <code>pipeline.py</code>, <code>eval_small.py</code>, <code>ui/app.py</code>.</li> <li>Combine the two environments (local vs watsonx).</li> <li>Include some governance / evaluation flavour.</li> </ul> <p>Encourage teams to adapt scope to fit the available time.</p>"},{"location":"tracks/capstone/capstone-overview/#suggested-facilitation-tips","title":"Suggested Facilitation Tips","text":"<ul> <li>Keep teams small (2\u20134 people) so everyone can contribute.</li> <li>Encourage early \u201cthin slice\u201d demos:</li> <li>Simple but working RAG / agent path first.</li> <li>Add polish only if time allows.</li> <li>Have a floating \u201chelp desk\u201d (one or two facilitators) for debugging.</li> </ul> <p>The capstone is about learning and collaboration, not perfection.</p>"},{"location":"tracks/capstone/capstone-project-ideas/","title":"Capstone Project Ideas","text":""},{"location":"tracks/capstone/capstone-project-ideas/#how-to-use-this-list","title":"How to Use This List","text":"<ul> <li>Pick one idea or merge aspects from several.</li> <li>Adapt scope to fit 3\u20134 hours of work.</li> <li>Feel free to leverage code from:</li> <li>Day 1\u20133 labs.</li> <li><code>labs-src</code> notebooks.</li> <li><code>accelerator</code> Python modules &amp; notebooks.</li> </ul>"},{"location":"tracks/capstone/capstone-project-ideas/#1-workshop-faq-assistant-rag-over-the-course","title":"1. Workshop FAQ Assistant (RAG over the course)","text":"<ul> <li>Description</li> <li> <p>Build an assistant that can answer questions about the workshop itself:</p> <ul> <li>Agenda, labs, files, concepts.</li> </ul> </li> <li> <p>Required pieces</p> </li> <li>Corpus = workshop docs, READMEs, and lab guides.</li> <li>RAG in both:<ul> <li>Ollama env.</li> <li>watsonx env.</li> </ul> </li> <li> <p>Optional: deploy via <code>accelerator</code> service.</p> </li> <li> <p>Helpful accelerator files</p> </li> <li><code>rag/retriever.py</code>, <code>rag/pipeline.py</code>, <code>rag/prompt.py</code></li> <li><code>tools/chunk.py</code>, <code>tools/embed_index.py</code></li> <li> <p><code>service/api.py</code>, <code>ui/app.py</code></p> </li> <li> <p>Suggested stretch goals</p> </li> <li>Side-by-side comparison (Ollama vs watsonx).</li> <li>Evaluation with <code>tools/eval_small.py</code>.</li> </ul>"},{"location":"tracks/capstone/capstone-project-ideas/#2-internal-policy-hr-helper","title":"2. Internal Policy / HR Helper","text":"<ul> <li>Domain</li> <li> <p>Synthetic HR/policy docs.</p> </li> <li> <p>Build</p> </li> <li> <p>RAG with safety-aware prompting and refusal patterns.</p> </li> <li> <p>Use accelerator as</p> </li> <li> <p>A properly configured microservice for HR policy queries.</p> </li> <li> <p>Helpful reference notebooks</p> </li> <li>RAG examples in <code>labs-src</code>.</li> <li> <p>Governance notebooks for policy checks.</p> </li> <li> <p>Stretch goals</p> </li> <li>Confidence indicator (e.g. high/medium/low).</li> <li>Refusal or handoff for out-of-scope questions.</li> <li>Governance metrics via Evaluation Studio.</li> </ul>"},{"location":"tracks/capstone/capstone-project-ideas/#3-rag-debugger-evaluator","title":"3. RAG Debugger &amp; Evaluator","text":"<ul> <li>Focus</li> <li> <p>Evaluation rather than a new use case.</p> </li> <li> <p>Build</p> </li> <li> <p>Harness that compares different RAG settings/backends:</p> <ul> <li><code>k</code> values.</li> <li>Embedding models.</li> <li>Retriever strategies.</li> </ul> </li> <li> <p>Accelerator integration</p> </li> <li>Implement <code>tools/eval_small.py</code> to call <code>/ask</code>.</li> <li> <p>Use <code>notebook:Analyze_Log_and_Feedback.ipynb</code> to explore logs.</p> </li> <li> <p>Helpful reference notebooks</p> </li> <li>RAG notebooks in <code>labs-src</code>.</li> <li> <p>Governance evaluation notebook.</p> </li> <li> <p>Stretch goals</p> </li> <li>Simple dashboards (e.g. Streamlit or notebooks).</li> <li>Export to governance as evaluation datasets.</li> </ul>"},{"location":"tracks/capstone/capstone-project-ideas/#4-team-knowledge-hub-bot","title":"4. Team Knowledge Hub Bot","text":"<ul> <li>Corpus</li> <li> <p>Short articles written by team members about their domains.</p> </li> <li> <p>Build</p> </li> <li> <p>RAG assistant that attributes answers to authors and links to sources.</p> </li> <li> <p>Use accelerator to</p> </li> <li> <p>Host the service with a tailored <code>ui/app.py</code> Streamlit interface.</p> </li> <li> <p>Helpful reference notebooks</p> </li> <li><code>labs-src/use-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb</code></li> <li> <p>Accelerator ingestion notebooks.</p> </li> <li> <p>Stretch goals</p> </li> <li>User authentication hooks.</li> <li>Session-aware UI and feedback collection.</li> </ul>"},{"location":"tracks/capstone/capstone-project-ideas/#5-orchestrated-assistant-with-tools","title":"5. Orchestrated Assistant with Tools","text":"<ul> <li>Build</li> <li> <p>Agent in watsonx env that chooses between tools:</p> <ul> <li>Accelerator RAG API.</li> <li>Calculator.</li> <li>Possibly others (ticket creation, CRM, etc.).</li> </ul> </li> <li> <p>Helpful files</p> </li> <li><code>service/api.py</code>, <code>service/deps.py</code> (as the RAG microservice).</li> <li>Agent notebook from Day 3.</li> <li> <p>Governance notebooks (<code>labs-src</code>).</p> </li> <li> <p>Stretch goals</p> </li> <li>Logging suitable for governance.</li> <li>Simple \u201csession view\u201d for conversations and tool calls.</li> </ul>"},{"location":"tracks/capstone/capstone-project-ideas/#custom-project-ideas","title":"Custom Project Ideas","text":"<p>Encourage participants to propose:</p> <ul> <li>Domain-specific assistants (e.g., finance, support, internal engineering docs).</li> <li>Integrations with existing datasets or APIs.</li> <li>Extensions to accelerator:</li> <li>Additional endpoints (e.g. <code>/ingest</code>, <code>/batch-ask</code>).</li> <li>Batch scoring APIs.</li> <li>Admin UI for corpus management.</li> </ul>"},{"location":"tracks/capstone/capstone-project-ideas/#tips-for-success","title":"Tips for Success","text":"<ul> <li>Start from existing lab notebooks and accelerator scripts.</li> <li>Aim for a clear end-to-end story: data \u2192 RAG \u2192 API/UI \u2192 evaluation.</li> <li>Limit scope to something demoable within the time.</li> <li>Focus on one or two new ideas, not everything at once.</li> </ul>"},{"location":"tracks/day0-env/prereqs-and-accounts/","title":"0.1 Prerequisites &amp; Accounts","text":"<p>Welcome to Day 0 of the watsonx Workshop Series \ud83d\udc4b  </p> <p>This is our \"pre-flight check\" session. The goal is simple: by the end of this module, you'll know exactly what you need (laptop, software, cloud accounts, repos) so that Day 1 can be 100% hands-on instead of 100% debugging.</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#audience-workshop-overview","title":"Audience &amp; Workshop Overview","text":"<p>This workshop is designed for:</p> <ul> <li>Data scientists &amp; ML engineers who want to go from \"LLM playground\" to RAG and agents in production.</li> <li>Developers &amp; architects who need to connect LLMs to real systems (APIs, data stores, governance).</li> <li>Technical leaders evaluating how watsonx.ai, local LLMs (Ollama), and a RAG accelerator fit into their stack.</li> </ul> <p>You don't need to be a deep learning researcher, but you should be comfortable with:</p> <ul> <li>Basic Python (functions, virtualenvs, <code>pip</code>, Jupyter).</li> <li>Running commands in a terminal.</li> <li>Very basic Docker concepts (build image, run container).</li> </ul>"},{"location":"tracks/day0-env/prereqs-and-accounts/#workshop-structure","title":"Workshop structure","text":"<p>We'll work across 3 core days plus an optional Day 0 and optional Capstone:</p> <ul> <li>Day 0 (Monday, 2h) \u2013 Environment setup  </li> <li>Install tools, clone repos, test notebooks.</li> <li>Day 1 (Tuesday) \u2013 LLMs &amp; Prompting (Ollama vs watsonx.ai)</li> <li>Day 2 (Wednesday) \u2013 RAG (Retrieval-Augmented Generation)</li> <li>Day 3 (Thursday) \u2013 Orchestration, Agents &amp; Recap  </li> <li>Use your RAG as a service, think about governance.</li> <li>Optional Capstone (Friday) \u2013 Team project &amp; demos</li> </ul>"},{"location":"tracks/day0-env/prereqs-and-accounts/#three-core-codebases-well-use","title":"Three core codebases we'll use","text":"<p>We will spend the entire workshop inside three codebases:</p> <ol> <li><code>simple-ollama-environment</code>    Local LLM sandbox built around Ollama:</li> <li>Python 3.11 + Jupyter.</li> <li>Ollama server, running models on your machine or in Docker.</li> <li> <p>Notebook: <code>notebooks/ollama_quickstart.ipynb</code>.</p> </li> <li> <p><code>simple-watsonx-enviroment</code>    watsonx.ai sandbox for Granite / Llama models:</p> </li> <li>Python 3.11 + Jupyter.</li> <li>IBM watsonx.ai SDK + LangChain integration.</li> <li>Notebook: <code>notebooks/watsonx_quickstart.ipynb</code>.</li> <li> <p>Reads credentials from a <code>.env</code> file.</p> </li> <li> <p><code>accelerator/</code> (inside <code>watsonx-workshop</code>)    A RAG production skeleton:</p> </li> <li>API: FastAPI service in <code>service/</code> wrapping <code>rag/pipeline.py</code>.</li> <li>RAG core: <code>rag/</code> (<code>pipeline.py</code>, <code>retriever.py</code>, <code>prompt.py</code>).</li> <li>Tools: ingestion, chunking, embedding, evaluation under <code>tools/</code>.</li> <li>UI: Streamlit app in <code>ui/app.py</code>.</li> <li>Assets &amp; notebooks under <code>assets/</code>.</li> </ol> <p>We start with the two env repos on Day 0 and begin \"wiring in\" the accelerator from Day 2 onwards.</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#technical-prerequisites","title":"Technical Prerequisites","text":"<p>Before you can follow the labs, make sure your machine meets these requirements.</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#operating-system","title":"Operating system","text":"<p>You should be able to use any of:</p> <ul> <li>Windows 10+</li> <li>macOS 12+</li> <li>Linux (Ubuntu 20.04+, Debian, Fedora, etc.)</li> </ul> <p>If you're on a locked-down corporate laptop, you may need help from IT to install Docker or run containers.</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#minimum-hardware","title":"Minimum hardware","text":"<p>These are not hard limits, but they're good guidelines:</p> <ul> <li>CPU: 4+ cores  </li> <li>RAM: 16 GB recommended (8 GB possible with smaller models)  </li> <li>Disk: 20\u201330 GB free (Docker images + models + notebooks)</li> </ul> <p>For local LLMs via Ollama:</p> <ul> <li>Tiny models (0.5B\u20131B parameters) are fine on 8 GB RAM.</li> <li>7B models are happier with ~16 GB RAM.</li> <li>Bigger models (13B, 33B) are not recommended on small laptops.</li> </ul>"},{"location":"tracks/day0-env/prereqs-and-accounts/#software-you-must-have","title":"Software you must have","text":"<p>You will need:</p> <ul> <li>Git</li> <li>Python 3.11</li> <li>Docker (Docker Desktop on Win/macOS, Docker Engine on Linux)</li> <li>A modern web browser (Chrome, Edge, Firefox, Safari)</li> </ul> <p>You can survive without Docker, but the experience will be smoother with it, especially for Ollama.</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#accounts-access","title":"Accounts &amp; Access","text":"<p>To use watsonx.ai you need an IBM Cloud account and access to the watsonx services.</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#ibm-cloud","title":"IBM Cloud","text":"<ol> <li>Create or use an existing IBM Cloud account.</li> <li>Ensure you have access to:</li> <li>watsonx.ai</li> <li>(Optional, but recommended) watsonx.governance</li> <li>(Optional) watsonx.orchestrate</li> </ol> <p>Your instructor / organizer should tell you:</p> <ul> <li>Which region to use (e.g., <code>us-south</code>).</li> <li>Whether you'll use a shared project or create your own.</li> <li>If there is a pre-configured resource group.</li> </ul>"},{"location":"tracks/day0-env/prereqs-and-accounts/#watsonx-project-information","title":"watsonx project information","text":"<p>You will need:</p> <ul> <li>IBM Cloud API key</li> <li>watsonx endpoint URL   e.g. <code>https://us-south.ml.cloud.ibm.com</code></li> <li>Project or space ID   (depending on how your environment is configured)</li> </ul> <p>These values go into <code>.env</code> for <code>simple-watsonx-enviroment</code>:</p> <pre><code>IBM_CLOUD_API_KEY=...\nIBM_CLOUD_URL=https://us-south.ml.cloud.ibm.com\nIBM_CLOUD_PROJECT_ID=...\n</code></pre> <p>or, using the compatibility names:</p> <pre><code>WATSONX_APIKEY=...\nWATSONX_URL=https://us-south.ml.cloud.ibm.com\nPROJECT_ID=...\n</code></pre> <p>We'll walk through this again in 0.3 Setup <code>simple-watsonx-enviroment</code>.</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#tools-to-install-before-day-0-optional-but-strongly-recommended","title":"Tools to Install Before Day 0 (Optional but strongly recommended)","text":"<p>If you have time before the workshop, install these locally so Day 0 is just validation.</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#git","title":"Git","text":"<ul> <li>Windows:   Download Git for Windows from the official site and follow the installer.</li> <li>macOS:   Git usually comes via Xcode Command Line Tools:</li> </ul> <p><pre><code>xcode-select --install\n</code></pre> * Linux (Ubuntu example):</p> <pre><code>sudo apt-get update\nsudo apt-get install -y git\n</code></pre>"},{"location":"tracks/day0-env/prereqs-and-accounts/#python-311","title":"Python 3.11","text":"<ul> <li>Windows:   Install from python.org and check \"Add to PATH\".</li> <li>macOS (Homebrew):</li> </ul> <p><pre><code>brew install python@3.11\n</code></pre> * Linux (Ubuntu 22.04 example):</p> <pre><code>sudo apt-get update\nsudo apt-get install -y python3.11 python3.11-venv python3.11-dev\n</code></pre> <p>Verify:</p> <pre><code>python3.11 --version\n</code></pre>"},{"location":"tracks/day0-env/prereqs-and-accounts/#docker","title":"Docker","text":"<ul> <li>Windows / macOS:   Install Docker Desktop from docker.com and enable WSL2/backing engine as needed.</li> <li>Linux (Ubuntu):   Follow the official Docker Engine docs, or something like:</li> </ul> <pre><code>curl -fsSL https://get.docker.com | sh\nsudo usermod -aG docker \"$USER\"\n</code></pre> <p>Then log out &amp; back in so your user is in the <code>docker</code> group.</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#jupyter-optional","title":"Jupyter (optional)","text":"<p>We install Jupyter via the project <code>Makefile</code>, but if you want a global install:</p> <pre><code>pip install jupyter\n</code></pre>"},{"location":"tracks/day0-env/prereqs-and-accounts/#reference-repositories-assets","title":"Reference Repositories &amp; Assets","text":"<p>During the workshop you will clone and/or have access to:</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#repositories","title":"Repositories","text":"<ul> <li> <p><code>simple-ollama-environment</code>   Minimal Python 3.11 + Jupyter + Ollama setup, with:</p> </li> <li> <p>Docker support.</p> </li> <li> <p><code>notebooks/ollama_quickstart.ipynb</code>.</p> </li> <li> <p><code>simple-watsonx-enviroment</code>   Minimal Python 3.11 + Jupyter + watsonx.ai integration:</p> </li> <li> <p><code>.env.sample</code> for credentials.</p> </li> <li><code>notebooks/watsonx_quickstart.ipynb</code>.</li> <li> <p>Dockerfile + Makefile for easy setup.</p> </li> <li> <p><code>watsonx-workshop</code>   The repository that hosts:</p> </li> <li> <p>This documentation.</p> </li> <li> <p>The <code>accelerator/</code> folder:</p> <ul> <li><code>rag/</code> \u2013 retrieval + pipeline code.</li> <li><code>service/</code> \u2013 FastAPI API.</li> <li><code>tools/</code> \u2013 ingestion &amp; evaluation scripts.</li> <li><code>ui/</code> \u2013 Streamlit UI.</li> <li><code>labs-src/</code> \u2013 reference notebooks for RAG &amp; governance.</li> </ul> </li> </ul>"},{"location":"tracks/day0-env/prereqs-and-accounts/#notebook-galleries","title":"Notebook galleries","text":"<p>We won't run all of these line-by-line, but they are excellent reference implementations you can borrow from.</p>"},{"location":"tracks/day0-env/prereqs-and-accounts/#labs-src-rag-governance-examples","title":"<code>labs-src/</code> \u2013 RAG &amp; governance examples","text":"<ul> <li><code>use-watsonx-elasticsearch-and-langchain-to-answer-questions-rag.ipynb</code></li> <li><code>use-watsonx-and-elasticsearch-python-sdk-to-answer-questions-rag.ipynb</code></li> <li><code>use-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb</code></li> <li><code>ibm-watsonx-governance-evaluation-studio-getting-started.ipynb</code></li> <li><code>ibm-watsonx-governance-governed-agentic-catalog.ipynb</code></li> </ul>"},{"location":"tracks/day0-env/prereqs-and-accounts/#acceleratorassetsnotebook-rag-production-workflow","title":"<code>accelerator/assets/notebook/</code> \u2013 RAG production workflow","text":"<ul> <li><code>notebook:Process_and_Ingest_Data_into_Vector_DB.ipynb</code></li> <li><code>notebook:Process_and_Ingest_Data_from_COS_into_vector_DB.ipynb</code></li> <li><code>notebook:Ingestion_of_Expert_Profile_data_to_Vector_DB.ipynb</code></li> <li><code>notebook:QnA_with_RAG.ipynb</code></li> <li><code>notebook:Create_and_Deploy_QnA_AI_Service.ipynb</code></li> <li><code>notebook:Test_Queries_for_Vector_DB.ipynb</code></li> <li><code>notebook:Analyze_Log_and_Feedback.ipynb</code></li> </ul> <p>These will show you:</p> <ul> <li>How to go from raw documents \u2192 chunks \u2192 embeddings \u2192 vector DB.</li> <li>How to implement a Q&amp;A RAG pipeline.</li> <li>How to package and deploy a Q&amp;A service.</li> <li>How to analyze logs and feedback.</li> </ul>"},{"location":"tracks/day0-env/prereqs-and-accounts/#what-you-will-have-after-day-0","title":"What You Will Have After Day 0","text":"<p>By the end of Day 0, you should have:</p> <ul> <li> <p>\u2705 Cloned:</p> </li> <li> <p><code>simple-ollama-environment</code></p> </li> <li><code>simple-watsonx-enviroment</code></li> <li><code>watsonx-workshop</code> (with <code>accelerator/</code> and <code>labs-src/</code>)</li> <li>\u2705 Working Jupyter in both env repos.</li> <li>\u2705 A basic Ollama chat running from <code>ollama_quickstart.ipynb</code>.</li> <li>\u2705 A basic Granite / watsonx.ai call running from <code>watsonx_quickstart.ipynb</code>.</li> <li>\u2705 The <code>accelerator/</code> folder available locally.</li> <li>\u2705 All reference notebooks (<code>labs-src/</code> and accelerator notebooks) opening in Jupyter.</li> </ul> <p>When those boxes are ticked, you're ready to hit the ground running on Day 1.</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/","title":"0.2 Setup <code>simple-ollama-environment</code>","text":"<p>In this section we'll get your local LLM sandbox running: Python 3.11, Jupyter, and Ollama packaged together in a reproducible way.</p> <p>You can choose either:</p> <ul> <li>A Docker-first setup (recommended for consistency), or</li> <li>A local virtual environment using your host's Python 3.11 and an existing Ollama install.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#goal","title":"Goal","text":"<p>By the end of this lab you will:</p> <ul> <li>Have the <code>simple-ollama-environment</code> repo cloned.</li> <li>Be able to launch a Jupyter Notebook.</li> <li>Run <code>notebooks/ollama_quickstart.ipynb</code> and chat with a local LLM (e.g., <code>qwen2.5:0.5b-instruct</code> or <code>llama3.2:1b</code>).</li> </ul>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#repository-overview","title":"Repository Overview","text":"<p>Once cloned, you'll see something like:</p> <pre><code>simple-ollama-environment/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 assets/\n\u2502   \u2514\u2500\u2500 screenshot.png (example)\n\u2514\u2500\u2500 notebooks/\n    \u2514\u2500\u2500 ollama_quickstart.ipynb\n</code></pre> <p>Key pieces:</p> <ul> <li> <p><code>Dockerfile</code>   Builds a container image that bundles:</p> </li> <li> <p>Python 3.11</p> </li> <li>Jupyter</li> <li>Ollama (server + CLI)</li> <li> <p>A small pre-pulled model (configurable)</p> </li> <li> <p><code>Makefile</code>   Cross-platform shortcuts for:</p> </li> <li> <p><code>make install</code> \u2013 local venv + kernel.</p> </li> <li><code>make build-container</code> \u2013 Docker image.</li> <li> <p><code>make run-container</code> \u2013 run image with ports &amp; volumes.</p> </li> <li> <p><code>pyproject.toml</code>   Python dependencies and metadata.</p> </li> <li> <p><code>notebooks/ollama_quickstart.ipynb</code>   A notebook that:</p> </li> <li> <p>Ensures the Python Ollama client is installed.</p> </li> <li>Verifies the Ollama API is reachable.</li> <li>Runs a basic chat call.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#step-1-clone-the-repository","title":"Step 1 \u2013 Clone the Repository","text":"<p>Pick or create a parent folder for all workshop repos:</p> <pre><code>mkdir -p ~/projects/watsonx-workshop\ncd ~/projects/watsonx-workshop\n</code></pre> <p>Clone:</p> <pre><code>git clone https://github.com/ruslanmv/simple-ollama-environment.git\ncd simple-ollama-environment\n</code></pre> <p>You should now be inside the repo root.</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#step-2-choose-setup-path","title":"Step 2 \u2013 Choose Setup Path","text":"<p>You have two main options.</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#option-a-docker-recommended","title":"Option A \u2013 Docker (recommended)","text":"<p>Best if:</p> <ul> <li>You want minimal local setup.</li> <li>You're happy to let Docker handle Python + Ollama.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#a1-build-the-container-image","title":"A.1 Build the container image","text":"<p>From the repo root:</p> <pre><code>make build-container\n</code></pre> <p>Under the hood this runs <code>docker build</code> and creates an image (for example <code>simple-ollama-environment:latest</code>) that includes:</p> <ul> <li>Python 3.11 + dependencies.</li> <li>Jupyter.</li> <li>Ollama server + client.</li> <li>A tiny pre-pulled model (configurable via <code>PREPULL</code> build arg).</li> </ul>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#a2-run-the-container","title":"A.2 Run the container","text":"<p>Launch the environment:</p> <pre><code>make run-container\n</code></pre> <p>This will:</p> <ul> <li>Start a container named something like <code>simple-ollama-env</code>.</li> <li> <p>Expose:</p> </li> <li> <p>Jupyter on <code>http://localhost:8888</code></p> </li> <li>Ollama API on <code>http://localhost:11434</code></li> <li>Mount your current folder into <code>/workspace</code> so notebooks and changes persist.</li> </ul> <p>If you prefer to run manually:</p> <pre><code>docker run -d --name simple-ollama-env \\\n  -p 8888:8888 -p 11434:11434 \\\n  -v \"$PWD\":/workspace \\\n  -v ollama_models:/root/.ollama \\\n  docker.io/ruslanmv/simple-ollama-environment:latest\n</code></pre> <p>Tip: The <code>ollama_models</code> named volume keeps your models cached across container runs.</p> <p>Open Jupyter:</p> <ul> <li>Go to <code>http://localhost:8888</code> in your browser.</li> <li>Copy the token from the container logs if needed (<code>docker logs simple-ollama-env</code>).</li> </ul>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#option-b-local-python-environment","title":"Option B \u2013 Local Python Environment","text":"<p>Best if:</p> <ul> <li>You already have Ollama installed on your host.</li> <li>You prefer to run Jupyter directly on your machine.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#b1-install-ollama-on-your-machine","title":"B.1 Install Ollama on your machine","text":"<p>Follow https://ollama.com/download or:</p> <ul> <li>macOS</li> </ul> <p><pre><code>brew install --cask ollama\n</code></pre> * Windows   Use the GUI installer or <code>winget install Ollama.Ollama</code>. * Linux</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre> <p>Make sure the Ollama service is running (it usually starts automatically).</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#b2-create-a-virtual-environment-install-deps","title":"B.2 Create a virtual environment &amp; install deps","text":"<p>From the repo root:</p> <pre><code>make install\n</code></pre> <p>This will:</p> <ul> <li>Create a Python 3.11 virtualenv.</li> <li>Install dependencies from <code>pyproject.toml</code>.</li> <li>Register a Jupyter kernel called something like \"Python 3.11 (simple-env)\".</li> </ul> <p>To start Jupyter:</p> <pre><code>jupyter notebook\n</code></pre> <p>Then choose the \"Python 3.11 (simple-env)\" kernel.</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#step-3-install-configure-ollama-models","title":"Step 3 \u2013 Install &amp; Configure Ollama Models","text":"<p>If you're using the Docker image with <code>OLLAMA_PREPULL</code>, some models may already be present. Otherwise, you can pull them yourself.</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#pull-a-small-model","title":"Pull a small model","text":"<p>Examples:</p> <pre><code># From host or inside container:\nollama pull qwen2.5:0.5b-instruct\nollama pull llama3.2:1b\n</code></pre> <p>These are small enough to work well on most laptops.</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#quick-health-check","title":"Quick health check","text":"<p>With the container running, you can test:</p> <pre><code>curl http://localhost:11434/api/tags\n</code></pre> <p>You should see JSON listing available models.</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#step-4-run-ollama_quickstartipynb","title":"Step 4 \u2013 Run <code>ollama_quickstart.ipynb</code>","text":"<p>Now let's test end-to-end.</p> <ol> <li>Open Jupyter (either inside the container or local).</li> <li>Navigate to <code>notebooks/</code>.</li> <li>Open <code>ollama_quickstart.ipynb</code>.</li> <li>Run the cells top to bottom.</li> </ol> <p>You should see something along the lines of:</p> <pre><code>import ollama\n\nresponse = ollama.chat(\n    model=\"qwen2.5:0.5b-instruct\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about AI and coffee.\"}],\n)\nprint(response[\"message\"][\"content\"])\n</code></pre> <p>If everything is wired correctly, you'll get a text response from the model.</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#how-this-relates-to-rag-the-accelerator","title":"How This Relates to RAG &amp; the Accelerator","text":"<p>Right now, you're just sending plain prompts to a local model, but the same patterns will be used later when you:</p> <ul> <li>Implement a local RAG notebook (<code>rag_local_ollama.ipynb</code>).</li> <li>Compare local RAG vs watsonx.ai RAG.</li> <li>Treat local LLMs and watsonx.ai as interchangeable \"generation backends\" in the accelerator.</li> </ul> <p>What you're learning here:</p> <ul> <li> <p>How to:</p> </li> <li> <p>Talk to Ollama's HTTP API / Python client.</p> </li> <li>Run notebooks in a controlled environment.</li> <li> <p>Will directly transfer to:</p> </li> <li> <p>Calling watsonx.ai in the other repo.</p> </li> <li>Plugging a watsonx.ai LLM into the <code>accelerator/rag/pipeline.py</code>.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tracks/day0-env/setup-simple-ollama-environment/#ollama-not-reachable","title":"Ollama not reachable","text":"<ul> <li>Make sure the container is running (<code>docker ps</code>) or the desktop app/service is started.</li> <li>Check that <code>curl http://localhost:11434/api/tags</code> returns JSON.</li> <li>In Docker: ensure you mapped <code>-p 11434:11434</code>.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#jupyter-token-issues","title":"Jupyter token issues","text":"<ul> <li>If the browser asks for a token:</li> </ul> <pre><code>docker logs simple-ollama-env | grep \"http://127.0.0.1\"\n</code></pre> <p>Copy the URL with the token.</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#model-too-big-out-of-memory","title":"Model too big / out of memory","text":"<ul> <li> <p>If 7B or 13B models crash:</p> </li> <li> <p>Use smaller models (0.5B\u20131B).</p> </li> <li>Close other applications.</li> <li>Reduce concurrency.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#ports-already-in-use","title":"Ports already in use","text":"<ul> <li>Another app may be using <code>8888</code> or <code>11434</code>.</li> <li>In Docker, you can adjust:</li> </ul> <pre><code>docker run -d --name simple-ollama-env \\\n  -p 8890:8888 -p 11435:11434 \\\n  ...\n</code></pre> <p>Then Jupyter will be on <code>http://localhost:8890</code>, Ollama on <code>http://localhost:11435</code>.</p>"},{"location":"tracks/day0-env/setup-simple-ollama-environment/#checklist","title":"Checklist","text":"<p>Before moving on:</p> <ul> <li>\u2705 Repo cloned (<code>simple-ollama-environment</code>)</li> <li>\u2705 Dependencies installed (Docker image or venv)</li> <li>\u2705 Jupyter starts successfully</li> <li>\u2705 <code>ollama_quickstart.ipynb</code> runs a model and prints a response</li> </ul> <p>If all green: you're ready to set up <code>simple-watsonx-enviroment</code> next.</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/","title":"0.3 Setup <code>simple-watsonx-enviroment</code>","text":"<p>Now we'll set up your watsonx.ai sandbox: a clean Python 3.11 + Jupyter environment that knows how to talk to Granite / Llama models hosted on IBM watsonx.ai.</p> <p>You can run it locally (virtualenv) or via Docker with minimal fuss.</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#goal","title":"Goal","text":"<p>By the end of this lab you will:</p> <ul> <li>Have the <code>simple-watsonx-enviroment</code> repo cloned.</li> <li>Provide IBM Cloud credentials via a <code>.env</code> file.</li> <li>Run <code>notebooks/watsonx_quickstart.ipynb</code> and generate text with a Granite model.</li> <li>Understand how this environment relates to the RAG accelerator you'll use on Day 2\u20133.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#repository-overview","title":"Repository Overview","text":"<p>The repo layout looks like:</p> <pre><code>simple-watsonx-enviroment/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 .env.sample\n\u251c\u2500\u2500 notebooks/\n\u2502   \u2514\u2500\u2500 watsonx_quickstart.ipynb\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 mac/\n    \u251c\u2500\u2500 ubuntu/\n    \u2514\u2500\u2500 windows/\n</code></pre> <p>Key components:</p> <ul> <li> <p><code>Dockerfile</code>   Builds a container with:</p> </li> <li> <p>Python 3.11.</p> </li> <li>Jupyter.</li> <li><code>ibm-watsonx-ai</code> SDK.</li> <li> <p><code>langchain-ibm</code> for LLM integration.</p> </li> <li> <p><code>Makefile</code>   Offers shortcuts like:</p> </li> <li> <p><code>make install</code> \u2013 local venv + Jupyter kernel.</p> </li> <li><code>make build-container</code> \u2013 build Docker image.</li> <li> <p><code>make run-container</code> \u2013 run container with <code>.env</code>.</p> </li> <li> <p><code>.env.sample</code>   Template for your IBM Cloud credentials. You'll copy this to <code>.env</code>.</p> </li> <li> <p><code>notebooks/watsonx_quickstart.ipynb</code>   First contact with watsonx.ai from Python.</p> </li> </ul>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#step-1-clone-the-repository","title":"Step 1 \u2013 Clone the Repository","text":"<p>From your main workshop folder:</p> <pre><code>cd ~/projects/watsonx-workshop   # or your path\ngit clone https://github.com/ruslanmv/simple-watsonx-enviroment.git\ncd simple-watsonx-enviroment\n</code></pre> <p>You now have both env repos side by side.</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#step-2-configure-env-credentials","title":"Step 2 \u2013 Configure <code>.env</code> (Credentials)","text":"<p>This is the most important step: teaching the environment how to authenticate to watsonx.ai.</p> <ol> <li>Copy the sample file:</li> </ol> <pre><code>cp .env.sample .env\n</code></pre> <ol> <li>Edit <code>.env</code> with your IBM Cloud details:</li> </ol> <pre><code># Preferred variables\nIBM_CLOUD_API_KEY=your_api_key_here\nIBM_CLOUD_URL=https://us-south.ml.cloud.ibm.com\nIBM_CLOUD_PROJECT_ID=your_project_id_here\n\n# Compatibility aliases (optional)\nWATSONX_APIKEY=${IBM_CLOUD_API_KEY}\nWATSONX_URL=${IBM_CLOUD_URL}\nPROJECT_ID=${IBM_CLOUD_PROJECT_ID}\n</code></pre> <p>Where to find these values:</p> <ul> <li> <p>IBM_CLOUD_API_KEY</p> </li> <li> <p>IBM Cloud console \u2192 Manage \u2192 Access (IAM) \u2192 API keys.</p> </li> <li> <p>IBM_CLOUD_URL</p> </li> <li> <p>Typically <code>https://&lt;region&gt;.ml.cloud.ibm.com</code>, e.g. <code>us-south</code>.</p> </li> <li> <p>IBM_CLOUD_PROJECT_ID</p> </li> <li> <p>From your watsonx.ai project (or space) details in the UI.</p> </li> </ul> <p>\ud83d\udd10 Treat this file as secret. Don't commit <code>.env</code> to git.</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#step-3-choose-setup-path","title":"Step 3 \u2013 Choose Setup Path","text":""},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#option-a-local-virtualenv","title":"Option A \u2013 Local (virtualenv)","text":"<p>From the repo root:</p> <pre><code>make install\n</code></pre> <p>This will:</p> <ul> <li>Create a virtual environment.</li> <li>Install Python dependencies from <code>pyproject.toml</code>.</li> <li>Register a Jupyter kernel, e.g. \"Python 3.11 (watsonx-env)\".</li> </ul> <p>Start Jupyter:</p> <pre><code>jupyter notebook\n</code></pre> <p>Then choose the watsonx-env kernel when opening notebooks.</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#option-b-docker-recommended-for-team-consistency","title":"Option B \u2013 Docker (recommended for team consistency)","text":"<p>From the repo root:</p> <pre><code>make build-container\nmake run-container\n</code></pre> <p>This will:</p> <ul> <li>Build an image (e.g. <code>simple-watsonx-env:latest</code>).</li> <li> <p>Run a container:</p> </li> <li> <p>Mounts your <code>.env</code>.</p> </li> <li>Mounts current directory to <code>/workspace</code>.</li> <li>Exposes Jupyter on <code>http://localhost:8888</code>.</li> </ul> <p>Equivalent manual command:</p> <pre><code>docker run -d --name watsonx-env \\\n  --env-file .env \\\n  -p 8888:8888 \\\n  -v \"$(pwd)\":/workspace \\\n  simple-watsonx-env:latest\n</code></pre> <p>Now open <code>http://localhost:8888</code> in your browser.</p> <p>Your notebooks and code edits stay on the host because of the volume mount.</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#step-4-run-watsonx_quickstartipynb","title":"Step 4 \u2013 Run <code>watsonx_quickstart.ipynb</code>","text":"<p>Time to confirm that credentials + environment are correct.</p> <ol> <li>Open Jupyter (local or container).</li> <li>Navigate to <code>notebooks/</code>.</li> <li>Open <code>watsonx_quickstart.ipynb</code>.</li> <li>Run the cells in order.</li> </ol> <p>A typical pattern inside the notebook looks like:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom ibm_watsonx_ai import APIClient, Credentials\nfrom ibm_watsonx_ai.foundation_models import ModelInference\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n\nload_dotenv()\n\napi_key = os.getenv(\"IBM_CLOUD_API_KEY\") or os.getenv(\"WATSONX_APIKEY\")\nurl = os.getenv(\"IBM_CLOUD_URL\") or os.getenv(\"WATSONX_URL\")\nproject_id = os.getenv(\"IBM_CLOUD_PROJECT_ID\") or os.getenv(\"PROJECT_ID\")\n\ncredentials = Credentials(url=url, api_key=api_key)\nclient = APIClient(credentials=credentials, project_id=project_id)\n\nmodel_id = \"ibm/granite-13b-instruct-v2\"\nprompt = \"Write a short story about a robot who wants to be a painter.\"\n\nparams = {\n    GenParams.DECODING_METHOD: \"greedy\",\n    GenParams.MAX_NEW_TOKENS: 200,\n}\n\nmodel = ModelInference(\n    model_id=model_id,\n    credentials=credentials,\n    project_id=project_id,\n)\nresponse = model.generate_text(prompt=prompt, params=params)\nprint(response)\n</code></pre> <p>If everything is configured correctly, you'll see model output printed in the notebook.</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#optional-langchain-integration","title":"Optional: LangChain Integration","text":"<p>If you prefer LangChain style:</p> <pre><code>from langchain_ibm import WatsonxLLM\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\napi_key = os.getenv(\"IBM_CLOUD_API_KEY\") or os.getenv(\"WATSONX_APIKEY\")\nurl = os.getenv(\"IBM_CLOUD_URL\") or os.getenv(\"WATSONX_URL\")\nproject_id = os.getenv(\"IBM_CLOUD_PROJECT_ID\") or os.getenv(\"PROJECT_ID\")\n\nllm = WatsonxLLM(\n    model_id=\"ibm/granite-13b-instruct-v2\",\n    url=url,\n    apikey=api_key,\n    project_id=project_id,\n    params={\"decoding_method\": \"greedy\", \"max_new_tokens\": 128},\n)\n\nprint(llm.invoke(\"Give me 3 study tips for Python.\"))\n</code></pre> <p>We'll build on this pattern in later labs.</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#connection-to-the-accelerator-project","title":"Connection to the <code>accelerator/</code> Project","text":"<p>The accelerator inside <code>watsonx-workshop/accelerator/</code> is where you'll build a production-like RAG service:</p> <ul> <li> <p>Core RAG logic:</p> </li> <li> <p><code>rag/retriever.py</code></p> </li> <li><code>rag/pipeline.py</code></li> <li><code>rag/prompt.py</code></li> <li> <p>API:</p> </li> <li> <p><code>service/api.py</code> \u2013 FastAPI app exposing <code>POST /ask</code>.</p> </li> <li><code>service/deps.py</code> \u2013 holds configuration (URL, API key, project, index names).</li> <li> <p>Tools:</p> </li> <li> <p><code>tools/chunk.py</code>, <code>tools/extract.py</code>, <code>tools/embed_index.py</code>, <code>tools/eval_small.py</code></p> </li> <li> <p>UI:</p> </li> <li> <p><code>ui/app.py</code> \u2013 Streamlit front-end.</p> </li> </ul> <p>The patterns you used in <code>watsonx_quickstart.ipynb</code>:</p> <ul> <li>Loading credentials from <code>.env</code>.</li> <li>Creating a watsonx.ai client.</li> <li>Calling Granite with structured params.</li> </ul> <p>\u2026 will later be refactored into:</p> <ul> <li>Setup code in <code>service/deps.py</code>.</li> <li>Model invocation logic in <code>rag/pipeline.py</code>.</li> </ul> <p>Think of <code>simple-watsonx-enviroment</code> as your playground and <code>accelerator/</code> as your real application.</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#reference-notebooks-in-labs-src-and-acceleratorassetsnotebook","title":"Reference Notebooks in <code>labs-src/</code> and <code>accelerator/assets/notebook/</code>","text":"<p>Once your environment is stable, it's worth quickly skimming some reference notebooks:</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#rag-vector-db-examples-labs-src","title":"RAG &amp; vector DB examples (<code>labs-src/</code>)","text":"<ul> <li>Elasticsearch + LangChain <code>use-watsonx-elasticsearch-and-langchain-to-answer-questions-rag.ipynb</code></li> <li>Elasticsearch Python SDK <code>use-watsonx-and-elasticsearch-python-sdk-to-answer-questions-rag.ipynb</code></li> <li>Chroma + LangChain <code>use-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb</code></li> </ul> <p>These will inspire your implementation of:</p> <ul> <li>RAG pipelines in Day 2 labs.</li> <li><code>retriever.py</code> &amp; <code>pipeline.py</code> in the accelerator.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#accelerator-notebooks-acceleratorassetsnotebook","title":"Accelerator notebooks (<code>accelerator/assets/notebook/</code>)","text":"<ul> <li> <p>Ingestion &amp; indexing:</p> </li> <li> <p><code>Process_and_Ingest_Data_into_Vector_DB.ipynb</code></p> </li> <li><code>Process_and_Ingest_Data_from_COS_into_vector_DB.ipynb</code></li> <li><code>Ingestion_of_Expert_Profile_data_to_Vector_DB.ipynb</code></li> <li> <p>RAG Q&amp;A:</p> </li> <li> <p><code>QnA_with_RAG.ipynb</code></p> </li> <li><code>Create_and_Deploy_QnA_AI_Service.ipynb</code></li> <li><code>Test_Queries_for_Vector_DB.ipynb</code></li> <li> <p>Evaluation:</p> </li> <li> <p><code>Analyze_Log_and_Feedback.ipynb</code></p> </li> </ul> <p>These show \"end-to-end\" RAG workflows with IBM-flavored tooling.</p>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#401-403-authentication-errors","title":"401 / 403 \u2013 Authentication errors","text":"<ul> <li> <p>Verify:</p> </li> <li> <p><code>IBM_CLOUD_API_KEY</code> is correct.</p> </li> <li>You pasted the whole key (no trailing spaces).</li> <li>You're using the correct <code>IBM_CLOUD_URL</code> for your region.</li> <li>The project ID is valid and you have access.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#project-not-found-404","title":"\"Project not found\" / 404","text":"<ul> <li>Double-check the Project ID in the watsonx.ai UI.</li> <li>Ensure you're using the right region and project/space type.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#env-not-loading","title":"<code>.env</code> not loading","text":"<ul> <li>Make sure <code>.env</code> is in the repo root (same folder as <code>Makefile</code>, <code>Dockerfile</code>).</li> <li>Ensure the notebook calls <code>load_dotenv()</code> at the top.</li> <li>If running via Docker, confirm <code>--env-file .env</code> is passed.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#jupyter-kernel-missing","title":"Jupyter kernel missing","text":"<ul> <li>Re-run:</li> </ul> <pre><code>make install\n</code></pre> <ul> <li>Restart Jupyter and select the new kernel.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#corporate-proxies","title":"Corporate proxies","text":"<ul> <li> <p>You may need to configure <code>HTTP_PROXY</code> / <code>HTTPS_PROXY</code> environment variables when:</p> </li> <li> <p>Building Docker images.</p> </li> <li>Running containers that access the internet.</li> </ul>"},{"location":"tracks/day0-env/setup-simple-watsonx-enviroment/#checklist","title":"Checklist","text":"<p>Before moving to the final Day 0 step:</p> <ul> <li>\u2705 <code>simple-watsonx-enviroment</code> cloned.</li> <li> <p>\u2705 <code>.env</code> configured with:</p> </li> <li> <p>API key</p> </li> <li>URL</li> <li>Project/space ID</li> <li>\u2705 Dependencies installed (local venv or Docker image).</li> <li>\u2705 <code>watsonx_quickstart.ipynb</code> runs and returns a Granite response.</li> <li>\u2705 You know where the <code>accelerator/</code> project is and can open its notebooks.</li> </ul> <p>Next up: we'll run a combined verification of both environments.</p>"},{"location":"tracks/day0-env/verify-environments/","title":"0.4 Verify Both Environments","text":"<p>At this point you've set up:</p> <ul> <li><code>simple-ollama-environment</code> \u2013 local LLM sandbox.</li> <li><code>simple-watsonx-enviroment</code> \u2013 watsonx.ai sandbox.</li> </ul> <p>This final Day 0 module is a sanity check to make sure everything works together, and that you're ready for Day 1.</p>"},{"location":"tracks/day0-env/verify-environments/#goal","title":"Goal","text":"<ul> <li>Confirm you can:</li> <li>Run a local model via Ollama inside a notebook.</li> <li>Run a Granite model via watsonx.ai inside a notebook.</li> <li>Confirm that:</li> <li>The <code>accelerator/</code> folder is present and notebooks open.</li> <li>The <code>labs-src/</code> reference notebooks open.</li> <li>End with a clear ready / not ready checklist.</li> </ul>"},{"location":"tracks/day0-env/verify-environments/#quick-verification-script-notebook","title":"Quick Verification Script / Notebook","text":"<p>You can create a tiny notebook (e.g. <code>verify_envs.ipynb</code>) in your main folder that does:</p> <pre><code># verify_envs.ipynb\n\nimport os\nfrom dotenv import load_dotenv\n\nprint(\"\ud83d\udd0e Verifying environments...\")\n\n# 1) Test Ollama client\ntry:\n    import ollama\n    print(\"\u2705 ollama Python package is importable\")\n\n    res = ollama.chat(\n        model=\"qwen2.5:0.5b-instruct\",  # or any model you've pulled\n        messages=[{\"role\": \"user\", \"content\": \"Say hello from Ollama.\"}],\n    )\n    print(\"Ollama says:\", res[\"message\"][\"content\"][:100], \"...\")\nexcept Exception as e:\n    print(\"\u274c Ollama check failed:\", e)\n\n# 2) Test watsonx.ai client\ntry:\n    load_dotenv()  # pick up .env from simple-watsonx-enviroment if you run this there\n    from ibm_watsonx_ai import Credentials\n    from ibm_watsonx_ai.foundation_models import ModelInference\n    from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n\n    api_key = os.getenv(\"IBM_CLOUD_API_KEY\") or os.getenv(\"WATSONX_APIKEY\")\n    url = os.getenv(\"IBM_CLOUD_URL\") or os.getenv(\"WATSONX_URL\")\n    project_id = os.getenv(\"IBM_CLOUD_PROJECT_ID\") or os.getenv(\"PROJECT_ID\")\n\n    if not api_key or not url or not project_id:\n        raise ValueError(\"Missing one of IBM_CLOUD_API_KEY / IBM_CLOUD_URL / IBM_CLOUD_PROJECT_ID\")\n\n    creds = Credentials(url=url, api_key=api_key)\n    model = ModelInference(\n        model_id=\"ibm/granite-13b-instruct-v2\",  # or another allowed Granite model\n        credentials=creds,\n        project_id=project_id,\n    )\n\n    params = {\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.MAX_NEW_TOKENS: 50,\n    }\n\n    response = model.generate_text(\n        prompt=\"Say hello from watsonx.ai in one sentence.\",\n        params=params,\n    )\n    print(\"\u2705 watsonx.ai call succeeded:\")\n    print(response)\nexcept Exception as e:\n    print(\"\u274c watsonx.ai check failed:\", e)\n</code></pre> <p>You don't have to create this combined notebook, but it's a nice, quick sanity check.</p> <p>Alternatively, you can simply:</p> <ul> <li>Run <code>ollama_quickstart.ipynb</code> in <code>simple-ollama-environment</code>.</li> <li>Run <code>watsonx_quickstart.ipynb</code> in <code>simple-watsonx-enviroment</code>.</li> </ul>"},{"location":"tracks/day0-env/verify-environments/#pair-check-exercise","title":"Pair Check Exercise","text":"<p>If you're in a classroom setting, do a quick pair verification:</p> <ol> <li>Pair up with someone next to you.</li> <li> <p>Each person shows:</p> </li> <li> <p>Jupyter running in <code>simple-ollama-environment</code>.</p> </li> <li><code>ollama_quickstart.ipynb</code> successfully returns a model response.</li> <li> <p>Then each person shows:</p> </li> <li> <p>Jupyter running in <code>simple-watsonx-enviroment</code>.</p> </li> <li><code>watsonx_quickstart.ipynb</code> successfully returns a Granite response.</li> </ol> <p>This often surfaces:</p> <ul> <li>Small typos in <code>.env</code>.</li> <li>Misconfigured paths.</li> <li>Port conflicts.</li> </ul> <p>And you get to practice explaining what you did \u2013 which already reinforces Day 1 concepts.</p>"},{"location":"tracks/day0-env/verify-environments/#confirm-accelerator-notebook-packs","title":"Confirm Accelerator &amp; Notebook Packs","text":"<p>Next, verify your project scaffolding is complete.</p>"},{"location":"tracks/day0-env/verify-environments/#check-the-accelerator-directory","title":"Check the <code>accelerator/</code> directory","text":"<p>From the <code>watsonx-workshop</code> repo root:</p> <pre><code>ls accelerator\n</code></pre> <p>You should see something like:</p> <pre><code>assets/  assettypes/  config.yaml  rag/  service/  tools/  ui/  ...\n</code></pre> <p>Try opening one of the accelerator notebooks (read-only is fine for now):</p> <ul> <li><code>accelerator/assets/notebook/notebook:Create_and_Deploy_QnA_AI_Service.ipynb</code></li> </ul> <p>Make sure:</p> <ul> <li>Jupyter loads the notebook.</li> <li>You can scroll through the cells.</li> </ul>"},{"location":"tracks/day0-env/verify-environments/#check-the-labs-src-folder","title":"Check the <code>labs-src/</code> folder","text":"<p>From the same repo:</p> <pre><code>ls labs-src\n</code></pre> <p>You should see:</p> <pre><code>README.md\nuse-watsonx-elasticsearch-and-langchain-to-answer-questions-rag.ipynb\nuse-watsonx-and-elasticsearch-python-sdk-to-answer-questions-rag.ipynb\nuse-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb\nibm-watsonx-governance-evaluation-studio-getting-started.ipynb\nibm-watsonx-governance-governed-agentic-catalog.ipynb\n</code></pre> <p>Open one as a preview, for example:</p> <ul> <li><code>use-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb</code></li> </ul> <p>We'll use these as reference implementations during Day 2\u20133.</p>"},{"location":"tracks/day0-env/verify-environments/#common-failure-modes","title":"Common Failure Modes","text":"<p>Here are some frequent issues and what to do about them.</p>"},{"location":"tracks/day0-env/verify-environments/#ollama-issues","title":"Ollama issues","text":"<ul> <li> <p>\"Connection refused\" / timeout</p> </li> <li> <p>Ensure Ollama server is running:</p> <ul> <li>In Docker: container up with port <code>11434</code> exposed.</li> <li>Local: Ollama app/service started.</li> <li>\"Model not found\"</li> </ul> </li> <li> <p>Pull the model:</p> <p><pre><code>ollama pull qwen2.5:0.5b-instruct\n</code></pre> * Out-of-memory</p> </li> <li> <p>Use smaller models (e.g., 0.5B\u20131B variants).</p> </li> </ul>"},{"location":"tracks/day0-env/verify-environments/#watsonxai-issues","title":"watsonx.ai issues","text":"<ul> <li> <p>401 / 403</p> </li> <li> <p>Check API key, URL, project ID in <code>.env</code>.</p> </li> <li> <p>Confirm you have permissions to use watsonx.ai in that project.</p> </li> <li> <p>Project not found / invalid region</p> </li> <li> <p>Double-check region and project ID from the watsonx UI.</p> </li> </ul>"},{"location":"tracks/day0-env/verify-environments/#docker-issues","title":"Docker issues","text":"<ul> <li> <p>Ports already in use</p> </li> <li> <p>Change host port mapping (<code>-p 8890:8888</code>, etc.).</p> </li> <li> <p>Cannot connect to Docker Daemon</p> </li> <li> <p>Ensure Docker Desktop or Docker Engine is running.</p> </li> <li>On Linux, check your user is in the <code>docker</code> group.</li> </ul>"},{"location":"tracks/day0-env/verify-environments/#what-to-do-if-something-fails","title":"What to Do If Something Fails","text":"<p>If you hit issues:</p> <ol> <li> <p>Capture the error</p> </li> <li> <p>Copy the error message and the command you ran.</p> </li> <li> <p>Ask for help</p> </li> <li> <p>Instructor / Slack / Teams channel.</p> </li> <li> <p>Fallback paths</p> </li> <li> <p>If local Docker or Ollama is blocked:</p> <ul> <li>You can still follow many labs in the watsonx environment.</li> <li>Or use a pre-provisioned VM / cloud notebook if your team provides one.</li> </ul> </li> </ol> <p>The key is: by the time Day 1 starts, you should at least have one working LLM path (preferably both).</p>"},{"location":"tracks/day0-env/verify-environments/#end-of-day-0-checklist","title":"End-of-Day 0 Checklist","text":"<p>Tick off each of these:</p> <ul> <li> <p>\u2705 <code>simple-ollama-environment</code>:</p> </li> <li> <p>Repo cloned.</p> </li> <li>Jupyter working.</li> <li><code>ollama_quickstart.ipynb</code> returns a model response.</li> <li> <p>\u2705 <code>simple-watsonx-enviroment</code>:</p> </li> <li> <p>Repo cloned.</p> </li> <li><code>.env</code> configured with valid IBM Cloud API key, URL, project ID.</li> <li>Jupyter working.</li> <li><code>watsonx_quickstart.ipynb</code> returns a Granite response.</li> <li> <p>\u2705 <code>accelerator/</code>:</p> </li> <li> <p>Folder present.</p> </li> <li>Notebooks under <code>accelerator/assets/notebook/</code> open.</li> <li> <p>\u2705 <code>labs-src/</code>:</p> </li> <li> <p>Notebooks open and are readable.</p> </li> <li> <p>\u2705 You can comfortably:</p> </li> <li> <p>Switch between repos.</p> </li> <li> <p>Explain (at a high level) the difference between:</p> <ul> <li>Local LLMs via Ollama.</li> <li>Hosted LLMs via watsonx.ai.</li> </ul> </li> </ul> <p>If the answer is \"yes\" to all of the above: \ud83c\udf89 Congratulations! You're ready for Day 1 \u2013 LLMs &amp; Prompting.</p>"},{"location":"tracks/day1-llm/","title":"Day 1 \u2013 LLMs &amp; Prompting - Workshop Materials","text":"<p>Welcome to Day 1 of the watsonx Workshop! This directory contains all materials for learning LLM fundamentals and prompt engineering.</p>"},{"location":"tracks/day1-llm/#quick-start","title":"Quick Start","text":"<ol> <li>Theory First (Morning, 4 hours):</li> <li>Read <code>llm-concepts.md</code></li> <li>Read <code>prompt-patterns-theory.md</code></li> <li> <p>Read <code>eval-safety-theory.md</code></p> </li> <li> <p>Labs Second (Afternoon, 4 hours):</p> </li> <li>Follow <code>lab-1-quickstart-two-envs.md</code></li> <li>Follow <code>lab-2-prompt-templates.md</code></li> <li> <p>Follow <code>lab-3-micro-eval.md</code></p> </li> <li> <p>Reference:</p> </li> <li>See <code>day1-summary-and-schedule.md</code> for complete overview</li> </ol>"},{"location":"tracks/day1-llm/#file-structure","title":"File Structure","text":"<pre><code>day1-llm/\n\u251c\u2500\u2500 README.md (this file)\n\u251c\u2500\u2500 day1-summary-and-schedule.md\n\u2502\n\u251c\u2500\u2500 Theory (Morning)\n\u2502   \u251c\u2500\u2500 llm-concepts.md\n\u2502   \u251c\u2500\u2500 prompt-patterns-theory.md\n\u2502   \u2514\u2500\u2500 eval-safety-theory.md\n\u2502\n\u251c\u2500\u2500 Lab Instructions (Afternoon)\n\u2502   \u251c\u2500\u2500 lab-1-quickstart-two-envs.md\n\u2502   \u251c\u2500\u2500 lab-2-prompt-templates.md\n\u2502   \u2514\u2500\u2500 lab-3-micro-eval.md\n\u2502\n\u2514\u2500\u2500 Notebooks (Created by you during labs)\n    \u251c\u2500\u2500 ollama_quickstart.ipynb\n    \u251c\u2500\u2500 watsonx_quickstart.ipynb\n    \u251c\u2500\u2500 prompt_patterns_ollama.ipynb\n    \u251c\u2500\u2500 prompt_patterns_watsonx.ipynb\n    \u2514\u2500\u2500 micro_evaluation.ipynb\n</code></pre>"},{"location":"tracks/day1-llm/#learning-objectives","title":"Learning Objectives","text":""},{"location":"tracks/day1-llm/#theory-modules","title":"Theory Modules","text":"<p>1.0 LLM Concepts (<code>llm-concepts.md</code>) - Understand tokens, context windows, and key parameters - Compare local (Ollama) vs managed (watsonx.ai) deployments - Learn cost and resource considerations - See how LLMs fit in production architecture</p> <p>1.2 Prompt Patterns (<code>prompt-patterns-theory.md</code>) - Master common prompt patterns (instruction, few-shot, CoT, style transfer) - Learn prompt design principles - Create reusable templates - Understand accelerator prompt structure</p> <p>1.3 Evaluation &amp; Safety (<code>eval-safety-theory.md</code>) - Know why evaluation matters - Understand evaluation signals (correctness, coherence, style, latency) - Learn safety considerations - Design production monitoring</p>"},{"location":"tracks/day1-llm/#lab-modules","title":"Lab Modules","text":"<p>Lab 1.1: Quickstart (<code>lab-1-quickstart-two-envs.md</code>) - Duration: 45 minutes - Run first prompts in Ollama and watsonx.ai - Modify parameters (temperature, max_tokens) - Compare outputs and latency</p> <p>Lab 1.2: Prompt Templates (<code>lab-2-prompt-templates.md</code>) - Duration: 60 minutes - Build reusable templates for summarization, style transfer, Q&amp;A - Implement in both environments - Run comparative experiments</p> <p>Lab 1.3: Micro-Evaluation (<code>lab-3-micro-eval.md</code>) - Duration: 60 minutes - Create test set of prompts - Apply rating rubric - Analyze results with pandas and visualizations</p>"},{"location":"tracks/day1-llm/#prerequisites","title":"Prerequisites","text":"<p>Before Day 1: - \u2705 Complete Day 0 (environment setup) - \u2705 <code>simple-ollama-environment</code> working - \u2705 <code>simple-watsonx-enviroment</code> with valid credentials - \u2705 Jupyter accessible in both environments</p>"},{"location":"tracks/day1-llm/#workshop-flow","title":"Workshop Flow","text":"<pre><code>Morning (Theory)\n\u251c\u2500\u2500 9:00-10:30   \u2502 1.0 LLM Concepts\n\u251c\u2500\u2500 10:30-10:45  \u2502 Break\n\u251c\u2500\u2500 10:45-11:45  \u2502 1.2 Prompt Patterns\n\u251c\u2500\u2500 11:45-12:00  \u2502 Q&amp;A\n\u2514\u2500\u2500 12:00-13:00  \u2502 Lunch\n\nAfternoon (Labs)\n\u251c\u2500\u2500 13:00-13:45  \u2502 Lab 1.1: Quickstart\n\u251c\u2500\u2500 13:45-14:45  \u2502 Lab 1.2: Templates\n\u251c\u2500\u2500 14:45-15:00  \u2502 Break\n\u251c\u2500\u2500 15:00-16:00  \u2502 Lab 1.3: Evaluation\n\u251c\u2500\u2500 16:00-16:30  \u2502 1.3 Evaluation Theory\n\u2514\u2500\u2500 16:30-17:00  \u2502 Wrap-up &amp; Q&amp;A\n</code></pre>"},{"location":"tracks/day1-llm/#key-concepts","title":"Key Concepts","text":""},{"location":"tracks/day1-llm/#tokens","title":"Tokens","text":"<ul> <li>Sub-units of text (~4 chars/token in English)</li> <li>Models have token limits (context windows)</li> <li>Costs calculated per token</li> </ul>"},{"location":"tracks/day1-llm/#temperature","title":"Temperature","text":"<ul> <li><code>0.0</code> = Deterministic, focused</li> <li><code>0.7-1.0</code> = Balanced creativity</li> <li><code>1.5+</code> = Very creative, less predictable</li> </ul>"},{"location":"tracks/day1-llm/#prompt-patterns","title":"Prompt Patterns","text":"<ol> <li>Instruction: Direct command</li> <li>Few-shot: Examples before task</li> <li>Chain-of-thought: Step-by-step reasoning</li> <li>Style transfer: Rewrite in different tone</li> <li>Summarization: Condense content</li> </ol>"},{"location":"tracks/day1-llm/#evaluation","title":"Evaluation","text":"<ul> <li>Correctness: Matches ground truth?</li> <li>Coherence: Logical and relevant?</li> <li>Style: Follows format?</li> <li>Latency: Fast enough?</li> </ul>"},{"location":"tracks/day1-llm/#deliverables","title":"Deliverables","text":"<p>By end of Day 1, you will have: - \u2705 5 working Jupyter notebooks - \u2705 Reusable prompt templates - \u2705 Evaluation results (CSV + visualizations) - \u2705 Understanding of LLM fundamentals</p>"},{"location":"tracks/day1-llm/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":""},{"location":"tracks/day1-llm/#ollama","title":"Ollama","text":"<p>Problem: Connection refused Solution: Check if Ollama service is running <pre><code>curl http://localhost:11434/api/tags\n</code></pre></p> <p>Problem: Model not found Solution: Pull the model <pre><code>ollama pull qwen2.5:0.5b-instruct\n</code></pre></p>"},{"location":"tracks/day1-llm/#watsonxai","title":"watsonx.ai","text":"<p>Problem: Invalid API key Solution: Check <code>.env</code> file, verify key in IBM Cloud console</p> <p>Problem: Rate limit Solution: Add delays between requests (<code>time.sleep(0.5)</code>)</p>"},{"location":"tracks/day1-llm/#general","title":"General","text":"<p>Problem: Wrong Python kernel Solution: Select correct kernel in Jupyter (e.g., \"Python 3.11 (simple-env)\")</p>"},{"location":"tracks/day1-llm/#tips-for-success","title":"Tips for Success","text":""},{"location":"tracks/day1-llm/#theory-sessions","title":"Theory Sessions","text":"<ul> <li>Take notes on key concepts</li> <li>Ask questions when unclear</li> <li>Relate concepts to your use cases</li> </ul>"},{"location":"tracks/day1-llm/#lab-sessions","title":"Lab Sessions","text":"<ul> <li>Follow instructions step-by-step</li> <li>Experiment beyond the examples</li> <li>Document interesting findings</li> <li>Help peers when possible</li> </ul>"},{"location":"tracks/day1-llm/#time-management","title":"Time Management","text":"<ul> <li>Don't get stuck on one issue too long</li> <li>Ask for help after 5-10 minutes</li> <li>Labs build on each other\u2014complete in order</li> </ul>"},{"location":"tracks/day1-llm/#resources","title":"Resources","text":""},{"location":"tracks/day1-llm/#documentation","title":"Documentation","text":"<ul> <li>IBM Granite Models</li> <li>watsonx.ai Docs</li> <li>Ollama Documentation</li> </ul>"},{"location":"tracks/day1-llm/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>OpenAI Prompt Guide</li> <li>Anthropic Prompt Tips</li> <li>Granite Prompting</li> </ul>"},{"location":"tracks/day1-llm/#python-data-science","title":"Python &amp; Data Science","text":"<ul> <li>Pandas Documentation</li> <li>Jupyter Notebook Guide</li> </ul>"},{"location":"tracks/day1-llm/#next-steps","title":"Next Steps","text":""},{"location":"tracks/day1-llm/#after-day-1","title":"After Day 1","text":"<ol> <li>Review your notebooks and notes</li> <li>Complete optional homework (expand test sets)</li> <li>Read ahead: Day 2 materials on RAG</li> </ol>"},{"location":"tracks/day1-llm/#day-2-preview","title":"Day 2 Preview","text":"<p>Tomorrow we'll learn: - Retrieval-Augmented Generation (RAG) - Vector databases (Elasticsearch, Chroma) - Embedding models - Accelerator integration</p>"},{"location":"tracks/day1-llm/#getting-help","title":"Getting Help","text":""},{"location":"tracks/day1-llm/#during-workshop","title":"During Workshop","text":"<ul> <li>\ud83d\udcac Chat/Slack for questions</li> <li>\ud83d\ude4b Raise hand for blocking issues</li> <li>\ud83d\udc65 Discuss with neighbors</li> </ul>"},{"location":"tracks/day1-llm/#after-workshop","title":"After Workshop","text":"<ul> <li>\ud83d\udce7 Email instructors</li> <li>\ud83d\udcbb GitHub issues/discussions</li> <li>\ud83c\udf10 Community forums</li> </ul>"},{"location":"tracks/day1-llm/#feedback","title":"Feedback","text":"<p>Your feedback helps improve the workshop!</p> <p>Please share: - What worked well? - What was confusing? - What would you like more/less of? - Suggestions for improvement?</p>"},{"location":"tracks/day1-llm/#license-attribution","title":"License &amp; Attribution","text":"<p>Materials created for the watsonx Workshop Series.</p> <p>Based on: - IBM Granite documentation - watsonx.ai best practices - Community contributions</p>"},{"location":"tracks/day1-llm/#version-history","title":"Version History","text":"<ul> <li>v1.0 (2025-01): Initial release</li> <li>Theory modules</li> <li>Lab instructions</li> <li>Reference notebooks</li> </ul> <p>Ready to start? Begin with <code>llm-concepts.md</code>! \ud83d\ude80</p> <p>For questions or issues, contact your workshop instructor.</p>"},{"location":"tracks/day1-llm/MANIFEST/","title":"Day 1 Workshop Materials - Complete Manifest","text":""},{"location":"tracks/day1-llm/MANIFEST/#generated-files-overview","title":"Generated Files Overview","text":"<p>This document lists all files generated for the Day 1 \u2013 LLMs &amp; Prompting workshop, along with their purpose and usage instructions.</p>"},{"location":"tracks/day1-llm/MANIFEST/#core-documentation-files","title":"Core Documentation Files","text":""},{"location":"tracks/day1-llm/MANIFEST/#1-theory-materials-morning-session-4-hours","title":"1. Theory Materials (Morning Session - 4 hours)","text":""},{"location":"tracks/day1-llm/MANIFEST/#llm-conceptsmd","title":"<code>llm-concepts.md</code>","text":"<ul> <li>Purpose: Core LLM concepts and architecture</li> <li>Duration: ~75 minutes of content</li> <li>Topics:</li> <li>What is an LLM?</li> <li>Tokens, context windows, parameters</li> <li>Local vs managed deployments</li> <li>Cost considerations</li> <li>Accelerator architecture</li> <li>Usage: Read first in morning session</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#prompt-patterns-theorymd","title":"<code>prompt-patterns-theory.md</code>","text":"<ul> <li>Purpose: Prompt engineering patterns and best practices</li> <li>Duration: ~60 minutes of content</li> <li>Topics:</li> <li>Core prompt patterns (instruction, few-shot, CoT, style transfer)</li> <li>Prompt design principles</li> <li>Template creation</li> <li>Accelerator prompt structure</li> <li>Usage: Read after LLM concepts</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#eval-safety-theorymd","title":"<code>eval-safety-theory.md</code>","text":"<ul> <li>Purpose: Evaluation and safety considerations</li> <li>Duration: ~30 minutes of content</li> <li>Topics:</li> <li>Why evaluation matters</li> <li>Evaluation signals (correctness, coherence, style, latency)</li> <li>Safety and responsible AI</li> <li>Production monitoring</li> <li>Usage: Read at end of day or before Lab 1.3</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#2-lab-instructions-afternoon-session-4-hours","title":"2. Lab Instructions (Afternoon Session - 4 hours)","text":""},{"location":"tracks/day1-llm/MANIFEST/#lab-1-quickstart-two-envsmd","title":"<code>lab-1-quickstart-two-envs.md</code>","text":"<ul> <li>Purpose: Hands-on introduction to both environments</li> <li>Duration: 45 minutes</li> <li>Prerequisites: Day 0 completed</li> <li>Deliverables:</li> <li>Working notebooks in Ollama and watsonx</li> <li>Understanding of parameters (temperature, max_tokens)</li> <li>Comparison of outputs</li> <li>Usage: First afternoon lab</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#lab-2-prompt-templatesmd","title":"<code>lab-2-prompt-templates.md</code>","text":"<ul> <li>Purpose: Build reusable prompt templates</li> <li>Duration: 60 minutes</li> <li>Prerequisites: Lab 1.1 completed</li> <li>Deliverables:</li> <li><code>prompt_patterns_ollama.ipynb</code></li> <li><code>prompt_patterns_watsonx.ipynb</code></li> <li>Comparative analysis</li> <li>Usage: Second afternoon lab</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#lab-3-micro-evalmd","title":"<code>lab-3-micro-eval.md</code>","text":"<ul> <li>Purpose: Systematic evaluation framework</li> <li>Duration: 60 minutes</li> <li>Prerequisites: Labs 1.1 and 1.2 completed</li> <li>Deliverables:</li> <li><code>micro_evaluation.ipynb</code></li> <li>Evaluation results CSV</li> <li>Summary statistics and visualizations</li> <li>Usage: Final afternoon lab</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#3-supporting-documents","title":"3. Supporting Documents","text":""},{"location":"tracks/day1-llm/MANIFEST/#day1-summary-and-schedulemd","title":"<code>day1-summary-and-schedule.md</code>","text":"<ul> <li>Purpose: Complete workshop overview and schedule</li> <li>Contents:</li> <li>Detailed schedule with times</li> <li>Learning objectives by module</li> <li>Success criteria</li> <li>Instructor notes</li> <li>Connections to future days</li> <li>Usage: Reference for instructors and students</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#readmemd","title":"<code>README.md</code>","text":"<ul> <li>Purpose: Quick start guide and navigation</li> <li>Contents:</li> <li>File structure</li> <li>Quick start instructions</li> <li>Common issues and solutions</li> <li>Resources and links</li> <li>Usage: First file students should read</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#file-organization-for-mkdocs","title":"File Organization for MkDocs","text":""},{"location":"tracks/day1-llm/MANIFEST/#recommended-directory-structure","title":"Recommended Directory Structure","text":"<pre><code>docs/\n\u2514\u2500\u2500 tracks/\n    \u2514\u2500\u2500 day1-llm/\n        \u251c\u2500\u2500 README.md\n        \u251c\u2500\u2500 day1-summary-and-schedule.md\n        \u2502\n        \u251c\u2500\u2500 theory/\n        \u2502   \u251c\u2500\u2500 llm-concepts.md\n        \u2502   \u251c\u2500\u2500 prompt-patterns-theory.md\n        \u2502   \u2514\u2500\u2500 eval-safety-theory.md\n        \u2502\n        \u2514\u2500\u2500 labs/\n            \u251c\u2500\u2500 lab-1-quickstart-two-envs.md\n            \u251c\u2500\u2500 lab-2-prompt-templates.md\n            \u2514\u2500\u2500 lab-3-micro-eval.md\n</code></pre>"},{"location":"tracks/day1-llm/MANIFEST/#mkdocs-configuration","title":"MkDocs Configuration","text":"<p>Add to your <code>mkdocs.yml</code>:</p> <pre><code>nav:\n  - Home: index.md\n  - Day 0 - Environment Setup:\n      - Prerequisites: tracks/day0-env/prereqs-and-accounts.md\n      - Setup Ollama: tracks/day0-env/setup-simple-ollama-environment.md\n      - Setup watsonx: tracks/day0-env/setup-simple-watsonx-enviroment.md\n      - Verify: tracks/day0-env/verify-environments.md\n  - Day 1 - LLMs &amp; Prompting:\n      - Overview: tracks/day1-llm/README.md\n      - Schedule: tracks/day1-llm/day1-summary-and-schedule.md\n      - Theory:\n          - LLM Concepts: tracks/day1-llm/theory/llm-concepts.md\n          - Prompt Patterns: tracks/day1-llm/theory/prompt-patterns-theory.md\n          - Evaluation &amp; Safety: tracks/day1-llm/theory/eval-safety-theory.md\n      - Labs:\n          - Lab 1.1 Quickstart: tracks/day1-llm/labs/lab-1-quickstart-two-envs.md\n          - Lab 1.2 Templates: tracks/day1-llm/labs/lab-2-prompt-templates.md\n          - Lab 1.3 Evaluation: tracks/day1-llm/labs/lab-3-micro-eval.md\n</code></pre>"},{"location":"tracks/day1-llm/MANIFEST/#how-to-use-these-materials","title":"How to Use These Materials","text":""},{"location":"tracks/day1-llm/MANIFEST/#for-instructors","title":"For Instructors","text":"<p>Preparation: 1. Review all theory materials 2. Test all lab exercises in both environments 3. Prepare backup notebooks for common issues 4. Review <code>day1-summary-and-schedule.md</code> for timing and tips</p> <p>During Workshop: 1. Follow schedule in <code>day1-summary-and-schedule.md</code> 2. Use theory docs as presentation guides 3. Monitor student progress in labs 4. Address issues using solutions in lab docs</p> <p>After Workshop: 1. Collect feedback 2. Share solutions 3. Provide additional resources</p>"},{"location":"tracks/day1-llm/MANIFEST/#for-students","title":"For Students","text":"<p>Before Day 1: 1. Complete Day 0 setup 2. Verify both environments work 3. Read <code>README.md</code> for overview</p> <p>During Day 1: 1. Follow morning theory sessions 2. Take notes on key concepts 3. Complete labs in order 4. Ask questions when stuck</p> <p>After Day 1: 1. Review notebooks and notes 2. Complete optional homework 3. Prepare for Day 2 (RAG)</p>"},{"location":"tracks/day1-llm/MANIFEST/#for-self-paced-learning","title":"For Self-Paced Learning","text":"<ol> <li>Start with <code>README.md</code></li> <li>Read theory docs in order</li> <li>Complete labs at your own pace</li> <li>Join community for questions</li> <li>Share your results</li> </ol>"},{"location":"tracks/day1-llm/MANIFEST/#content-coverage","title":"Content Coverage","text":""},{"location":"tracks/day1-llm/MANIFEST/#theory-estimated-25-3-hours-reading","title":"Theory (Estimated 2.5-3 hours reading)","text":"File Topics Word Count Reading Time <code>llm-concepts.md</code> LLM fundamentals ~4000 words 60-75 min <code>prompt-patterns-theory.md</code> Prompt engineering ~3500 words 45-60 min <code>eval-safety-theory.md</code> Evaluation &amp; safety ~3000 words 30-45 min"},{"location":"tracks/day1-llm/MANIFEST/#labs-estimated-3-hours-hands-on","title":"Labs (Estimated 3 hours hands-on)","text":"File Focus Duration Deliverables <code>lab-1-quickstart-two-envs.md</code> Basic LLM usage 45 min 2 notebooks <code>lab-2-prompt-templates.md</code> Template creation 60 min 2 notebooks <code>lab-3-micro-eval.md</code> Evaluation 60 min 1 notebook + CSV"},{"location":"tracks/day1-llm/MANIFEST/#key-features","title":"Key Features","text":""},{"location":"tracks/day1-llm/MANIFEST/#comprehensive-coverage","title":"Comprehensive Coverage","text":"<ul> <li>\u2705 Complete theory for 4-hour morning session</li> <li>\u2705 Detailed lab instructions for 4-hour afternoon session</li> <li>\u2705 Step-by-step guidance with code examples</li> <li>\u2705 Troubleshooting sections for common issues</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#progressive-learning","title":"Progressive Learning","text":"<ul> <li>\u2705 Builds from basics to advanced concepts</li> <li>\u2705 Each lab builds on previous labs</li> <li>\u2705 Clear connections to production (accelerator)</li> <li>\u2705 Prepares for Day 2 (RAG)</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#practical-focus","title":"Practical Focus","text":"<ul> <li>\u2705 Real code examples that students can run</li> <li>\u2705 Comparative analysis (Ollama vs watsonx)</li> <li>\u2705 Reusable templates and patterns</li> <li>\u2705 Production-ready practices</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#well-structured","title":"Well-Structured","text":"<ul> <li>\u2705 Clear learning objectives for each module</li> <li>\u2705 Checkpoints to verify progress</li> <li>\u2705 Summary and wrap-up sections</li> <li>\u2705 Links to additional resources</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#customization","title":"Customization","text":""},{"location":"tracks/day1-llm/MANIFEST/#adapting-for-different-audiences","title":"Adapting for Different Audiences","text":"<p>For Beginners: - Extend theory sessions - Add more examples in labs - Provide pre-written notebook templates - Reduce optional sections</p> <p>For Advanced Users: - Condense theory - Add advanced challenges in labs - Include additional models/backends - Focus on production integration</p> <p>For Corporate Training: - Add company-specific use cases - Include governance requirements - Emphasize watsonx.ai features - Connect to enterprise workflows</p>"},{"location":"tracks/day1-llm/MANIFEST/#quality-assurance","title":"Quality Assurance","text":""},{"location":"tracks/day1-llm/MANIFEST/#verification-checklist","title":"Verification Checklist","text":"<p>Theory Materials: - \u2705 Technically accurate - \u2705 Clear explanations - \u2705 Good examples - \u2705 Appropriate depth</p> <p>Lab Materials: - \u2705 Tested in both environments - \u2705 Clear step-by-step instructions - \u2705 Code examples work - \u2705 Troubleshooting covers common issues</p> <p>Documentation: - \u2705 Well-organized - \u2705 Easy to navigate - \u2705 Compatible with MkDocs - \u2705 Links work</p>"},{"location":"tracks/day1-llm/MANIFEST/#maintenance","title":"Maintenance","text":""},{"location":"tracks/day1-llm/MANIFEST/#regular-updates-needed","title":"Regular Updates Needed","text":"<p>Quarterly: - Update model versions (e.g., new Granite releases) - Refresh watsonx.ai screenshots/examples - Add new community resources</p> <p>After Each Workshop: - Incorporate student feedback - Fix identified issues - Add commonly asked questions</p> <p>As Technology Evolves: - New prompt patterns - Updated best practices - New evaluation techniques</p>"},{"location":"tracks/day1-llm/MANIFEST/#success-metrics","title":"Success Metrics","text":""},{"location":"tracks/day1-llm/MANIFEST/#student-outcomes","title":"Student Outcomes","text":"<p>By end of Day 1, students should: - \u2705 Score 80%+ on concept quiz - \u2705 Complete all 3 labs - \u2705 Have 5 working notebooks - \u2705 Understand prompt engineering basics - \u2705 Ready for Day 2</p>"},{"location":"tracks/day1-llm/MANIFEST/#workshop-quality","title":"Workshop Quality","text":"<ul> <li>\u2705 90%+ completion rate</li> <li>\u2705 4.5+/5 average rating</li> <li>\u2705 &lt;10% technical issues</li> <li>\u2705 High engagement scores</li> </ul>"},{"location":"tracks/day1-llm/MANIFEST/#support","title":"Support","text":""},{"location":"tracks/day1-llm/MANIFEST/#for-issues","title":"For Issues","text":"<p>Documentation Issues: - Report unclear sections - Suggest improvements - Submit corrections</p> <p>Technical Issues: - Check troubleshooting sections - Ask instructor/community - Document for future reference</p> <p>Content Requests: - Additional examples - Deeper dives on topics - New lab exercises</p>"},{"location":"tracks/day1-llm/MANIFEST/#license-credits","title":"License &amp; Credits","text":"<p>Created For: watsonx Workshop Series Version: 1.0 (January 2025) Format: Markdown (MkDocs compatible)</p> <p>Based On: - IBM Granite documentation - watsonx.ai best practices - LLM prompt engineering research - Community feedback and contributions</p>"},{"location":"tracks/day1-llm/MANIFEST/#file-download-links","title":"File Download Links","text":"<p>All files are available in <code>/mnt/user-data/outputs/</code>:</p> <ol> <li>README.md</li> <li>day1-summary-and-schedule.md</li> <li>llm-concepts.md</li> <li>prompt-patterns-theory.md</li> <li>eval-safety-theory.md</li> <li>lab-1-quickstart-two-envs.md</li> <li>lab-2-prompt-templates.md</li> <li>lab-3-micro-eval.md</li> </ol> <p>All materials ready for download and deployment! \ud83c\udf89</p>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/","title":"Day 1 \u2013 LLMs &amp; Prompting - Complete Workshop Guide","text":"<p>Date: Day 1 of watsonx Workshop Duration: 8 hours (4 hours theory + 4 hours labs) Track: Core/Granite</p>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#workshop-schedule","title":"Workshop Schedule","text":""},{"location":"tracks/day1-llm/day1-summary-and-schedule/#morning-session-4-hours-theory","title":"Morning Session (4 hours) - Theory","text":"Time Duration Topic Description 9:00 - 9:15 15 min Welcome &amp; Setup Check Verify Day 0 completion 9:15 - 10:30 75 min 1.0 LLM Concepts Core concepts, local vs managed, architecture 10:30 - 10:45 15 min Break 10:45 - 11:45 60 min 1.2 Prompt Patterns Patterns, templates, best practices 11:45 - 12:00 15 min Q&amp;A 12:00 - 1:00 60 min Lunch"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#afternoon-session-4-hours-labs","title":"Afternoon Session (4 hours) - Labs","text":"Time Duration Topic Description 1:00 - 1:45 45 min Lab 1.1 Quickstart in both environments 1:45 - 2:45 60 min Lab 1.2 Prompt templates 2:45 - 3:00 15 min Break 3:00 - 4:00 60 min Lab 1.3 Micro-evaluation exercise 4:00 - 4:30 30 min 1.3 Evaluation Theory Safety, evaluation concepts 4:30 - 5:00 30 min Day 1 Wrap-up Q&amp;A, prepare for Day 2"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#learning-path","title":"Learning Path","text":"<pre><code>Day 0 (Prerequisites)\n    \u2193\nDay 1 Morning: Theory\n    \u2192 1.0 LLM Concepts\n    \u2192 1.2 Prompt Patterns\n    \u2193\nDay 1 Afternoon: Labs\n    \u2192 Lab 1.1: Quickstart\n    \u2192 Lab 1.2: Templates\n    \u2192 Lab 1.3: Evaluation\n    \u2193\nDay 2: RAG (Retrieval-Augmented Generation)\n</code></pre>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#materials-provided","title":"Materials Provided","text":""},{"location":"tracks/day1-llm/day1-summary-and-schedule/#theory-documents","title":"Theory Documents","text":"<ol> <li><code>llm-concepts.md</code> - Core LLM concepts and architecture</li> <li><code>prompt-patterns-theory.md</code> - Prompt engineering patterns</li> <li><code>eval-safety-theory.md</code> - Evaluation and safety</li> </ol>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#lab-instructions","title":"Lab Instructions","text":"<ol> <li><code>lab-1-quickstart-two-envs.md</code> - Lab 1.1 guide</li> <li><code>lab-2-prompt-templates.md</code> - Lab 1.2 guide</li> <li><code>lab-3-micro-eval.md</code> - Lab 1.3 guide</li> </ol>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#notebooks-to-be-created-by-participants","title":"Notebooks (to be created by participants)","text":"<ol> <li><code>ollama_quickstart.ipynb</code> - Ollama experiments</li> <li><code>watsonx_quickstart.ipynb</code> - watsonx.ai experiments</li> <li><code>prompt_patterns_ollama.ipynb</code> - Ollama templates</li> <li><code>prompt_patterns_watsonx.ipynb</code> - watsonx templates</li> <li><code>micro_evaluation.ipynb</code> - Evaluation framework</li> </ol>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#reference-materials","title":"Reference Materials","text":"<ul> <li><code>labs-src/</code> - Reference RAG notebooks</li> <li><code>accelerator/</code> - Production code structure</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#learning-objectives-by-module","title":"Learning Objectives by Module","text":""},{"location":"tracks/day1-llm/day1-summary-and-schedule/#10-llm-concepts","title":"1.0 LLM Concepts","text":"<ul> <li>\u2705 Understand tokens, context windows, parameters</li> <li>\u2705 Compare local vs managed deployments</li> <li>\u2705 Know cost and resource considerations</li> <li>\u2705 Understand accelerator architecture</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#12-prompt-patterns","title":"1.2 Prompt Patterns","text":"<ul> <li>\u2705 Recognize common prompt patterns</li> <li>\u2705 Build reusable templates</li> <li>\u2705 Apply prompt engineering principles</li> <li>\u2705 Design production prompts</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#lab-11-quickstart","title":"Lab 1.1: Quickstart","text":"<ul> <li>\u2705 Run prompts in Ollama and watsonx</li> <li>\u2705 Modify parameters (temperature, max_tokens)</li> <li>\u2705 Compare outputs and latency</li> <li>\u2705 Connect to accelerator</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#lab-12-templates","title":"Lab 1.2: Templates","text":"<ul> <li>\u2705 Create reusable templates</li> <li>\u2705 Implement across backends</li> <li>\u2705 Run comparative experiments</li> <li>\u2705 Plan accelerator integration</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#13-evaluation-safety","title":"1.3 Evaluation &amp; Safety","text":"<ul> <li>\u2705 Understand evaluation importance</li> <li>\u2705 Know basic evaluation signals</li> <li>\u2705 Recognize safety considerations</li> <li>\u2705 Plan production monitoring</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#lab-13-evaluation","title":"Lab 1.3: Evaluation","text":"<ul> <li>\u2705 Build test sets</li> <li>\u2705 Apply rating rubrics</li> <li>\u2705 Analyze results systematically</li> <li>\u2705 Design production logging schema</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#prerequisites-checklist","title":"Prerequisites Checklist","text":"<p>Before starting Day 1, ensure:</p> <ul> <li>\u2705 Day 0 completed</li> <li>\u2705 <code>simple-ollama-environment</code> working</li> <li>\u2705 <code>simple-watsonx-enviroment</code> working with credentials</li> <li>\u2705 Jupyter accessible in both environments</li> <li>\u2705 Ollama has at least one model pulled (e.g., <code>qwen2.5:0.5b-instruct</code>)</li> <li>\u2705 watsonx.ai credentials verified (API key, URL, project ID)</li> <li>\u2705 <code>watsonx-workshop</code> repo cloned (for accelerator reference)</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#key-concepts-summary","title":"Key Concepts Summary","text":""},{"location":"tracks/day1-llm/day1-summary-and-schedule/#tokens","title":"Tokens","text":"<ul> <li>Sub-units of text that LLMs process</li> <li>~4 characters per token (English average)</li> <li>Context window = max tokens (input + output)</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#temperature","title":"Temperature","text":"<ul> <li>0.0 = Deterministic, focused</li> <li>0.7-1.0 = Balanced</li> <li>1.5+ = Creative, unpredictable</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#prompt-patterns","title":"Prompt Patterns","text":"<ol> <li>Instruction: Direct command</li> <li>Few-shot: Examples before task</li> <li>Chain-of-thought: Step-by-step reasoning</li> <li>Style transfer: Rewrite in different tone</li> <li>Summarization: Condense content</li> </ol>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#evaluation-signals","title":"Evaluation Signals","text":"<ol> <li>Correctness: Matches ground truth?</li> <li>Coherence: Logical and relevant?</li> <li>Style: Follows format?</li> <li>Completeness: Addresses all parts?</li> <li>Latency: Fast enough?</li> </ol>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#instructor-notes","title":"Instructor Notes","text":""},{"location":"tracks/day1-llm/day1-summary-and-schedule/#morning-session-tips","title":"Morning Session Tips","text":"<ul> <li>LLM Concepts: Use diagrams for architecture</li> <li>Prompt Patterns: Live demo with watsonx Prompt Lab</li> <li>Keep theory interactive with questions</li> <li>Relate concepts to students' use cases</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#afternoon-session-tips","title":"Afternoon Session Tips","text":"<ul> <li>Lab 1.1: Ensure all students complete before moving on</li> <li>Lab 1.2: Encourage creativity in template design</li> <li>Lab 1.3: Form small groups for evaluation discussions</li> <li>Circulate during labs to answer questions</li> <li>Have backup notebooks ready for students with issues</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#common-issues","title":"Common Issues","text":"<ol> <li>Ollama not running: Check Docker container or service</li> <li>watsonx 401 errors: Verify credentials in <code>.env</code></li> <li>Rate limits: Remind students to pace requests</li> <li>Python environment: Ensure correct kernel selected</li> </ol>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#success-criteria","title":"Success Criteria","text":"<p>By end of Day 1, students should be able to:</p> <ol> <li>Explain how LLMs work at a high level</li> <li>Compare local and managed LLM deployments</li> <li>Write effective prompts for different tasks</li> <li>Build reusable prompt templates in Python</li> <li>Evaluate LLM outputs systematically</li> <li>Run notebooks in both Ollama and watsonx environments</li> <li>Understand how LLMs fit into the accelerator architecture</li> </ol>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#homework-optional","title":"Homework (Optional)","text":"<ol> <li>Expand test set: Add 10 more diverse prompts to Lab 1.3</li> <li>Try different models: </li> <li>Ollama: <code>llama3.2:3b</code>, <code>qwen2.5:1.5b</code></li> <li>watsonx: Try different Granite variants</li> <li>Advanced prompting: Implement a multi-turn conversation pattern</li> <li>Read ahead: Review Day 2 materials on RAG concepts</li> </ol>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#connections-to-future-days","title":"Connections to Future Days","text":""},{"location":"tracks/day1-llm/day1-summary-and-schedule/#day-2-rag","title":"Day 2 (RAG)","text":"<ul> <li>Today's prompts \u2192 prompts with retrieved context</li> <li>Single LLM call \u2192 retrieval + LLM pipeline</li> <li>Manual evaluation \u2192 automated RAG metrics (retrieval quality, answer quality)</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#day-3-agents-orchestration","title":"Day 3 (Agents &amp; Orchestration)","text":"<ul> <li>Static prompts \u2192 dynamic tool-calling prompts</li> <li>Single-turn \u2192 multi-turn conversations</li> <li>Basic evaluation \u2192 production monitoring</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#resources","title":"Resources","text":""},{"location":"tracks/day1-llm/day1-summary-and-schedule/#documentation","title":"Documentation","text":"<ul> <li>IBM Granite Models</li> <li>watsonx.ai Docs</li> <li>Ollama Docs</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>OpenAI Prompt Guide</li> <li>Granite Prompting Guide</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#community","title":"Community","text":"<ul> <li>IBM Granite GitHub</li> <li>watsonx Community</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#feedback-questions","title":"Feedback &amp; Questions","text":""},{"location":"tracks/day1-llm/day1-summary-and-schedule/#during-workshop","title":"During Workshop","text":"<ul> <li>Use chat/Slack for quick questions</li> <li>Raise hand for blocking issues</li> <li>Share interesting findings with the group</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#after-workshop","title":"After Workshop","text":"<ul> <li>Complete feedback survey</li> <li>Share lab solutions with peers</li> <li>Join community discussions</li> </ul>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#day-1-completion-checklist","title":"Day 1 Completion Checklist","text":"<p>Theory: - \u2705 Attended 1.0 LLM Concepts session - \u2705 Attended 1.2 Prompt Patterns session - \u2705 Attended 1.3 Evaluation &amp; Safety session</p> <p>Labs: - \u2705 Completed Lab 1.1 (Quickstart) - \u2705 Completed Lab 1.2 (Templates) - \u2705 Completed Lab 1.3 (Evaluation)</p> <p>Deliverables: - \u2705 Working notebooks in both environments - \u2705 Prompt templates created - \u2705 Evaluation results CSV generated</p> <p>Understanding: - \u2705 Can explain LLM concepts - \u2705 Can write effective prompts - \u2705 Can evaluate LLM outputs - \u2705 Ready for Day 2 (RAG)</p>"},{"location":"tracks/day1-llm/day1-summary-and-schedule/#next-day-2-preview","title":"Next: Day 2 Preview","text":"<p>Tomorrow we'll: 1. Add retrieval to our LLM calls (RAG) 2. Integrate with the accelerator codebase 3. Build a production-ready RAG service 4. Learn about vector databases and embeddings</p> <p>Prepare by: - Reviewing today's materials - Ensuring accelerator code is accessible - Thinking about documents you'd like to use for RAG</p> <p>Congratulations on completing Day 1! \ud83c\udf89</p> <p>You've built a strong foundation in LLM fundamentals and prompt engineering. Tomorrow, we'll take it to the next level with RAG.</p>"},{"location":"tracks/day1-llm/eval-safety-theory/","title":"1.3 Lightweight Evaluation &amp; Safety","text":"<p>As LLMs become central to your applications, you need ways to measure their performance and ensure they behave safely. This module covers practical evaluation strategies and responsible AI considerations.</p>"},{"location":"tracks/day1-llm/eval-safety-theory/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will:</p> <ul> <li>Understand why evaluation matters for LLM applications</li> <li>Know basic evaluation signals for assessing LLM outputs</li> <li>Be aware of safety and responsible use considerations</li> <li>Understand how evaluation integrates with the accelerator</li> </ul>"},{"location":"tracks/day1-llm/eval-safety-theory/#why-evaluation-matters","title":"Why Evaluation Matters","text":""},{"location":"tracks/day1-llm/eval-safety-theory/#the-problem","title":"The Problem","text":"<p>LLMs are powerful but unpredictable. Without evaluation, you risk:</p> <p>Hallucinations: Model generates plausible but false information <pre><code>Q: \"What is IBM's revenue in 2025?\"\nBad Answer: \"IBM's revenue in 2025 was $87.4 billion.\" [Made up number]\n</code></pre></p> <p>Inconsistent Answers: Same question, different responses <pre><code>Monday: \"The capital of Australia is Sydney.\"\nTuesday: \"The capital of Australia is Canberra.\" [Correct]\n</code></pre></p> <p>Business Impact: Wrong answers can lead to: - Lost customer trust - Compliance violations - Financial losses - Reputational damage</p>"},{"location":"tracks/day1-llm/eval-safety-theory/#the-solution","title":"The Solution","text":"<p>Systematic evaluation helps you: - Catch problems before they reach users - Compare different models or prompts - Track quality over time - Build confidence in your system</p>"},{"location":"tracks/day1-llm/eval-safety-theory/#simple-evaluation-signals","title":"Simple Evaluation Signals","text":""},{"location":"tracks/day1-llm/eval-safety-theory/#1-correctness-ground-truth-comparison","title":"1. Correctness (Ground Truth Comparison)","text":"<p>What it is: Does the answer match known correct information?</p> <p>How to measure: <pre><code>def evaluate_correctness(model_answer: str, ground_truth: str) -&gt; int:\n    \"\"\"Returns 1 if correct, 0 if incorrect\"\"\"\n    # Simple exact match\n    if model_answer.strip().lower() == ground_truth.strip().lower():\n        return 1\n\n    # Or use semantic similarity\n    similarity = compute_similarity(model_answer, ground_truth)\n    return 1 if similarity &gt; 0.8 else 0\n</code></pre></p> <p>Example: <pre><code>Question: \"What year was IBM founded?\"\nModel Answer: \"1911\"\nGround Truth: \"1911\"\nScore: 1 (Correct)\n\n---\n\nQuestion: \"What year was IBM founded?\"\nModel Answer: \"1920\"\nGround Truth: \"1911\"\nScore: 0 (Incorrect)\n</code></pre></p> <p>When to use: When you have known correct answers (test sets, benchmarks)</p>"},{"location":"tracks/day1-llm/eval-safety-theory/#2-coherence-and-relevance","title":"2. Coherence and Relevance","text":"<p>What it is: Is the answer logically sound and on-topic?</p> <p>How to measure (manual rubric): <pre><code>5 - Highly coherent, directly addresses the question\n4 - Mostly coherent, relevant but may have minor issues\n3 - Somewhat coherent, partially relevant\n2 - Incoherent or largely irrelevant\n1 - Nonsensical or completely off-topic\n</code></pre></p> <p>Example: <pre><code>Question: \"How do I reset my password?\"\nAnswer: \"To reset your password, click 'Forgot Password' on the login page...\"\nScore: 5 (Highly coherent and relevant)\n\n---\n\nQuestion: \"How do I reset my password?\"\nAnswer: \"Our company was founded in 2010 and has grown significantly...\"\nScore: 1 (Off-topic)\n</code></pre></p> <p>When to use: For open-ended questions without clear ground truth</p>"},{"location":"tracks/day1-llm/eval-safety-theory/#3-styleformat-adherence","title":"3. Style/Format Adherence","text":"<p>What it is: Does the output follow specified formatting?</p> <p>How to measure: <pre><code>def evaluate_format(response: str, expected_format: str) -&gt; int:\n    \"\"\"Check if response matches expected format\"\"\"\n    if expected_format == \"json\":\n        try:\n            json.loads(response)\n            return 1\n        except:\n            return 0\n\n    elif expected_format == \"bullet_points\":\n        lines = response.split('\\n')\n        bullet_lines = [l for l in lines if l.strip().startswith(('-', '*', '\u2022'))]\n        return 1 if len(bullet_lines) &gt;= 3 else 0\n\n    # Add more format checks as needed\n</code></pre></p> <p>Example: <pre><code>Instruction: \"List 3 benefits of cloud computing in bullet points.\"\n\nGood response:\n- Scalability and flexibility\n- Cost-effectiveness\n- High availability\nScore: 1\n\nBad response:\n\"Cloud computing offers many benefits including scalability...\"\nScore: 0 (Not in bullet format)\n</code></pre></p> <p>When to use: For structured outputs, API responses, report generation</p>"},{"location":"tracks/day1-llm/eval-safety-theory/#4-completeness","title":"4. Completeness","text":"<p>What it is: Does the answer address all parts of the question?</p> <p>How to measure: <pre><code>Question has 3 sub-parts:\n- Answers all 3: Score = 1.0\n- Answers 2 of 3: Score = 0.66\n- Answers 1 of 3: Score = 0.33\n- Answers 0: Score = 0\n</code></pre></p> <p>Example: <pre><code>Question: \"What is RAG, how does it work, and what are its benefits?\"\n\nFull Answer:\n\"RAG stands for Retrieval-Augmented Generation. It works by first retrieving \nrelevant documents, then using them as context for the LLM. Benefits include \nreduced hallucinations and up-to-date information.\"\nScore: 1.0 (Addresses all 3 parts)\n\nPartial Answer:\n\"RAG stands for Retrieval-Augmented Generation and helps reduce hallucinations.\"\nScore: 0.66 (Only 2 of 3 parts)\n</code></pre></p>"},{"location":"tracks/day1-llm/eval-safety-theory/#5-latency","title":"5. Latency","text":"<p>What it is: How long does the model take to respond?</p> <p>How to measure: <pre><code>import time\n\nstart = time.time()\nresponse = llm.generate(prompt)\nlatency = time.time() - start\n\n# Evaluate against threshold\nif latency &lt; 1.0:\n    score = 1  # Excellent\nelif latency &lt; 3.0:\n    score = 0.75  # Good\nelif latency &lt; 5.0:\n    score = 0.5  # Acceptable\nelse:\n    score = 0.25  # Too slow\n</code></pre></p> <p>When to use: Always, especially for user-facing applications</p>"},{"location":"tracks/day1-llm/eval-safety-theory/#safety-responsible-use","title":"Safety &amp; Responsible Use","text":""},{"location":"tracks/day1-llm/eval-safety-theory/#potential-risky-categories","title":"Potential Risky Categories","text":"<p>LLMs can potentially generate harmful content in these categories:</p> <ol> <li>Personal Information (PII)</li> <li>Social security numbers, credit cards, passwords</li> <li> <p>Addresses, phone numbers, email addresses</p> </li> <li> <p>Harmful Content</p> </li> <li>Hate speech, discrimination, harassment</li> <li>Violence, self-harm, dangerous activities</li> <li> <p>Illegal activities, fraud schemes</p> </li> <li> <p>Misinformation</p> </li> <li>Medical advice without disclaimers</li> <li>Financial advice as fact</li> <li> <p>False claims about public figures</p> </li> <li> <p>Bias and Fairness</p> </li> <li>Stereotyping based on protected attributes</li> <li>Unfair treatment of groups</li> <li> <p>Lack of representation</p> </li> <li> <p>Privacy Violations</p> </li> <li>Exposing training data</li> <li>Re-identifying anonymized data</li> <li>Leaking confidential business information</li> </ol>"},{"location":"tracks/day1-llm/eval-safety-theory/#mitigation-strategies","title":"Mitigation Strategies","text":""},{"location":"tracks/day1-llm/eval-safety-theory/#1-clear-instructions-to-the-model","title":"1. Clear Instructions to the Model","text":"<p>Example system prompt: <pre><code>SAFE_SYSTEM_PROMPT = \"\"\"You are a helpful assistant. Follow these guidelines:\n\n1. Do not generate, process, or request personal information (PII)\n2. Do not provide medical, legal, or financial advice\n3. If asked to do something harmful or illegal, politely decline\n4. If unsure, express uncertainty rather than guessing\n5. Be respectful and unbiased in all responses\n6. If a question is outside your scope, redirect to appropriate resources\n\nNow, assist the user with their request.\"\"\"\n</code></pre></p>"},{"location":"tracks/day1-llm/eval-safety-theory/#2-proper-context-scope","title":"2. Proper Context &amp; Scope","text":"<p>Limit the model's role: <pre><code># Instead of:\n\"You are an AI that can do anything.\"\n\n# Use:\n\"You are a customer support assistant for [Product]. \nYou help with account issues, billing questions, and basic troubleshooting.\nFor technical issues, escalate to the engineering team.\"\n</code></pre></p>"},{"location":"tracks/day1-llm/eval-safety-theory/#3-guardrails-and-review","title":"3. Guardrails and Review","text":"<p>Pre-processing (before sending to LLM): <pre><code>def check_input_safety(user_input: str) -&gt; bool:\n    \"\"\"Screen user input for harmful content\"\"\"\n    harmful_patterns = [\n        r'how to (hack|crack|steal)',\n        r'(bomb|weapon) (instructions|recipe)',\n        # ... more patterns\n    ]\n\n    for pattern in harmful_patterns:\n        if re.search(pattern, user_input, re.IGNORECASE):\n            return False  # Block request\n\n    return True  # Allow request\n</code></pre></p> <p>Post-processing (after LLM generates response): <pre><code>def check_output_safety(response: str) -&gt; bool:\n    \"\"\"Screen LLM output for harmful content\"\"\"\n    # Check for PII patterns\n    if re.search(r'\\d{3}-\\d{2}-\\d{4}', response):  # SSN pattern\n        return False\n\n    # Check for hate speech indicators\n    # Use a library like Detoxify or Perspective API\n    toxicity_score = check_toxicity(response)\n    if toxicity_score &gt; 0.7:\n        return False\n\n    return True\n</code></pre></p> <p>Fallback responses: <pre><code>SAFETY_FALLBACK = \"\"\"I'm not able to help with that request. \nIf you need assistance with [product/service], please contact [appropriate channel].\"\"\"\n</code></pre></p>"},{"location":"tracks/day1-llm/eval-safety-theory/#4-human-in-the-loop","title":"4. Human-in-the-Loop","text":"<p>For high-stakes applications: - Preview before send: Show user what the model will say - Feedback mechanisms: Allow users to report issues - Audit trails: Log all interactions for review - Manual review: Have humans spot-check outputs</p>"},{"location":"tracks/day1-llm/eval-safety-theory/#5-model-choice-and-configuration","title":"5. Model Choice and Configuration","text":"<p>Choose appropriate models: - Models trained with safety alignment (e.g., Granite's built-in guardrails) - Avoid models fine-tuned on unfiltered web data for sensitive applications</p> <p>Configure safely: <pre><code># Lower temperature for factual tasks (more deterministic, less creative)\nparams = {\n    \"temperature\": 0.2,  # Less randomness\n    \"top_p\": 0.1,        # More focused\n    \"max_tokens\": 200,   # Limit response length\n}\n</code></pre></p>"},{"location":"tracks/day1-llm/eval-safety-theory/#how-this-ties-into-the-accelerator","title":"How This Ties into the Accelerator","text":""},{"location":"tracks/day1-llm/eval-safety-theory/#evaluation-entry-points","title":"Evaluation Entry Points","text":""},{"location":"tracks/day1-llm/eval-safety-theory/#1-toolseval_smallpy","title":"1. <code>tools/eval_small.py</code>","text":"<p>Purpose: Run a small evaluation dataset through your RAG system</p> <p>What you'll implement on Day 2-3: <pre><code># tools/eval_small.py\n\nimport pandas as pd\nfrom accelerator.rag.pipeline import RAGPipeline\n\ndef evaluate_rag_system(test_file: str, output_file: str):\n    \"\"\"\n    Evaluate RAG system on a test set\n\n    Args:\n        test_file: CSV with columns [question, ground_truth, category]\n        output_file: Where to save results\n    \"\"\"\n    # Load test data\n    df = pd.read_csv(test_file)\n\n    # Initialize pipeline\n    pipeline = RAGPipeline()\n\n    results = []\n    for idx, row in df.iterrows():\n        # Generate answer\n        response = pipeline.answer_question(row['question'])\n\n        # Evaluate\n        correctness = evaluate_correctness(response, row['ground_truth'])\n        relevance = evaluate_relevance(response, row['question'])\n\n        results.append({\n            'question': row['question'],\n            'model_answer': response,\n            'ground_truth': row['ground_truth'],\n            'correctness': correctness,\n            'relevance': relevance,\n            'category': row['category']\n        })\n\n    # Save results\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(output_file, index=False)\n\n    # Summary stats\n    print(f\"Overall Correctness: {results_df['correctness'].mean():.2%}\")\n    print(f\"Overall Relevance: {results_df['relevance'].mean():.2%}\")\n</code></pre></p>"},{"location":"tracks/day1-llm/eval-safety-theory/#2-acceleratorassetsnotebookanalyze_log_and_feedbackipynb","title":"2. <code>accelerator/assets/notebook/Analyze_Log_and_Feedback.ipynb</code>","text":"<p>Purpose: Analyze logs and user feedback from the production service</p> <p>What it does: - Load logs from <code>service/api.py</code> - Analyze patterns:   - Most common questions   - Average response time   - User feedback ratings   - Error rates - Generate insights for improvement</p> <p>Example log entry: <pre><code>{\n  \"timestamp\": \"2025-01-15T10:30:00Z\",\n  \"question\": \"How do I reset my password?\",\n  \"answer\": \"To reset your password...\",\n  \"retrieved_docs\": [\"doc_123\", \"doc_456\"],\n  \"latency_ms\": 1250,\n  \"user_feedback\": \"helpful\",\n  \"model\": \"granite-13b-instruct\"\n}\n</code></pre></p>"},{"location":"tracks/day1-llm/eval-safety-theory/#your-manual-rubric-influences","title":"Your Manual Rubric Influences:","text":"<p>What you log (in <code>service/api.py</code>): <pre><code>@app.post(\"/ask\")\nasync def ask_question(request: QuestionRequest):\n    start_time = time.time()\n\n    # Generate answer\n    response = pipeline.answer_question(request.question)\n\n    # Log for later evaluation\n    log_entry = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"question\": request.question,\n        \"answer\": response.answer,\n        \"retrieved_docs\": response.source_ids,\n        \"latency_ms\": (time.time() - start_time) * 1000,\n        \"model\": config.model_id,\n        # Evaluation placeholders\n        \"user_feedback\": None,  # Filled in later\n        \"correctness\": None,    # Can be auto-evaluated if ground truth available\n    }\n\n    logger.info(log_entry)\n    return response\n</code></pre></p> <p>What metrics you compute (in <code>eval_small.py</code>): <pre><code># Metrics derived from your rubric\nmetrics = {\n    \"correctness\": df['correctness'].mean(),\n    \"relevance\": df['relevance'].mean(),\n    \"format_adherence\": df['format_ok'].mean(),\n    \"avg_latency_ms\": df['latency_ms'].mean(),\n    \"p95_latency_ms\": df['latency_ms'].quantile(0.95),\n}\n</code></pre></p>"},{"location":"tracks/day1-llm/eval-safety-theory/#how-this-connects-to-lab-13","title":"How This Connects to Lab 1.3","text":""},{"location":"tracks/day1-llm/eval-safety-theory/#what-youll-build","title":"What You'll Build","text":"<p>In Lab 1.3, you'll create a micro-evaluation framework:</p> <ol> <li>Test set: 5-10 diverse prompts</li> <li>Data collection: Run prompts through Ollama and watsonx</li> <li>Rating rubric: Apply manual ratings (correctness, clarity, style)</li> <li>Analysis: Compare backends, identify patterns</li> </ol>"},{"location":"tracks/day1-llm/eval-safety-theory/#example-output-dataframe","title":"Example Output (DataFrame)","text":"prompt backend answer correctness clarity style_match \"Summarize AI...\" ollama \"AI is...\" 4 5 4 \"Summarize AI...\" watsonx \"Artificial...\" 5 5 5 \"Extract emails...\" ollama \"john@...\" 5 4 5"},{"location":"tracks/day1-llm/eval-safety-theory/#skills-youll-develop","title":"Skills You'll Develop","text":"<ul> <li>Programmatic evaluation loops</li> <li>Rating rubric design</li> <li>Comparative analysis</li> <li>Data-driven decision making</li> </ul>"},{"location":"tracks/day1-llm/eval-safety-theory/#connection-to-production","title":"Connection to Production","text":"<p>This micro-framework is a prototype for: - <code>tools/eval_small.py</code> (automated evaluation) - <code>Analyze_Log_and_Feedback.ipynb</code> (production analytics) - Continuous monitoring dashboards</p>"},{"location":"tracks/day1-llm/eval-safety-theory/#reference-notebooks","title":"Reference Notebooks","text":""},{"location":"tracks/day1-llm/eval-safety-theory/#governance-evaluation","title":"Governance &amp; Evaluation","text":"<p><code>ibm-watsonx-governance-evaluation-studio-getting-started.ipynb</code>: - Shows watsonx.governance evaluation features - Demonstrates automated evaluation at scale - Connects to compliance tracking</p> <p>From this, you'll learn: - How to structure evaluation datasets - Metrics that matter for enterprise applications - Integration with governance workflows</p> <p>Progression: <pre><code>Lab 1.3 (Manual, 10 questions)\n    \u2193\neval_small.py (Automated, 100 questions)\n    \u2193\nEvaluation Studio (Continuous, production scale)\n</code></pre></p>"},{"location":"tracks/day1-llm/eval-safety-theory/#best-practices-for-evaluation","title":"Best Practices for Evaluation","text":""},{"location":"tracks/day1-llm/eval-safety-theory/#do","title":"\u2705 Do","text":"<ol> <li>Start simple: Don't over-engineer evaluation initially</li> <li>Automate what you can: Manual review doesn't scale</li> <li>Track over time: Evaluation is ongoing, not one-time</li> <li>Test edge cases: Don't just test happy paths</li> <li>Involve stakeholders: Domain experts should validate quality</li> <li>Version everything: Track prompts, models, and test sets</li> </ol>"},{"location":"tracks/day1-llm/eval-safety-theory/#dont","title":"\u274c Don't","text":"<ol> <li>Skip evaluation: \"It looks good\" isn't enough</li> <li>Rely solely on accuracy: Context matters (latency, safety, cost)</li> <li>Forget about drift: Models and data change over time</li> <li>Ignore user feedback: Real usage reveals issues testing doesn't</li> <li>Over-optimize for metrics: Gaming metrics != real quality</li> </ol>"},{"location":"tracks/day1-llm/eval-safety-theory/#evaluation-maturity-model","title":"Evaluation Maturity Model","text":""},{"location":"tracks/day1-llm/eval-safety-theory/#level-1-ad-hoc","title":"Level 1: Ad Hoc","text":"<ul> <li>Manual testing with a few examples</li> <li>\"Looks good to me\" approval</li> <li>No systematic tracking</li> </ul>"},{"location":"tracks/day1-llm/eval-safety-theory/#level-2-basic-lab-13-targets-this","title":"Level 2: Basic (\u2190 Lab 1.3 targets this)","text":"<ul> <li>Small test set (10-50 examples)</li> <li>Manual rubric</li> <li>Occasional re-evaluation</li> </ul>"},{"location":"tracks/day1-llm/eval-safety-theory/#level-3-systematic-eval_smallpy-targets-this","title":"Level 3: Systematic (\u2190 <code>eval_small.py</code> targets this)","text":"<ul> <li>Curated test set (100-500 examples)</li> <li>Automated metrics where possible</li> <li>Regular evaluation runs</li> <li>Version control for prompts and results</li> </ul>"},{"location":"tracks/day1-llm/eval-safety-theory/#level-4-continuous-production-goal","title":"Level 4: Continuous (\u2190 Production goal)","text":"<ul> <li>Large test set + production monitoring</li> <li>Automated evaluation pipeline</li> <li>A/B testing framework</li> <li>Real-time alerting on quality degradation</li> <li>Integration with CI/CD</li> </ul>"},{"location":"tracks/day1-llm/eval-safety-theory/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Evaluation is essential: Don't deploy LLMs without systematic quality checks</li> <li>Start simple: Basic metrics beat no metrics</li> <li>Safety first: Proactively mitigate risks with guardrails</li> <li>Iterate: Evaluation frameworks evolve with your application</li> <li>Automate: Scale evaluation as your system scales</li> </ul> <p>Next: Let's build your first evaluation framework in Lab 1.3!</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/","title":"Lab 1.1 \u2013 Quickstart in Both Environments","text":"<p>Duration: 45 minutes Difficulty: Beginner</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#lab-overview","title":"Lab Overview","text":"<p>In this lab, you'll get hands-on experience with LLMs in both local (Ollama) and managed (watsonx.ai) environments. The goal is to become comfortable switching between repos, sending prompts, and observing differences in outputs.</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lab, you will be able to:</p> <ul> <li>Send basic prompts to both Ollama and watsonx.ai</li> <li>Modify key parameters (temperature, max_tokens)</li> <li>Compare outputs and latency between backends</li> <li>Understand where the accelerator fits in</li> </ul>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#prerequisites","title":"Prerequisites","text":"<p>Required (from Day 0): - \u2705 <code>simple-ollama-environment</code> set up and working - \u2705 <code>simple-watsonx-enviroment</code> set up with valid credentials - \u2705 Both Jupyter environments accessible</p> <p>Optional but recommended: - Basic familiarity with Python and Jupyter notebooks - <code>watsonx-workshop</code> repo cloned (for accelerator reference)</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#step-1-quick-warm-up-with-ollama","title":"Step 1 \u2013 Quick Warm-Up with Ollama","text":""},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#11-open-the-ollama-notebook","title":"1.1 Open the Ollama Notebook","text":"<p>Navigate to your <code>simple-ollama-environment</code> repo:</p> <pre><code>cd ~/projects/watsonx-workshop/simple-ollama-environment\njupyter notebook\n</code></pre> <p>Open <code>notebooks/ollama_quickstart.ipynb</code> (or create a new notebook if you prefer).</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#12-run-a-simple-hello-prompt","title":"1.2 Run a Simple \"Hello\" Prompt","text":"<p>Add and run this cell:</p> <pre><code>import ollama\n\n# Simple greeting\nresponse = ollama.chat(\n    model=\"qwen2.5:0.5b-instruct\",  # or your preferred small model\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello! Who are you?\"}\n    ]\n)\n\nprint(response[\"message\"][\"content\"])\n</code></pre> <p>Expected output: The model should introduce itself.</p> <p>Note the: - Response time (how fast was it?) - Response style (casual, formal, verbose?)</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#13-run-a-short-reasoning-prompt","title":"1.3 Run a Short Reasoning Prompt","text":"<p>Now try a more challenging prompt:</p> <pre><code>import ollama\nimport time\n\nprompt = \"\"\"A farmer has 17 sheep, and all but 9 die. How many are left?\n\nLet's think step by step:\"\"\"\n\nstart_time = time.time()\n\nresponse = ollama.chat(\n    model=\"qwen2.5:0.5b-instruct\",\n    messages=[{\"role\": \"user\", \"content\": prompt}]\n)\n\nelapsed = time.time() - start_time\n\nprint(f\"Response (took {elapsed:.2f}s):\\n\")\nprint(response[\"message\"][\"content\"])\n</code></pre> <p>Questions to consider: - Did the model reason correctly? (Answer should be 9 sheep) - How long did it take? - Was the reasoning process clear?</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#step-2-warm-up-with-watsonxai","title":"Step 2 \u2013 Warm-Up with watsonx.ai","text":""},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#21-open-the-watsonx-notebook","title":"2.1 Open the watsonx Notebook","text":"<p>Navigate to your <code>simple-watsonx-enviroment</code> repo:</p> <pre><code>cd ~/projects/watsonx-workshop/simple-watsonx-enviroment\njupyter notebook\n</code></pre> <p>Open <code>notebooks/watsonx_quickstart.ipynb</code>.</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#22-verify-credentials","title":"2.2 Verify Credentials","text":"<p>Make sure your <code>.env</code> file is properly loaded:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Check credentials\napi_key = os.getenv(\"IBM_CLOUD_API_KEY\") or os.getenv(\"WATSONX_APIKEY\")\nurl = os.getenv(\"IBM_CLOUD_URL\") or os.getenv(\"WATSONX_URL\")\nproject_id = os.getenv(\"IBM_CLOUD_PROJECT_ID\") or os.getenv(\"PROJECT_ID\")\n\nprint(f\"API Key: {'\u2713 Set' if api_key else '\u2717 Missing'}\")\nprint(f\"URL: {url}\")\nprint(f\"Project ID: {'\u2713 Set' if project_id else '\u2717 Missing'}\")\n</code></pre> <p>All three should show as set.</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#23-run-the-same-hello-prompt","title":"2.3 Run the Same \"Hello\" Prompt","text":"<pre><code>import os\nimport time\nfrom dotenv import load_dotenv\nfrom ibm_watsonx_ai import Credentials\nfrom ibm_watsonx_ai.foundation_models import ModelInference\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n\nload_dotenv()\n\n# Setup\napi_key = os.getenv(\"IBM_CLOUD_API_KEY\") or os.getenv(\"WATSONX_APIKEY\")\nurl = os.getenv(\"IBM_CLOUD_URL\") or os.getenv(\"WATSONX_URL\")\nproject_id = os.getenv(\"IBM_CLOUD_PROJECT_ID\") or os.getenv(\"PROJECT_ID\")\n\ncredentials = Credentials(url=url, api_key=api_key)\n\n# Create model instance\nmodel = ModelInference(\n    model_id=\"ibm/granite-13b-instruct-v2\",\n    credentials=credentials,\n    project_id=project_id,\n)\n\n# Simple greeting\nprompt = \"Hello! Who are you?\"\n\nparams = {\n    GenParams.DECODING_METHOD: \"greedy\",\n    GenParams.MAX_NEW_TOKENS: 100,\n}\n\nstart_time = time.time()\nresponse = model.generate_text(prompt=prompt, params=params)\nelapsed = time.time() - start_time\n\nprint(f\"Response (took {elapsed:.2f}s):\\n\")\nprint(response)\n</code></pre> <p>Compare to Ollama: - How does the response differ in style? - Which was faster? - Which feels more \"polished\"?</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#24-run-the-reasoning-prompt","title":"2.4 Run the Reasoning Prompt","text":"<pre><code>prompt = \"\"\"A farmer has 17 sheep, and all but 9 die. How many are left?\n\nLet's think step by step:\"\"\"\n\nparams = {\n    GenParams.DECODING_METHOD: \"greedy\",\n    GenParams.MAX_NEW_TOKENS: 200,\n}\n\nstart_time = time.time()\nresponse = model.generate_text(prompt=prompt, params=params)\nelapsed = time.time() - start_time\n\nprint(f\"Response (took {elapsed:.2f}s):\\n\")\nprint(response)\n</code></pre> <p>Compare: - Did Granite reason correctly? - Was the reasoning clearer than Ollama's? - Latency difference?</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#step-3-modify-basic-parameters","title":"Step 3 \u2013 Modify Basic Parameters","text":"<p>Now let's see how parameters affect outputs.</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#31-temperature-experiment-ollama","title":"3.1 Temperature Experiment (Ollama)","text":"<p>Run the same prompt with different temperatures:</p> <pre><code>import ollama\n\nprompt = \"Write a creative opening line for a sci-fi novel.\"\n\nfor temp in [0.0, 0.5, 1.0, 1.5]:\n    print(f\"\\n{'='*60}\")\n    print(f\"Temperature: {temp}\")\n    print(f\"{'='*60}\")\n\n    response = ollama.chat(\n        model=\"qwen2.5:0.5b-instruct\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        options={\"temperature\": temp}\n    )\n\n    print(response[\"message\"][\"content\"])\n</code></pre> <p>Observations: - At temp=0.0, do you get the same response every time? - At temp=1.5, how much does the creativity increase?</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#32-temperature-experiment-watsonxai","title":"3.2 Temperature Experiment (watsonx.ai)","text":"<pre><code>prompt = \"Write a creative opening line for a sci-fi novel.\"\n\nfor temp in [0.0, 0.5, 1.0, 1.5]:\n    print(f\"\\n{'='*60}\")\n    print(f\"Temperature: {temp}\")\n    print(f\"{'='*60}\")\n\n    params = {\n        GenParams.DECODING_METHOD: \"sample\",  # Note: sample, not greedy\n        GenParams.TEMPERATURE: temp,\n        GenParams.MAX_NEW_TOKENS: 50,\n    }\n\n    response = model.generate_text(prompt=prompt, params=params)\n    print(response)\n</code></pre> <p>Note: For temperature &gt; 0, use <code>DECODING_METHOD: \"sample\"</code> (not \"greedy\").</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#33-max-tokens-experiment","title":"3.3 Max Tokens Experiment","text":"<p>See what happens when you limit output length:</p> <p>Ollama: <pre><code>prompt = \"Explain quantum computing in detail.\"\n\nfor max_tokens in [20, 50, 100]:\n    print(f\"\\n{'='*40}\")\n    print(f\"Max Tokens: {max_tokens}\")\n    print(f\"{'='*40}\")\n\n    response = ollama.chat(\n        model=\"qwen2.5:0.5b-instruct\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        options={\"num_predict\": max_tokens}  # Ollama uses 'num_predict'\n    )\n\n    print(response[\"message\"][\"content\"])\n</code></pre></p> <p>watsonx.ai: <pre><code>prompt = \"Explain quantum computing in detail.\"\n\nfor max_tokens in [20, 50, 100]:\n    print(f\"\\n{'='*40}\")\n    print(f\"Max Tokens: {max_tokens}\")\n    print(f\"{'='*40}\")\n\n    params = {\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.MAX_NEW_TOKENS: max_tokens,\n    }\n\n    response = model.generate_text(prompt=prompt, params=params)\n    print(response)\n</code></pre></p> <p>Observations: - At 20 tokens, is the response complete? - At 100 tokens, is there a noticeable quality difference?</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#step-4-optional-peek-at-accelerator-pipeline","title":"Step 4 (Optional) \u2013 Peek at Accelerator Pipeline","text":"<p>This step is read-only\u2014just to see where we're headed on Day 2.</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#41-open-acceleratorragpipelinepy","title":"4.1 Open accelerator/rag/pipeline.py","text":"<p>Navigate to the <code>watsonx-workshop</code> repo and open <code>accelerator/rag/pipeline.py</code> in a text editor or Jupyter:</p> <pre><code>cd ~/projects/watsonx-workshop/accelerator\ncat rag/pipeline.py  # or open in your editor\n</code></pre> <p>Look for: - A function like <code>answer_question(question: str) -&gt; str</code> - Currently, it might be a placeholder:</p> <pre><code>def answer_question(question: str) -&gt; str:\n    \"\"\"Answer a question (placeholder for now)\"\"\"\n    # TODO: Add retrieval\n    # TODO: Build prompt with context\n    # TODO: Call LLM\n    return \"This is a placeholder response.\"\n</code></pre>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#42-mental-mapping-exercise","title":"4.2 Mental Mapping Exercise","text":"<p>Think about how the single LLM call you just made maps to the accelerator:</p> <p>Your notebook (today): <pre><code>response = model.generate_text(prompt=\"What is RAG?\")\n</code></pre></p> <p>Accelerator (Day 2): <pre><code>def answer_question(question: str) -&gt; str:\n    # 1. Retrieve relevant docs\n    docs = retriever.search(question)\n\n    # 2. Build prompt with context\n    prompt = prompt_template.format(question=question, context=docs)\n\n    # 3. Generate answer (same call you just learned!)\n    response = model.generate_text(prompt=prompt)\n\n    return response\n</code></pre></p> <p>Key insight: The LLM call is the same\u2014RAG just adds context retrieval beforehand.</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#step-5-optional-peek-at-reference-notebooks","title":"Step 5 (Optional) \u2013 Peek at Reference Notebooks","text":""},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#51-open-a-rag-notebook","title":"5.1 Open a RAG Notebook","text":"<p>Navigate to <code>labs-src/</code> and open:</p> <pre><code>labs-src/use-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb\n</code></pre> <p>Don't run it all\u2014just: - Scroll to the section with the LLM call - Notice how context is injected into the prompt - Compare to your simple prompts above</p> <p>Example structure you'll see: <pre><code>context = \"...retrieved documents...\"\nprompt = f\"\"\"Based on this context:\n\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\nresponse = llm.generate(prompt)\n</code></pre></p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#reflection-questions","title":"Reflection Questions","text":"<p>Take a few minutes to think about and discuss (if in a group):</p> <ol> <li>What did you notice about differences between Ollama and watsonx?</li> <li>Speed?</li> <li>Answer quality?</li> <li> <p>Ease of use?</p> </li> <li> <p>Which one feels faster / more flexible?</p> </li> <li>Local = faster for small models?</li> <li> <p>Managed = faster for large models?</p> </li> <li> <p>When would you choose one over the other?</p> </li> <li>Privacy concerns \u2192 Ollama</li> <li>Production scale \u2192 watsonx.ai</li> <li> <p>Prototyping \u2192 Either!</p> </li> <li> <p>How did parameter changes affect outputs?</p> </li> <li>Temperature impact on creativity?</li> <li>Max tokens impact on completeness?</li> </ol>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#checkpoint","title":"Checkpoint","text":"<p>Before moving on, confirm:</p> <ul> <li>\u2705 You can run notebooks in both <code>simple-ollama-environment</code> and <code>simple-watsonx-enviroment</code></li> <li>\u2705 You successfully generated responses from both backends</li> <li>\u2705 You experimented with temperature and max_tokens</li> <li>\u2705 You've seen where the production pipeline (<code>accelerator/rag/pipeline.py</code>) will call the LLM</li> </ul> <p>If all boxes are checked, you're ready for Lab 1.2 \u2013 Prompt Templates!</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#ollama-issues","title":"Ollama Issues","text":"<p>\"Connection refused\": <pre><code># Check if Ollama is running\ncurl http://localhost:11434/api/tags\n\n# If not, start it\nollama serve  # or restart Docker container\n</code></pre></p> <p>\"Model not found\": <pre><code># Pull the model\nollama pull qwen2.5:0.5b-instruct\n</code></pre></p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#watsonxai-issues","title":"watsonx.ai Issues","text":"<p>\"Invalid API key\": - Check your <code>.env</code> file - Verify the key in IBM Cloud console - Ensure no extra spaces in the <code>.env</code> file</p> <p>\"Project not found\": - Verify the <code>PROJECT_ID</code> in IBM Cloud - Ensure you have access to the project</p> <p>\"Rate limit exceeded\": - Wait a few seconds between requests - If persistent, check your IBM Cloud quota</p>"},{"location":"tracks/day1-llm/lab-1-quickstart-two-envs/#next-steps","title":"Next Steps","text":"<p>Great work! You've completed Lab 1.1. </p> <p>Next: Move on to Lab 1.2 \u2013 Prompt Templates to learn how to build reusable prompt patterns.</p>"},{"location":"tracks/day1-llm/lab-2-prompt-templates/","title":"Lab 1.2 \u2013 Prompt Templates in Ollama &amp; watsonx","text":"<p>Duration: 60 minutes Difficulty: Intermediate</p>"},{"location":"tracks/day1-llm/lab-2-prompt-templates/#lab-overview","title":"Lab Overview","text":"<p>In this lab, you'll build reusable prompt templates for common tasks (summarization, style rewriting, Q&amp;A) and implement them in both Ollama and watsonx.ai environments.</p>"},{"location":"tracks/day1-llm/lab-2-prompt-templates/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Create reusable prompt templates using Python</li> <li>Implement templates across different LLM backends</li> <li>Compare outputs systematically</li> <li>Design prompts for the accelerator's RAG system</li> </ul>"},{"location":"tracks/day1-llm/lab-2-prompt-templates/#prerequisites","title":"Prerequisites","text":"<ul> <li>\u2705 Lab 1.1 completed</li> <li>\u2705 Understanding of prompt patterns from theory section 1.2</li> </ul> <p>Due to length constraints, please view the complete lab instructions for Lab 1.2 in the repository. The lab covers:</p> <p>Part A: Ollama Templates - Summarization template - Style rewrite template - Q&amp;A with context template</p> <p>Part B: watsonx.ai Templates - Same templates implemented for watsonx.ai</p> <p>Part C: Comparative Experiments - Run same prompts across both backends - Measure quality, latency, consistency</p> <p>Part D: Accelerator Integration - Plan production prompts for <code>rag/prompt.py</code></p>"},{"location":"tracks/day1-llm/lab-2-prompt-templates/#deliverables","title":"Deliverables","text":"<p>By the end of this lab, you should have: - \u2705 <code>prompt_patterns_ollama.ipynb</code> - working templates - \u2705 <code>prompt_patterns_watsonx.ipynb</code> - working templates - \u2705 Comparison results documented - \u2705 Draft design for production prompts</p>"},{"location":"tracks/day1-llm/lab-2-prompt-templates/#next-steps","title":"Next Steps","text":"<p>Continue to Lab 1.3 \u2013 Micro-Evaluation to build a systematic evaluation framework.</p>"},{"location":"tracks/day1-llm/lab-3-micro-eval/","title":"Lab 1.3 \u2013 Micro-Evaluation Exercise","text":"<p>Duration: 60 minutes Difficulty: Intermediate</p>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#lab-overview","title":"Lab Overview","text":"<p>Build a systematic evaluation framework to compare Ollama and watsonx.ai responses across multiple prompts. You'll create a test set, collect outputs, apply a rating rubric, and analyze results.</p>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Programmatically collect and evaluate model outputs</li> <li>Design and apply a rating rubric</li> <li>Store results in structured format (DataFrame)</li> <li>Lay groundwork for production evaluation</li> </ul>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#prerequisites","title":"Prerequisites","text":"<ul> <li>\u2705 Labs 1.1 and 1.2 completed</li> <li>\u2705 Basic Python &amp; pandas familiarity</li> <li>\u2705 Understanding of evaluation principles from section 1.3</li> </ul>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#step-1-define-a-test-set","title":"Step 1 \u2013 Define a Test Set","text":"<p>Create a diverse set of test prompts covering different task types.</p> <p>Create file: <code>evaluation_test_set.py</code></p> <pre><code># evaluation_test_set.py\n\nTEST_PROMPTS = [\n    {\n        \"prompt\": \"Summarize the following in 2 sentences: Machine learning is a subset of artificial intelligence that enables systems to learn from data without explicit programming.\",\n        \"task_type\": \"summarization\",\n        \"expected_keywords\": [\"machine learning\", \"AI\", \"data\", \"learn\"]\n    },\n    {\n        \"prompt\": \"Extract the main entities from: 'IBM released watsonx.ai in 2023 as an enterprise AI platform.'\",\n        \"task_type\": \"extraction\",\n        \"expected_keywords\": [\"IBM\", \"watsonx.ai\", \"2023\"]\n    },\n    {\n        \"prompt\": \"Rewrite formally: 'Hey team, the API is down, can someone check it ASAP?'\",\n        \"task_type\": \"style_transfer\",\n        \"expected_keywords\": [\"API\", \"unavailable\", \"investigate\"]\n    },\n    {\n        \"prompt\": \"Answer: What is the capital of France?\",\n        \"task_type\": \"qa_factual\",\n        \"ground_truth\": \"Paris\",\n        \"expected_keywords\": [\"Paris\"]\n    },\n    {\n        \"prompt\": \"Explain in simple terms: What is a REST API?\",\n        \"task_type\": \"explanation\",\n        \"expected_keywords\": [\"API\", \"web\", \"HTTP\", \"request\", \"response\"]\n    },\n    {\n        \"prompt\": \"List 3 benefits of cloud computing in bullet points.\",\n        \"task_type\": \"list_generation\",\n        \"expected_format\": \"bullets\"\n    },\n    {\n        \"prompt\": \"Translate to Spanish: 'Good morning, how are you?'\",\n        \"task_type\": \"translation\",\n        \"ground_truth\": \"Buenos d\u00edas, \u00bfc\u00f3mo est\u00e1s?\",\n        \"expected_keywords\": [\"Buenos d\u00edas\", \"c\u00f3mo\"]\n    },\n    {\n        \"prompt\": \"Write a haiku about artificial intelligence.\",\n        \"task_type\": \"creative\",\n        \"expected_format\": \"haiku\"\n    },\n    {\n        \"prompt\": \"Calculate: If a product costs $100 and has a 20% discount, what is the final price?\",\n        \"task_type\": \"reasoning\",\n        \"ground_truth\": \"$80\"\n    },\n    {\n        \"prompt\": \"Based on this context: 'Python 3.11 was released in October 2022.', answer: When was Python 3.11 released?\",\n        \"task_type\": \"qa_context\",\n        \"ground_truth\": \"October 2022\"\n    }\n]\n</code></pre>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#step-2-collect-outputs-from-both-backends","title":"Step 2 \u2013 Collect Outputs from Both Backends","text":"<p>Create a notebook that runs all prompts through both models.</p> <p>Create file: <code>micro_evaluation.ipynb</code></p> <pre><code>import pandas as pd\nimport time\nfrom typing import List, Dict\nimport ollama\nfrom ibm_watsonx_ai import Credentials\nfrom ibm_watsonx_ai.foundation_models import ModelInference\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\nfrom dotenv import load_dotenv\nimport os\n\n# Load test prompts\nfrom evaluation_test_set import TEST_PROMPTS\n\n# Setup watsonx.ai\nload_dotenv()\napi_key = os.getenv(\"IBM_CLOUD_API_KEY\") or os.getenv(\"WATSONX_APIKEY\")\nurl = os.getenv(\"IBM_CLOUD_URL\") or os.getenv(\"WATSONX_URL\")\nproject_id = os.getenv(\"IBM_CLOUD_PROJECT_ID\") or os.getenv(\"PROJECT_ID\")\n\nwatsonx_model = ModelInference(\n    model_id=\"ibm/granite-13b-instruct-v2\",\n    credentials=Credentials(url=url, api_key=api_key),\n    project_id=project_id,\n)\n</code></pre> <p>Data collection function:</p> <pre><code>def collect_responses() -&gt; List[Dict]:\n    \"\"\"Collect responses from both backends for all test prompts.\"\"\"\n    results = []\n\n    for i, test_case in enumerate(TEST_PROMPTS, 1):\n        prompt = test_case[\"prompt\"]\n        print(f\"\\n[{i}/{len(TEST_PROMPTS)}] Processing: {prompt[:50]}...\")\n\n        # Ollama response\n        try:\n            start = time.time()\n            ollama_resp = ollama.chat(\n                model=\"qwen2.5:0.5b-instruct\",\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            ollama_answer = ollama_resp[\"message\"][\"content\"]\n            ollama_time = time.time() - start\n        except Exception as e:\n            ollama_answer = f\"ERROR: {str(e)}\"\n            ollama_time = 0\n\n        # watsonx.ai response\n        try:\n            params = {\n                GenParams.DECODING_METHOD: \"greedy\",\n                GenParams.MAX_NEW_TOKENS: 200,\n            }\n            start = time.time()\n            watsonx_answer = watsonx_model.generate_text(prompt=prompt, params=params)\n            watsonx_time = time.time() - start\n        except Exception as e:\n            watsonx_answer = f\"ERROR: {str(e)}\"\n            watsonx_time = 0\n\n        # Store results for Ollama\n        results.append({\n            \"prompt\": prompt,\n            \"backend\": \"ollama\",\n            \"model\": \"qwen2.5:0.5b-instruct\",\n            \"answer\": ollama_answer,\n            \"latency_ms\": ollama_time * 1000,\n            \"task_type\": test_case[\"task_type\"],\n            \"ground_truth\": test_case.get(\"ground_truth\", \"\"),\n            \"expected_keywords\": test_case.get(\"expected_keywords\", []),\n            \"expected_format\": test_case.get(\"expected_format\", \"\"),\n            # Evaluation fields (to be filled in next step)\n            \"correctness\": None,\n            \"clarity\": None,\n            \"style_match\": None,\n            \"completeness\": None,\n            \"notes\": \"\"\n        })\n\n        # Store results for watsonx.ai\n        results.append({\n            \"prompt\": prompt,\n            \"backend\": \"watsonx\",\n            \"model\": \"granite-13b-instruct-v2\",\n            \"answer\": watsonx_answer,\n            \"latency_ms\": watsonx_time * 1000,\n            \"task_type\": test_case[\"task_type\"],\n            \"ground_truth\": test_case.get(\"ground_truth\", \"\"),\n            \"expected_keywords\": test_case.get(\"expected_keywords\", []),\n            \"expected_format\": test_case.get(\"expected_format\", \"\"),\n            \"correctness\": None,\n            \"clarity\": None,\n            \"style_match\": None,\n            \"completeness\": None,\n            \"notes\": \"\"\n        })\n\n        time.sleep(0.5)  # Rate limiting\n\n    return results\n\n# Collect all responses\nresults = collect_responses()\n\n# Create DataFrame\ndf = pd.DataFrame(results)\nprint(f\"\\n\u2713 Collected {len(df)} responses ({len(df)//2} prompts \u00d7 2 backends)\")\ndf.head()\n</code></pre>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#step-3-apply-a-rating-rubric","title":"Step 3 \u2013 Apply a Rating Rubric","text":"<p>Define evaluation functions and apply them:</p> <pre><code>def evaluate_correctness(row: pd.Series) -&gt; int:\n    \"\"\"\n    Evaluate correctness (1-5 scale).\n    5 = Fully correct, 3 = Partially correct, 1 = Incorrect\n    \"\"\"\n    answer = row['answer'].lower()\n\n    # If ground truth exists, check it\n    if row['ground_truth']:\n        gt = row['ground_truth'].lower()\n        if gt in answer:\n            return 5\n        else:\n            return 2  # Wrong but attempted\n\n    # Otherwise, check for expected keywords\n    if row['expected_keywords']:\n        found = sum(1 for kw in row['expected_keywords'] if kw.lower() in answer)\n        if found == len(row['expected_keywords']):\n            return 5\n        elif found &gt; len(row['expected_keywords']) / 2:\n            return 4\n        elif found &gt; 0:\n            return 3\n        else:\n            return 2\n\n    # Default (manual review needed)\n    return 3\n\ndef evaluate_clarity(row: pd.Series) -&gt; int:\n    \"\"\"\n    Evaluate clarity (1-5 scale).\n    5 = Crystal clear, 1 = Confusing/incoherent\n    \"\"\"\n    answer = row['answer']\n\n    # Simple heuristics\n    if len(answer) &lt; 10:\n        return 2  # Too short\n    if len(answer.split()) &lt; 5:\n        return 2\n    if \"ERROR\" in answer:\n        return 1\n\n    # Check for coherent sentence structure\n    if answer.strip().endswith(('.', '!', '?')):\n        return 5\n    else:\n        return 4\n\ndef evaluate_style_match(row: pd.Series) -&gt; int:\n    \"\"\"\n    Evaluate style/format match (1-5 scale).\n    5 = Perfect format, 1 = Wrong format\n    \"\"\"\n    answer = row['answer']\n    expected_format = row['expected_format']\n\n    if not expected_format:\n        return 5  # N/A, give benefit of doubt\n\n    if expected_format == \"bullets\":\n        if any(marker in answer for marker in ['-', '*', '\u2022', '1.', '2.']):\n            return 5\n        else:\n            return 2\n\n    if expected_format == \"haiku\":\n        lines = [l.strip() for l in answer.split('\\n') if l.strip()]\n        if len(lines) == 3:\n            return 5\n        else:\n            return 3\n\n    return 3  # Default\n\n# Apply evaluations\ndf['correctness'] = df.apply(evaluate_correctness, axis=1)\ndf['clarity'] = df.apply(evaluate_clarity, axis=1)\ndf['style_match'] = df.apply(evaluate_style_match, axis=1)\n\nprint(\"\u2713 Evaluations applied\")\ndf[['prompt', 'backend', 'answer', 'correctness', 'clarity', 'style_match']].head(10)\n</code></pre>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#step-4-analyze-results","title":"Step 4 \u2013 Analyze Results","text":"<p>Compute statistics and visualize:</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Summary statistics by backend\nsummary = df.groupby('backend').agg({\n    'correctness': 'mean',\n    'clarity': 'mean',\n    'style_match': 'mean',\n    'latency_ms': ['mean', 'median']\n}).round(2)\n\nprint(\"Summary by Backend:\")\nprint(summary)\nprint()\n\n# Summary by task type\ntask_summary = df.groupby(['backend', 'task_type']).agg({\n    'correctness': 'mean',\n    'clarity': 'mean'\n}).round(2)\n\nprint(\"Summary by Task Type:\")\nprint(task_summary)\n\n# Visualizations\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Correctness comparison\ndf.groupby('backend')['correctness'].mean().plot(kind='bar', ax=axes[0], color=['#1f77b4', '#ff7f0e'])\naxes[0].set_title('Average Correctness Score')\naxes[0].set_ylabel('Score (1-5)')\naxes[0].set_ylim(0, 5)\n\n# Clarity comparison\ndf.groupby('backend')['clarity'].mean().plot(kind='bar', ax=axes[1], color=['#1f77b4', '#ff7f0e'])\naxes[1].set_title('Average Clarity Score')\naxes[1].set_ylabel('Score (1-5)')\naxes[1].set_ylim(0, 5)\n\n# Latency comparison\ndf.groupby('backend')['latency_ms'].median().plot(kind='bar', ax=axes[2], color=['#1f77b4', '#ff7f0e'])\naxes[2].set_title('Median Latency')\naxes[2].set_ylabel('Milliseconds')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Interpretation: - Which backend scored higher on correctness? - Which was clearer? - Which was faster? - Were there task types where one significantly outperformed?</p>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#step-5-accelerator-alignment","title":"Step 5 \u2013 Accelerator Alignment","text":"<p>Think ahead to production monitoring.</p>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#define-production-schema","title":"Define Production Schema","text":"<pre><code># This is the schema you'll use in accelerator/service/api.py\n\nPRODUCTION_LOG_SCHEMA = {\n    \"timestamp\": \"2025-01-15T10:30:00Z\",\n    \"request_id\": \"uuid\",\n    \"question\": \"user's question\",\n    \"answer\": \"generated answer\",\n    \"retrieved_doc_ids\": [\"doc_1\", \"doc_2\"],\n    \"latency_ms\": 1250,\n    \"model\": \"granite-13b-instruct-v2\",\n    \"backend\": \"watsonx\",\n    # Evaluation fields (filled by eval_small.py or user feedback)\n    \"correctness\": None,\n    \"relevance\": None,\n    \"user_feedback\": None,  # \"helpful\", \"not_helpful\", etc.\n    \"notes\": \"\"\n}\n</code></pre> <p>What goes where: - <code>service/api.py</code> logs this structure - <code>tools/eval_small.py</code> reads logs and computes metrics - <code>Analyze_Log_and_Feedback.ipynb</code> visualizes trends</p>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#step-6-save-results","title":"Step 6 \u2013 Save Results","text":"<pre><code># Save to CSV\ndf.to_csv('micro_evaluation_results.csv', index=False)\nprint(\"\u2713 Results saved to micro_evaluation_results.csv\")\n\n# Also save summary\nwith open('micro_evaluation_summary.txt', 'w') as f:\n    f.write(\"Micro-Evaluation Summary\\n\")\n    f.write(\"=\"*60 + \"\\n\\n\")\n    f.write(str(summary))\n    f.write(\"\\n\\n\")\n    f.write(\"Task Type Breakdown:\\n\")\n    f.write(str(task_summary))\n\nprint(\"\u2713 Summary saved to micro_evaluation_summary.txt\")\n</code></pre>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#wrap-up-questions","title":"Wrap-Up Questions","text":"<ol> <li>What patterns did you observe?</li> <li>Were certain task types harder for one backend?</li> <li> <p>Latency vs. quality trade-offs?</p> </li> <li> <p>What would you improve?</p> </li> <li>More test cases?</li> <li>Better evaluation metrics?</li> <li> <p>Automated correctness checking?</p> </li> <li> <p>How does this connect to production?</p> </li> <li>This mini-framework \u2192 <code>eval_small.py</code></li> <li>Manual rubrics \u2192 automated metrics</li> <li>10 prompts \u2192 1000+ prompts</li> </ol>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#checkpoint","title":"Checkpoint","text":"<p>Before moving on: - \u2705 DataFrame with prompts, outputs, and ratings created - \u2705 Basic summary statistics computed - \u2705 Visualizations generated - \u2705 Draft schema for production logs defined</p> <p>If all boxes are checked, congratulations! You've completed Day 1 labs.</p>"},{"location":"tracks/day1-llm/lab-3-micro-eval/#next-steps","title":"Next Steps","text":"<p>Day 2: We'll add retrieval (RAG) to the mix and integrate with the accelerator.</p> <p>Optional homework:  - Expand test set to 20-30 prompts - Implement more sophisticated evaluation metrics - Try different models (if available)</p>"},{"location":"tracks/day1-llm/llm-concepts/","title":"1.0 LLM Concepts &amp; Architecture","text":"<p>Welcome to Day 1 of the watsonx Workshop! Today we'll explore Large Language Models (LLMs), their architecture, and how to work with them effectively.</p>"},{"location":"tracks/day1-llm/llm-concepts/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will:</p> <ul> <li>Understand core LLM terminology and constraints</li> <li>Compare local vs managed LLM deployment models</li> <li>Know how LLMs fit into production architectures</li> <li>Understand key parameters that control model behavior</li> </ul>"},{"location":"tracks/day1-llm/llm-concepts/#what-is-a-large-language-model","title":"What is a Large Language Model?","text":"<p>A Large Language Model (LLM) is a neural network trained on vast amounts of text data to understand and generate human-like text. Think of it as a powerful pattern-matching engine that has learned the statistical relationships between words, phrases, and concepts.</p>"},{"location":"tracks/day1-llm/llm-concepts/#key-characteristics","title":"Key Characteristics","text":"<p>Scale: LLMs contain billions of parameters (weights) that encode knowledge learned from training data. For example: - GPT-3: 175 billion parameters - Llama 3.2: 1-3 billion parameters (smaller variants) - Granite 13B: 13 billion parameters</p> <p>Training Data: Models are trained on diverse text sources including: - Books, articles, and documentation - Web pages and forums - Code repositories - Scientific papers</p> <p>Capabilities: Modern LLMs can: - Answer questions - Summarize documents - Write code - Translate languages - Extract structured information - Reason through problems (with varying degrees of success)</p>"},{"location":"tracks/day1-llm/llm-concepts/#key-concepts","title":"Key Concepts","text":""},{"location":"tracks/day1-llm/llm-concepts/#tokens-tokenization","title":"Tokens &amp; Tokenization","text":"<p>LLMs don't work with words\u2014they work with tokens. A token is a sub-unit of text that the model processes.</p> <p>Examples: - \"Hello\" \u2192 1 token - \"watsonx.ai\" \u2192 might be 2-3 tokens (depends on the tokenizer) - \"AI\" \u2192 1 token - A space or punctuation can be its own token</p> <p>Why this matters: - Models have token limits (context windows) - API costs are often calculated per token - Long documents need to be chunked to fit within token limits</p> <p>Rule of thumb:  - English: ~4 characters per token on average - Code: Often more tokens per line than natural language</p>"},{"location":"tracks/day1-llm/llm-concepts/#context-window-and-truncation","title":"Context Window and Truncation","text":"<p>The context window is the maximum number of tokens a model can process at once. This includes both: - Your input (prompt) - The model's output (completion)</p> <p>Common context window sizes: - Llama 3.2 (1B): 128K tokens - Granite 13B: 8K tokens (some variants) - GPT-4: 8K-32K tokens (depending on version)</p> <p>Truncation: If your input exceeds the context window, it gets truncated (cut off), which can lead to: - Missing important context - Incomplete responses - Errors</p> <p>Best practice: Always check token counts before sending prompts to ensure you stay within limits.</p>"},{"location":"tracks/day1-llm/llm-concepts/#temperature-top-k-top-p","title":"Temperature, Top-k, Top-p","text":"<p>These parameters control the randomness and creativity of model outputs.</p>"},{"location":"tracks/day1-llm/llm-concepts/#temperature-00-to-20","title":"Temperature (0.0 to 2.0)","text":"<p>Controls output randomness:</p> <ul> <li>Low (0.0-0.3): Deterministic, focused</li> <li>Use for: Code generation, factual Q&amp;A, structured outputs</li> <li> <p>Output: Consistent, predictable</p> </li> <li> <p>Medium (0.7-1.0): Balanced creativity</p> </li> <li>Use for: General conversation, content generation</li> <li> <p>Output: Varied but coherent</p> </li> <li> <p>High (1.5-2.0): Very creative, less predictable</p> </li> <li>Use for: Creative writing, brainstorming</li> <li>Output: Diverse, sometimes surprising</li> </ul> <p>Example: <pre><code># Temperature = 0.0\n\"The capital of France is Paris.\"\n\n# Temperature = 1.5\n\"The capital of France? Ah, the City of Light\u2014Paris! \nKnown for its cafes, the Eiffel Tower, and rich history...\"\n</code></pre></p>"},{"location":"tracks/day1-llm/llm-concepts/#top-k-integer","title":"Top-k (integer)","text":"<p>Limits the model to choose from the top-k most probable next tokens.</p> <ul> <li>Low k (1-10): Very focused, less diverse</li> <li>High k (50-100): More diverse options</li> </ul>"},{"location":"tracks/day1-llm/llm-concepts/#top-p-00-to-10-also-called-nucleus-sampling","title":"Top-p (0.0 to 1.0, also called \"nucleus sampling\")","text":"<p>Dynamically selects from the smallest set of tokens whose cumulative probability exceeds p.</p> <ul> <li>Low p (0.1-0.5): Conservative, focused</li> <li>High p (0.9-0.95): More diverse</li> </ul> <p>Typical settings: - Factual tasks: <code>temperature=0.2, top_p=0.1</code> - Creative tasks: <code>temperature=0.9, top_p=0.9</code></p>"},{"location":"tracks/day1-llm/llm-concepts/#latency-and-throughput","title":"Latency and Throughput","text":"<p>Latency: Time from sending a request to receiving the complete response. - Affected by: Model size, prompt length, generation length, hardware</p> <p>Throughput: Number of requests processed per unit time. - Important for: Production systems, batch processing</p> <p>Factors affecting performance: - Model size: Larger models = slower inference - Batch size: Processing multiple requests together improves throughput - Hardware: GPUs provide much faster inference than CPUs</p>"},{"location":"tracks/day1-llm/llm-concepts/#local-vs-managed-llms","title":"Local vs Managed LLMs","text":"<p>You have two main deployment options for LLMs in production:</p>"},{"location":"tracks/day1-llm/llm-concepts/#local-llms-eg-ollama","title":"Local LLMs (e.g., Ollama)","text":"<p>What it is: Running models on your own infrastructure (laptop, on-prem servers, private cloud).</p> <p>Pros: - \u2705 Privacy: Data never leaves your environment - \u2705 Control: Full control over model versions and updates - \u2705 Cost: No per-token API charges after initial setup - \u2705 Customization: Can fine-tune models for specific use cases - \u2705 Offline: Works without internet connectivity</p> <p>Cons: - \u274c Hardware requirements: Need GPUs for acceptable performance - \u274c Maintenance: You manage infrastructure, updates, scaling - \u274c Limited scale: Constrained by your hardware resources - \u274c Model selection: Limited to models that fit in your hardware</p> <p>Best for: - Prototyping and development - Privacy-sensitive applications - Organizations with existing GPU infrastructure - Small to medium workloads</p> <p>Example tools: - Ollama: Easy local LLM management - LM Studio: GUI for local models - vLLM: High-performance inference server</p>"},{"location":"tracks/day1-llm/llm-concepts/#managed-llms-eg-watsonxai","title":"Managed LLMs (e.g., watsonx.ai)","text":"<p>What it is: Using LLMs via cloud APIs where the provider handles infrastructure.</p> <p>Pros: - \u2705 Scale: Handle any workload, automatic scaling - \u2705 Governance: Built-in compliance, audit trails, guardrails - \u2705 Model catalog: Access to multiple models without hardware concerns - \u2705 SLAs: Guaranteed uptime and performance - \u2705 No maintenance: Provider handles infrastructure, updates, optimization - \u2705 Enterprise features: Multi-tenancy, access controls, monitoring</p> <p>Cons: - \u274c Cost: Pay per token (can add up at scale) - \u274c Data privacy: Data sent to cloud (though providers like IBM offer security guarantees) - \u274c Latency: Network overhead for each request - \u274c Less control: Dependent on provider's model versions and availability</p> <p>Best for: - Production applications at scale - Teams without deep ML infrastructure expertise - Regulated industries needing governance - Applications requiring multiple models</p> <p>watsonx.ai specifically offers: - IBM Granite models optimized for enterprise - Built-in governance and compliance tracking - Integration with IBM Cloud services - Prompt template management - Model deployment and monitoring</p>"},{"location":"tracks/day1-llm/llm-concepts/#cost-resource-considerations","title":"Cost &amp; Resource Considerations","text":""},{"location":"tracks/day1-llm/llm-concepts/#gpu-vs-cpu","title":"GPU vs CPU","text":"<p>GPU (Graphics Processing Unit): - Designed for parallel computations - Essential for training LLMs - Greatly accelerates inference (10-100x faster than CPU) - Cost: $1,000 - $10,000+ per card (consumer to enterprise)</p> <p>CPU (Central Processing Unit): - General-purpose computing - Can run small models (1-3B parameters) acceptably - Struggles with larger models (13B+) - Cost: Cheaper, already available in most systems</p> <p>Memory requirements: - Rough estimate: Model needs ~2 bytes per parameter (for FP16) - Example: 13B model needs ~26 GB GPU memory</p>"},{"location":"tracks/day1-llm/llm-concepts/#cloud-cost-dimensions","title":"Cloud Cost Dimensions","text":"<p>When using managed services like watsonx.ai:</p> <p>Token-based pricing: - Input tokens: Text you send - Output tokens: Text generated - Typically: $0.0001 - $0.001 per token (varies by model)</p> <p>Example calculation: <pre><code>Prompt: 1,000 tokens\nResponse: 500 tokens\nCost: (1000 + 500) \u00d7 $0.0002 = $0.30 per request\n</code></pre></p> <p>Cost optimization strategies: - Use smaller models when appropriate - Cache common responses - Implement prompt compression - Use batch processing for non-real-time workloads - Set max_tokens limits to control costs</p>"},{"location":"tracks/day1-llm/llm-concepts/#where-the-accelerator-fits-architecturally","title":"Where the Accelerator Fits Architecturally","text":"<p>Throughout this workshop, we'll reference the RAG Accelerator\u2014a production-ready skeleton for building LLM applications. Here's how it's structured:</p>"},{"location":"tracks/day1-llm/llm-concepts/#core-architecture","title":"Core Architecture","text":"<pre><code>accelerator/\n\u251c\u2500\u2500 rag/                    # RAG core logic\n\u2502   \u251c\u2500\u2500 pipeline.py        # Orchestrates retrieval + LLM\n\u2502   \u251c\u2500\u2500 retriever.py       # Vector DB queries (Elasticsearch/Chroma)\n\u2502   \u251c\u2500\u2500 prompt.py          # Shared prompt templates\n\u2502   \u2514\u2500\u2500 embedder.py        # Text embedding logic\n\u251c\u2500\u2500 service/               # Production API\n\u2502   \u251c\u2500\u2500 api.py            # FastAPI microservice (POST /ask)\n\u2502   \u251c\u2500\u2500 deps.py           # Configuration &amp; dependencies\n\u2502   \u2514\u2500\u2500 models.py         # Request/response schemas\n\u251c\u2500\u2500 tools/                 # CLI utilities\n\u2502   \u251c\u2500\u2500 chunk.py          # Document chunking\n\u2502   \u251c\u2500\u2500 extract.py        # Text extraction from PDFs, docs\n\u2502   \u251c\u2500\u2500 embed_index.py    # Embedding and indexing pipeline\n\u2502   \u2514\u2500\u2500 eval_small.py     # Evaluation harness\n\u251c\u2500\u2500 ui/                    # User interface\n\u2502   \u2514\u2500\u2500 app.py            # Streamlit front-end\n\u2514\u2500\u2500 config.yaml           # Central configuration\n</code></pre>"},{"location":"tracks/day1-llm/llm-concepts/#how-llms-fit-in","title":"How LLMs Fit In","text":"<p>On Day 1, we're focusing on pure LLM behavior (no retrieval). This maps to:</p> <p>Current state (Day 1): <pre><code># pipeline.py (simplified)\ndef answer_question(question: str) -&gt; str:\n    # Direct LLM call\n    response = llm.generate(prompt=question)\n    return response\n</code></pre></p> <p>Future state (Day 2-3): <pre><code># pipeline.py (with RAG)\ndef answer_question(question: str) -&gt; str:\n    # 1. Retrieve relevant context\n    context = retriever.search(question, top_k=5)\n\n    # 2. Build prompt with context\n    prompt = prompt_template.format(\n        question=question,\n        context=context\n    )\n\n    # 3. Generate answer\n    response = llm.generate(prompt=prompt)\n    return response\n</code></pre></p>"},{"location":"tracks/day1-llm/llm-concepts/#key-integration-points","title":"Key Integration Points","text":"<ol> <li>Model Selection (<code>service/deps.py</code>):</li> <li>Switches between Ollama and watsonx.ai</li> <li> <p>Manages credentials and endpoints</p> </li> <li> <p>Prompt Engineering (<code>rag/prompt.py</code>):</p> </li> <li>System instructions</li> <li>Few-shot examples</li> <li> <p>Context injection patterns</p> </li> <li> <p>Response Handling (<code>rag/pipeline.py</code>):</p> </li> <li>Parse structured outputs</li> <li>Extract citations</li> <li>Handle errors</li> </ol>"},{"location":"tracks/day1-llm/llm-concepts/#reference-notebooks","title":"Reference Notebooks","text":"<p>The workshop includes several reference notebooks that show LLMs in production contexts:</p>"},{"location":"tracks/day1-llm/llm-concepts/#rag-examples-labs-src","title":"RAG Examples (<code>labs-src/</code>)","text":"<ul> <li><code>use-watsonx-elasticsearch-and-langchain-to-answer-questions-rag.ipynb</code></li> <li>Full RAG pipeline with Elasticsearch</li> <li> <p>Shows prompt structure with context</p> </li> <li> <p><code>use-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb</code></p> </li> <li>Alternative vector DB (Chroma)</li> <li>LangChain integration patterns</li> </ul>"},{"location":"tracks/day1-llm/llm-concepts/#accelerator-notebooks-acceleratorassetsnotebook","title":"Accelerator Notebooks (<code>accelerator/assets/notebook/</code>)","text":"<ul> <li><code>QnA_with_RAG.ipynb</code></li> <li>End-to-end Q&amp;A with retrieval</li> <li> <p>Prompt engineering for RAG</p> </li> <li> <p><code>Create_and_Deploy_QnA_AI_Service.ipynb</code></p> </li> <li>Deploy RAG service to production</li> <li>API endpoint creation</li> </ul> <p>How to use these: - Don't run them line-by-line on Day 1 - Do open them to see:   - How prompts are structured   - How LLM calls are instrumented   - How outputs are validated</p>"},{"location":"tracks/day1-llm/llm-concepts/#how-this-connects-to-the-labs","title":"How This Connects to the Labs","text":""},{"location":"tracks/day1-llm/llm-concepts/#day-1-labs-today","title":"Day 1 Labs (Today)","text":"<ul> <li>Lab 1.1: Quick start with both Ollama and watsonx</li> <li>Focus: Basic LLM calls, parameter tuning</li> <li> <p>No retrieval, just prompts \u2192 responses</p> </li> <li> <p>Lab 1.2: Prompt templates</p> </li> <li>Build reusable prompt patterns</li> <li> <p>Compare behavior across backends</p> </li> <li> <p>Lab 1.3: Micro-evaluation</p> </li> <li>Rate LLM outputs</li> <li>Build a simple evaluation framework</li> </ul>"},{"location":"tracks/day1-llm/llm-concepts/#day-2-3-labs-upcoming","title":"Day 2-3 Labs (Upcoming)","text":"<ul> <li>Add retrieval (RAG)</li> <li>Integrate with the accelerator</li> <li>Build production-ready pipelines</li> <li>Add orchestration and agents</li> </ul> <p>Mental model: - Day 1 = Understanding the LLM building block - Day 2 = LLM + retrieval (RAG) - Day 3 = LLM + retrieval + orchestration (agents)</p>"},{"location":"tracks/day1-llm/llm-concepts/#further-reading","title":"Further Reading","text":""},{"location":"tracks/day1-llm/llm-concepts/#official-documentation","title":"Official Documentation","text":"<ul> <li>IBM Granite Models</li> <li>watsonx.ai Documentation</li> <li>Ollama Documentation</li> </ul>"},{"location":"tracks/day1-llm/llm-concepts/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>OpenAI Prompt Engineering Guide</li> <li>Anthropic Prompt Engineering</li> <li>Granite Prompting Guide</li> </ul>"},{"location":"tracks/day1-llm/llm-concepts/#llm-concepts","title":"LLM Concepts","text":"<ul> <li>Hugging Face NLP Course</li> <li>LLM Training &amp; Inference</li> <li>Understanding Tokenization</li> </ul>"},{"location":"tracks/day1-llm/llm-concepts/#responsible-ai","title":"Responsible AI","text":"<ul> <li>IBM AI Ethics</li> <li>Guardrails for LLMs</li> </ul>"},{"location":"tracks/day1-llm/llm-concepts/#summary","title":"Summary","text":"<p>You now understand:</p> <ul> <li>\u2705 What LLMs are and how they work at a high level</li> <li>\u2705 Key concepts: tokens, context windows, temperature, top-k/top-p</li> <li>\u2705 Trade-offs between local and managed deployments</li> <li>\u2705 Cost considerations for LLM applications</li> <li>\u2705 How LLMs fit into the accelerator architecture</li> </ul> <p>Next: Let's get hands-on with Lab 1.1 and actually run some prompts!</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/","title":"1.2 Prompt Patterns &amp; Templates","text":"<p>Understanding how to structure prompts effectively is crucial for getting reliable, high-quality outputs from LLMs. This module covers common prompt patterns and how to build reusable templates.</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will:</p> <ul> <li>Recognize common prompt patterns and when to use them</li> <li>Understand why structure matters in prompt engineering</li> <li>Know how to create reusable prompt templates</li> <li>See how the accelerator uses prompts in production</li> </ul>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#core-prompt-patterns","title":"Core Prompt Patterns","text":""},{"location":"tracks/day1-llm/prompt-patterns-theory/#1-instruction-prompts","title":"1. Instruction Prompts","text":"<p>The simplest pattern: give the model a clear instruction.</p> <p>Structure: <pre><code>[Instruction]\n</code></pre></p> <p>Examples: <pre><code>Summarize this text in 3 sentences.\n\nExtract all email addresses from the following document.\n\nTranslate this paragraph to French.\n</code></pre></p> <p>Best for: - Simple, well-defined tasks - When the model already knows what to do - Single-step operations</p> <p>Tips: - Be specific and direct - Use action verbs (summarize, extract, translate, list) - Specify output format if needed</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#2-few-shot-examples","title":"2. Few-Shot Examples","text":"<p>Provide examples of the task before asking the model to do it.</p> <p>Structure: <pre><code>[Instruction]\n\n[Example 1 Input]\n[Example 1 Output]\n\n[Example 2 Input]\n[Example 2 Output]\n\n[Your Input]\n</code></pre></p> <p>Example: <pre><code>Extract key entities from product reviews.\n\nReview: \"The new MacBook Pro is amazing! Great battery life.\"\nEntities: {\"product\": \"MacBook Pro\", \"sentiment\": \"positive\", \"feature\": \"battery life\"}\n\nReview: \"The iPhone camera is disappointing in low light.\"\nEntities: {\"product\": \"iPhone\", \"sentiment\": \"negative\", \"feature\": \"camera\"}\n\nReview: \"The Dell XPS has excellent build quality but the trackpad could be better.\"\nEntities:\n</code></pre></p> <p>Best for: - Tasks where the model needs clarification - Structured outputs (JSON, CSV) - Domain-specific tasks - Reducing hallucinations</p> <p>Tips: - Use 2-5 examples (more isn't always better) - Make examples diverse but representative - Ensure examples match your desired output format exactly</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#3-chain-of-thought-cot","title":"3. Chain-of-Thought (CoT)","text":"<p>Encourage the model to reason step-by-step before answering.</p> <p>Structure: <pre><code>[Problem]\n\nLet's think step by step:\n</code></pre></p> <p>Example: <pre><code>Question: A store has 42 apples. They sell 15 in the morning and 8 in the afternoon. \nHow many apples are left?\n\nLet's think step by step:\n1. Started with: 42 apples\n2. Sold in morning: 15 apples\n3. Remaining after morning: 42 - 15 = 27 apples\n4. Sold in afternoon: 8 apples\n5. Final remaining: 27 - 8 = 19 apples\n\nAnswer: 19 apples\n</code></pre></p> <p>Best for: - Math problems - Logical reasoning - Complex multi-step tasks - When you need to audit the reasoning process</p> <p>\u26a0\ufe0f Workshop Note: - We use CoT judiciously (it's powerful but can increase latency) - For production, consider whether you need the reasoning or just the answer - CoT uses more tokens = higher cost</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#4-style-transfer","title":"4. Style Transfer","text":"<p>Ask the model to rewrite content in a different style or tone.</p> <p>Structure: <pre><code>Rewrite the following [current style] text in a [target style] style:\n\n[Original text]\n</code></pre></p> <p>Examples: <pre><code>Rewrite this technical documentation in a casual, beginner-friendly style:\n\nOriginal: \"The API implements RESTful principles with JWT-based authentication...\"\nCasual: \"Our API is super easy to use! You just need to get a token first...\"\n\n---\n\nRewrite this casual email in a formal business tone:\n\nOriginal: \"Hey! Can you check out that bug? It's kinda urgent.\"\nFormal: \"Good afternoon. I would appreciate your prompt attention to the defect...\"\n</code></pre></p> <p>Best for: - Content adaptation - Marketing copy variations - Documentation at different reading levels</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#5-summarization-rewrite","title":"5. Summarization / Rewrite","text":"<p>Ask the model to condense or reformulate content.</p> <p>Common variants:</p> <p>Abstractive summary: <pre><code>Summarize the following article in 3 sentences:\n\n[Article text]\n</code></pre></p> <p>Extractive summary: <pre><code>Extract the 5 most important sentences from this document:\n\n[Document text]\n</code></pre></p> <p>Length-constrained: <pre><code>Summarize this in exactly 50 words:\n\n[Text]\n</code></pre></p> <p>Audience-specific: <pre><code>Summarize this technical paper for a non-technical executive audience (2 paragraphs):\n\n[Paper]\n</code></pre></p> <p>Best for: - Long documents - Content curation - Executive summaries - Social media posts from longer content</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#prompt-design-principles","title":"Prompt Design Principles","text":""},{"location":"tracks/day1-llm/prompt-patterns-theory/#1-clarity-and-specificity","title":"1. Clarity and Specificity","text":"<p>Bad: <pre><code>Tell me about AI.\n</code></pre></p> <p>Good: <pre><code>Explain the difference between supervised and unsupervised machine learning \nin 3 paragraphs, with one example of each.\n</code></pre></p> <p>Why it matters: Vague prompts lead to vague, unfocused responses.</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#2-role-and-persona","title":"2. Role and Persona","text":"<p>Give the model a role to frame its responses.</p> <p>Structure: <pre><code>You are a [role] with expertise in [domain].\n\n[Task]\n</code></pre></p> <p>Examples: <pre><code>You are a senior software architect with 15 years of experience in distributed systems.\n\nDesign a scalable architecture for a real-time chat application.\n\n---\n\nYou are a friendly customer support agent for a SaaS product.\n\nHelp the user understand why their payment failed.\n</code></pre></p> <p>Best for: - Setting the right tone - Accessing domain-specific knowledge - Consistency across conversations</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#3-constraints-and-formatting","title":"3. Constraints and Formatting","text":"<p>Explicitly state output requirements.</p> <p>Examples: <pre><code>List 5 key findings. Use bullet points. Keep each point under 20 words.\n\n---\n\nRespond in valid JSON format with keys: title, summary, tags (array).\n\n---\n\nYour response must be under 100 words and appropriate for a general audience.\n</code></pre></p> <p>Why it matters: LLMs will follow format constraints if clearly stated.</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#4-providing-context-and-examples","title":"4. Providing Context and Examples","text":"<p>More context = better responses.</p> <p>Insufficient context: <pre><code>Fix this bug.\n</code></pre></p> <p>Better: <pre><code>I have a Python function that's supposed to calculate discounts but returns negative values.\nHere's the code:\n\n[code]\n\nExpected behavior: discount should be between 0-100.\nActual behavior: discount is -25.\n\nPlease identify the bug and provide a corrected version.\n</code></pre></p> <p>When to provide context: - Always for technical tasks - When the model needs background information - For domain-specific questions</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#prompt-templates","title":"Prompt Templates","text":""},{"location":"tracks/day1-llm/prompt-patterns-theory/#what-is-a-template","title":"What is a Template?","text":"<p>A prompt template is a reusable pattern with placeholders for variable content.</p> <p>Benefits: - Consistency: Same structure every time - Maintainability: Update once, apply everywhere - Testability: Easier to evaluate prompt changes - Scalability: Supports batch processing</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#simple-python-templates","title":"Simple Python Templates","text":"<p>Using f-strings: <pre><code>def summarize(text: str, length: int = 3) -&gt; str:\n    prompt = f\"\"\"Summarize the following text in {length} sentences:\n\n{text}\n\nSummary:\"\"\"\n    return llm.generate(prompt)\n</code></pre></p> <p>Using str.format(): <pre><code>TEMPLATE = \"\"\"You are a helpful assistant.\n\nTask: {task}\n\nInput: {input_text}\n\nOutput format: {output_format}\n\"\"\"\n\nprompt = TEMPLATE.format(\n    task=\"Extract entities\",\n    input_text=\"IBM released watsonx.ai in 2023.\",\n    output_format=\"JSON with keys: organization, product, year\"\n)\n</code></pre></p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#langchain-templates","title":"LangChain Templates","text":"<p>LangChain provides more sophisticated templating:</p> <pre><code>from langchain.prompts import PromptTemplate\n\ntemplate = PromptTemplate(\n    input_variables=[\"task\", \"examples\", \"input\"],\n    template=\"\"\"You are an AI assistant specializing in {task}.\n\nHere are some examples:\n{examples}\n\nNow, process this input:\n{input}\n\"\"\"\n)\n\nprompt = template.format(\n    task=\"sentiment analysis\",\n    examples=\"Positive: 'Great product!'\\nNegative: 'Terrible experience.'\",\n    input=\"The service was okay.\"\n)\n</code></pre> <p>Advanced: ChatPromptTemplate: <pre><code>from langchain.prompts import ChatPromptTemplate\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant specialized in {domain}.\"),\n    (\"human\", \"{user_input}\"),\n])\n\nmessages = template.format_messages(\n    domain=\"data analysis\",\n    user_input=\"How do I calculate correlation?\"\n)\n</code></pre></p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#how-the-accelerator-uses-prompts","title":"How the Accelerator Uses Prompts","text":"<p>The accelerator centralizes prompt logic in <code>rag/prompt.py</code>:</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#current-structure","title":"Current Structure","text":"<pre><code># accelerator/rag/prompt.py\n\nSYSTEM = \"\"\"You are a careful and accurate assistant.\nYou answer questions based on provided context.\nIf you cannot find the answer in the context, say so.\"\"\"\n\nUSER_TEMPLATE = \"\"\"Context:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n</code></pre>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#on-day-2-3-youll-extend-this","title":"On Day 2-3, You'll Extend This","text":"<p>Multi-turn conversations: <pre><code>CHAT_TEMPLATE = \"\"\"You are a helpful assistant. Use the following context to answer questions.\n\nContext:\n{context}\n\nConversation history:\n{history}\n\nUser: {question}\nAssistant:\"\"\"\n</code></pre></p> <p>Citation formatting: <pre><code>USER_TEMPLATE_WITH_CITATIONS = \"\"\"Context:\n{context}\n\nQuestion: {question}\n\nAnswer the question and cite your sources using [1], [2], etc.\n\nAnswer:\"\"\"\n</code></pre></p> <p>Safety guidance: <pre><code>SYSTEM_WITH_SAFETY = \"\"\"You are a helpful and safe assistant.\n- Base answers on provided context only\n- Do not generate harmful, biased, or inappropriate content\n- If asked to do something outside your scope, politely decline\n- Cite sources when available\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n</code></pre></p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#integration-points","title":"Integration Points","text":"<p>In <code>pipeline.py</code>: <pre><code>from .prompt import SYSTEM, USER_TEMPLATE\n\ndef answer_question(question: str, context: List[str]) -&gt; str:\n    # Format context\n    context_str = \"\\n\\n\".join([f\"[{i+1}] {doc}\" for i, doc in enumerate(context)])\n\n    # Build prompt\n    user_prompt = USER_TEMPLATE.format(\n        context=context_str,\n        question=question\n    )\n\n    # Generate\n    response = llm.generate(\n        system=SYSTEM,\n        prompt=user_prompt\n    )\n\n    return response\n</code></pre></p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#reference-notebooks","title":"Reference Notebooks","text":""},{"location":"tracks/day1-llm/prompt-patterns-theory/#rag-prompt-examples","title":"RAG Prompt Examples","text":"<p><code>use-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb</code>: <pre><code># Example from the notebook\nfrom langchain.prompts import PromptTemplate\n\nrag_prompt = PromptTemplate(\n    template=\"\"\"Use the following pieces of context to answer the question at the end.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:\"\"\",\n    input_variables=[\"context\", \"question\"],\n)\n</code></pre></p> <p><code>QnA_with_RAG.ipynb</code> (accelerator): - Shows how context is concatenated - Demonstrates citation patterns - Includes error handling for edge cases</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#key-observations","title":"Key Observations","text":"<ol> <li>Context injection: Always happens before the question</li> <li>Structured formats: JSON/XML for tool calling</li> <li>Safety prompts: Explicitly state boundaries</li> <li>Few-shot in RAG: Sometimes examples are included to show citation format</li> </ol>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#examples-to-reuse-in-lab-12","title":"Examples to Reuse in Lab 1.2","text":""},{"location":"tracks/day1-llm/prompt-patterns-theory/#example-1-summarization","title":"Example 1: Summarization","text":"<p>Template: <pre><code>SUMMARIZE_TEMPLATE = \"\"\"Summarize the following text in {num_sentences} sentences.\nFocus on the main points and key takeaways.\n\nText:\n{text}\n\nSummary:\"\"\"\n</code></pre></p> <p>Usage: <pre><code>prompt = SUMMARIZE_TEMPLATE.format(\n    num_sentences=3,\n    text=\"[Your long text here]\"\n)\n</code></pre></p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#example-2-style-transfer","title":"Example 2: Style Transfer","text":"<p>Template: <pre><code>REWRITE_TEMPLATE = \"\"\"Rewrite the following text in a {target_tone} tone:\n\nOriginal text:\n{original}\n\nRewritten text:\"\"\"\n</code></pre></p> <p>Usage: <pre><code>prompt = REWRITE_TEMPLATE.format(\n    target_tone=\"formal business\",\n    original=\"Hey, can you check that out?\"\n)\n</code></pre></p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#example-3-qa-with-context","title":"Example 3: Q&amp;A with Context","text":"<p>Template: <pre><code>QA_TEMPLATE = \"\"\"Based on the following information, answer the question.\nIf the information doesn't contain the answer, say \"I don't have enough information.\"\n\nInformation:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n</code></pre></p> <p>Usage: <pre><code>prompt = QA_TEMPLATE.format(\n    context=\"watsonx.ai was released by IBM in 2023...\",\n    question=\"When was watsonx.ai released?\"\n)\n</code></pre></p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#connection-to-labs","title":"Connection to Labs","text":""},{"location":"tracks/day1-llm/prompt-patterns-theory/#lab-12-prompt-templates","title":"Lab 1.2: Prompt Templates","text":"<p>In this lab, you'll:</p> <ol> <li>Create templates for:</li> <li>Summarization</li> <li>Style rewrite</li> <li> <p>Q&amp;A with context</p> </li> <li> <p>Implement in both environments:</p> </li> <li><code>prompt_patterns_ollama.ipynb</code> (local)</li> <li> <p><code>prompt_patterns_watsonx.ipynb</code> (managed)</p> </li> <li> <p>Compare results:</p> </li> <li>Same template, different models</li> <li>Measure quality and consistency</li> </ol>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#looking-ahead","title":"Looking Ahead","text":"<p>Day 2: These templates become the foundation for RAG prompts Day 3: Templates extended for multi-turn agents and tool calling</p>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"tracks/day1-llm/prompt-patterns-theory/#do","title":"\u2705 Do","text":"<ul> <li>Start with simple, clear instructions</li> <li>Use few-shot examples for structured outputs</li> <li>Specify output format explicitly</li> <li>Test prompts with edge cases</li> <li>Version control your templates</li> <li>Measure prompt performance systematically</li> </ul>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#dont","title":"\u274c Don't","text":"<ul> <li>Use vague or ambiguous language</li> <li>Mix multiple tasks in one prompt</li> <li>Assume the model knows your context</li> <li>Forget to handle error cases</li> <li>Over-engineer prompts prematurely</li> </ul>"},{"location":"tracks/day1-llm/prompt-patterns-theory/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Prompts are code: Treat them with the same rigor as application code</li> <li>Structure matters: Well-structured prompts are more reliable</li> <li>Templates enable scale: Reusable patterns save time and improve consistency</li> <li>Test and iterate: Prompt engineering is empirical\u2014what works for one task may not work for another</li> </ul> <p>Next: Time to build these patterns hands-on in Lab 1.2!</p>"},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/","title":"Day 2 RAG Workshop - Complete Code &amp; Solutions Guide","text":""},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Theory Summary</li> <li>Lab 2.1: Local RAG with Ollama - Complete Solution</li> <li>Lab 2.2: RAG with watsonx.ai - Complete Solution</li> <li>Lab 2.3: Twin RAG Pipelines - Complete Solution</li> <li>Lab 2.4: Evaluation Harness - Complete Solution</li> <li>Production Deployment Code</li> <li>Troubleshooting Guide</li> </ol>"},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#theory-summary","title":"Theory Summary","text":""},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#rag-core-concepts","title":"RAG Core Concepts","text":"<pre><code>\"\"\"\nRAG Pipeline Flow:\n1. INGESTION: Documents \u2192 Text Extraction \u2192 Chunking\n2. INDEXING: Chunks \u2192 Embeddings \u2192 Vector Store\n3. RETRIEVAL: Query \u2192 Embedding \u2192 Similarity Search \u2192 Relevant Chunks\n4. GENERATION: Query + Context \u2192 LLM \u2192 Answer\n\"\"\"\n\n# Key Components\nCOMPONENTS = {\n    \"Document Store\": \"Repository of knowledge (PDFs, text, web pages)\",\n    \"Chunker\": \"Splits documents into semantic units\",\n    \"Embeddings\": \"Dense vector representations of text\",\n    \"Vector Store\": \"Database optimized for similarity search\",\n    \"Retriever\": \"Finds relevant chunks for a query\",\n    \"LLM\": \"Generates answers from retrieved context\",\n    \"Prompt\": \"Template combining context and question\"\n}\n\n# Popular Tools\nTOOLS = {\n    \"Embeddings\": [\"all-MiniLM-L6-v2\", \"text-embedding-ada-002\", \"slate-30m\"],\n    \"Vector Stores\": [\"Chroma\", \"FAISS\", \"Elasticsearch\", \"Pinecone\"],\n    \"LLMs\": [\"Ollama (llama2, mistral)\", \"watsonx.ai (Granite)\", \"OpenAI (GPT)\"]\n}\n</code></pre>"},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#lab-21-solution-local-rag-with-ollama","title":"Lab 2.1 Solution: Local RAG with Ollama","text":""},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#complete-working-code","title":"Complete Working Code","text":"<pre><code># ============================================================================\n# LAB 2.1: LOCAL RAG WITH OLLAMA - COMPLETE SOLUTION\n# ============================================================================\n\n# Cell 1: Setup and Imports\n\"\"\"\nInstall required packages:\npip install langchain langchain-community chromadb sentence-transformers ollama\n\"\"\"\n\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\nfrom langchain.text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.llms import Ollama\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nimport pandas as pd\nfrom pathlib import Path\nimport time\n\n# Cell 2: Configuration\nCONFIG = {\n    \"corpus_path\": \"data/corpus\",\n    \"chunk_size\": 1000,\n    \"chunk_overlap\": 200,\n    \"embedding_model\": \"all-MiniLM-L6-v2\",\n    \"llm_model\": \"llama2\",\n    \"vector_db_path\": \"./chroma_db\",\n    \"top_k\": 5,\n    \"temperature\": 0.0\n}\n\n# Cell 3: Prepare Sample Corpus\ndef create_sample_corpus():\n    \"\"\"Create sample documents for testing\"\"\"\n    corpus_dir = Path(CONFIG[\"corpus_path\"])\n    corpus_dir.mkdir(parents=True, exist_ok=True)\n\n    documents = {\n        \"rag_overview.txt\": \"\"\"\n# Retrieval Augmented Generation Overview\n\nRetrieval Augmented Generation (RAG) is a technique that enhances Large Language Models \nby providing them with relevant external knowledge retrieved from a document corpus.\n\n## Key Benefits\n1. Reduces hallucinations by grounding responses in factual data\n2. Enables access to current information beyond training cutoff\n3. Allows domain-specific knowledge without retraining\n4. Provides source attribution for transparency\n\n## Core Components\n- Document Store: Repository of knowledge\n- Embeddings: Convert text to vectors\n- Vector Store: Database for similarity search\n- Retriever: Finds relevant documents\n- LLM: Generates final answer\n\"\"\",\n        \"chunking_strategies.txt\": \"\"\"\n# Chunking Strategies in RAG\n\nChunking is the process of splitting documents into smaller units for better retrieval.\n\n## Fixed-Size Chunking\n- Split by character or token count\n- Simple but may break semantic units\n- Typical size: 500-1500 tokens\n\n## Semantic Chunking\n- Split by paragraphs or sections\n- Preserves meaning and context\n- Better for coherent retrieval\n\n## Recursive Chunking\n- Tries multiple separators in order\n- First tries paragraphs, then sentences, then words\n- Best of both worlds\n\n## Best Practices\n- Include overlap between chunks (10-20%)\n- Preserve metadata with each chunk\n- Test different sizes for your use case\n\"\"\",\n        \"vector_stores.txt\": \"\"\"\n# Vector Stores for RAG\n\nVector stores are specialized databases for storing and searching embeddings.\n\n## Chroma\n- Lightweight and easy to use\n- Great for local development\n- Persistent storage option\n- Open source\n\n## Elasticsearch\n- Enterprise-grade and scalable\n- Supports hybrid search (keyword + vector)\n- Production-ready\n- Distributed architecture\n\n## FAISS\n- High-performance similarity search\n- In-memory or disk-based\n- From Facebook AI Research\n- Excellent for large-scale\n\n## Pinecone\n- Fully managed cloud service\n- Auto-scaling\n- Simple API\n- Pay-as-you-go pricing\n\"\"\"\n    }\n\n    for filename, content in documents.items():\n        with open(corpus_dir / filename, \"w\") as f:\n            f.write(content)\n\n    print(f\"Created {len(documents)} sample documents in {corpus_dir}\")\n    return corpus_dir\n\n# Create corpus\ncorpus_path = create_sample_corpus()\n\n# Cell 4: Load Documents\nprint(\"=\" * 80)\nprint(\"STEP 1: LOADING DOCUMENTS\")\nprint(\"=\" * 80)\n\nloader = DirectoryLoader(\n    CONFIG[\"corpus_path\"],\n    glob=\"**/*.txt\",\n    loader_cls=TextLoader\n)\n\ndocuments = loader.load()\nprint(f\"\u2713 Loaded {len(documents)} documents\")\n\nfor i, doc in enumerate(documents):\n    print(f\"\\nDocument {i+1}:\")\n    print(f\"  Source: {doc.metadata['source']}\")\n    print(f\"  Length: {len(doc.page_content)} characters\")\n    print(f\"  Preview: {doc.page_content[:100]}...\")\n\n# Cell 5: Chunk Documents\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STEP 2: CHUNKING DOCUMENTS\")\nprint(\"=\" * 80)\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=CONFIG[\"chunk_size\"],\n    chunk_overlap=CONFIG[\"chunk_overlap\"],\n    length_function=len,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n)\n\nchunks = text_splitter.split_documents(documents)\nprint(f\"\u2713 Created {len(chunks)} chunks from {len(documents)} documents\")\n\n# Analyze chunks\nchunk_lengths = [len(chunk.page_content) for chunk in chunks]\nprint(f\"\\nChunk Statistics:\")\nprint(f\"  Average length: {sum(chunk_lengths)/len(chunk_lengths):.0f} chars\")\nprint(f\"  Min length: {min(chunk_lengths)} chars\")\nprint(f\"  Max length: {max(chunk_lengths)} chars\")\n\n# Show sample chunk\nprint(f\"\\nSample Chunk:\")\nprint(f\"  Content: {chunks[0].page_content[:200]}...\")\nprint(f\"  Metadata: {chunks[0].metadata}\")\n\n# Cell 6: Create Embeddings\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STEP 3: CREATING EMBEDDINGS\")\nprint(\"=\" * 80)\n\nembeddings = HuggingFaceEmbeddings(\n    model_name=CONFIG[\"embedding_model\"],\n    model_kwargs={'device': 'cpu'},\n    encode_kwargs={'normalize_embeddings': True}\n)\n\n# Test embedding\ntest_text = \"What is RAG?\"\ntest_embedding = embeddings.embed_query(test_text)\nprint(f\"\u2713 Embedding model loaded: {CONFIG['embedding_model']}\")\nprint(f\"  Embedding dimension: {len(test_embedding)}\")\nprint(f\"  Sample values: {test_embedding[:5]}\")\n\n# Cell 7: Build Vector Store\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STEP 4: BUILDING VECTOR STORE\")\nprint(\"=\" * 80)\n\nstart_time = time.time()\n\nvectorstore = Chroma.from_documents(\n    documents=chunks,\n    embedding=embeddings,\n    persist_directory=CONFIG[\"vector_db_path\"]\n)\n\nbuild_time = time.time() - start_time\nprint(f\"\u2713 Vector store created in {build_time:.2f} seconds\")\nprint(f\"  Total vectors: {vectorstore._collection.count()}\")\nprint(f\"  Storage path: {CONFIG['vector_db_path']}\")\n\n# Persist\nvectorstore.persist()\nprint(\"\u2713 Vector store persisted to disk\")\n\n# Cell 8: Test Retrieval\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STEP 5: TESTING RETRIEVAL\")\nprint(\"=\" * 80)\n\ntest_queries = [\n    \"What is RAG?\",\n    \"How does chunking work?\",\n    \"What are the benefits of using vector stores?\"\n]\n\nfor query in test_queries:\n    print(f\"\\n Query: {query}\")\n    results = vectorstore.similarity_search_with_score(query, k=2)\n\n    for i, (doc, score) in enumerate(results, 1):\n        print(f\"  Result {i} (score: {score:.3f}):\")\n        print(f\"    {doc.page_content[:150]}...\")\n        print(f\"    Source: {doc.metadata['source']}\")\n\n# Cell 9: Initialize LLM\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STEP 6: INITIALIZING LLM\")\nprint(\"=\" * 80)\n\ntry:\n    llm = Ollama(\n        model=CONFIG[\"llm_model\"],\n        temperature=CONFIG[\"temperature\"]\n    )\n\n    # Test LLM\n    test_response = llm(\"Say 'Hello, I am ready!'\")\n    print(f\"\u2713 LLM initialized: {CONFIG['llm_model']}\")\n    print(f\"  Test response: {test_response}\")\nexcept Exception as e:\n    print(f\"\u2717 Error initializing LLM: {e}\")\n    print(\"  Make sure Ollama is running: ollama serve\")\n    print(f\"  And model is pulled: ollama pull {CONFIG['llm_model']}\")\n\n# Cell 10: Build RAG Pipeline\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STEP 7: BUILDING RAG PIPELINE\")\nprint(\"=\" * 80)\n\n# Custom prompt template\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\nIf you don't know the answer based on the context, just say that you don't know, don't try to make up an answer.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer: \"\"\"\n\nPROMPT = PromptTemplate(\n    template=prompt_template,\n    input_variables=[\"context\", \"question\"]\n)\n\n# Create QA chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(\n        search_kwargs={\"k\": CONFIG[\"top_k\"]}\n    ),\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": PROMPT}\n)\n\nprint(\"\u2713 RAG pipeline built successfully\")\n\n# Cell 11: Answer Questions\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STEP 8: TESTING RAG PIPELINE\")\nprint(\"=\" * 80)\n\ndef answer_question(query: str, verbose: bool = True):\n    \"\"\"Answer a question using the RAG pipeline\"\"\"\n    start_time = time.time()\n\n    result = qa_chain({\"query\": query})\n\n    elapsed_time = time.time() - start_time\n\n    if verbose:\n        print(f\"\\n{'='*80}\")\n        print(f\"Question: {query}\")\n        print(f\"{'='*80}\")\n        print(f\"\\nAnswer:\\n{result['result']}\")\n        print(f\"\\nSources:\")\n        for i, doc in enumerate(result['source_documents'], 1):\n            print(f\"  {i}. {doc.metadata['source']}\")\n            print(f\"     {doc.page_content[:100]}...\\n\")\n        print(f\"Response time: {elapsed_time:.2f} seconds\")\n\n    return {\n        \"question\": query,\n        \"answer\": result['result'],\n        \"sources\": [doc.metadata['source'] for doc in result['source_documents']],\n        \"num_sources\": len(result['source_documents']),\n        \"response_time\": elapsed_time\n    }\n\n# Test questions\ntest_questions = [\n    \"What is Retrieval Augmented Generation?\",\n    \"What are the main benefits of RAG?\",\n    \"Explain different chunking strategies\",\n    \"What is Chroma and when should I use it?\",\n    \"What are the core components of RAG?\"\n]\n\nresults = []\nfor question in test_questions:\n    result = answer_question(question)\n    results.append(result)\n\n# Cell 12: Analyze Results\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STEP 9: ANALYZING RESULTS\")\nprint(\"=\" * 80)\n\nresults_df = pd.DataFrame(results)\n\nprint(\"\\nSummary Statistics:\")\nprint(f\"  Total questions: {len(results)}\")\nprint(f\"  Average response time: {results_df['response_time'].mean():.2f} seconds\")\nprint(f\"  Average sources per answer: {results_df['num_sources'].mean():.1f}\")\n\n# Save results\nresults_df.to_csv(\"lab_2.1_results.csv\", index=False)\nprint(f\"\\n\u2713 Results saved to lab_2.1_results.csv\")\n\n# Cell 13: Evaluation Function\ndef evaluate_answer(question: str, answer: str, expected_keywords: list):\n    \"\"\"\n    Simple evaluation: check if answer contains expected keywords\n\n    Args:\n        question: The query\n        answer: Generated answer\n        expected_keywords: List of keywords that should appear\n\n    Returns:\n        dict with evaluation metrics\n    \"\"\"\n    answer_lower = answer.lower()\n    found_keywords = [kw for kw in expected_keywords if kw.lower() in answer_lower]\n\n    score = len(found_keywords) / len(expected_keywords) if expected_keywords else 0\n\n    return {\n        \"question\": question,\n        \"contains_all_keywords\": len(found_keywords) == len(expected_keywords),\n        \"keyword_coverage\": score,\n        \"found_keywords\": found_keywords,\n        \"missing_keywords\": [kw for kw in expected_keywords if kw not in found_keywords]\n    }\n\n# Example evaluation\neval_result = evaluate_answer(\n    question=\"What is RAG?\",\n    answer=results[0]['answer'],\n    expected_keywords=[\"retrieval\", \"generation\", \"language model\", \"knowledge\"]\n)\n\nprint(\"\\nEvaluation Example:\")\nprint(f\"  Keyword coverage: {eval_result['keyword_coverage']:.0%}\")\nprint(f\"  Found: {eval_result['found_keywords']}\")\nprint(f\"  Missing: {eval_result['missing_keywords']}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LAB 2.1 COMPLETE! \u2705\")\nprint(\"=\" * 80)\nprint(\"\\nKey Achievements:\")\nprint(\"  \u2713 Loaded and chunked documents\")\nprint(\"  \u2713 Created embeddings and vector store\")\nprint(\"  \u2713 Built working RAG pipeline\")\nprint(\"  \u2713 Tested with multiple questions\")\nprint(\"  \u2713 Evaluated answer quality\")\nprint(\"\\nNext: Lab 2.2 - RAG with watsonx.ai\")\n</code></pre>"},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#lab-22-solution-rag-with-watsonxai","title":"Lab 2.2 Solution: RAG with watsonx.ai","text":""},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#complete-working-code_1","title":"Complete Working Code","text":"<pre><code># ============================================================================\n# LAB 2.2: RAG WITH WATSONX.AI - COMPLETE SOLUTION\n# ============================================================================\n\n# Cell 1: Setup and Imports\n\"\"\"\nInstall required packages:\npip install ibm-watsonx-ai langchain langchain-ibm\npip install elasticsearch langchain-elasticsearch\npip install python-dotenv\n\"\"\"\n\nimport os\nimport getpass\nfrom dotenv import load_dotenv\nfrom pathlib import Path\nimport time\nimport pandas as pd\n\nfrom ibm_watsonx_ai import APIClient, Credentials\nfrom ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, DecodingMethods\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n\nfrom langchain_ibm import WatsonxLLM, WatsonxEmbeddings\nfrom langchain.text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_elasticsearch import ElasticsearchStore\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\n# Cell 2: Configuration and Credentials\nprint(\"=\" * 80)\nprint(\"WATSONX.AI RAG SETUP\")\nprint(\"=\" * 80)\n\n# Load environment variables\nload_dotenv()\n\n# Prompt for credentials if not in environment\ndef get_credential(var_name: str, prompt_text: str):\n    value = os.getenv(var_name)\n    if not value:\n        value = getpass.getpass(f\"{prompt_text}: \")\n        os.environ[var_name] = value\n    return value\n\n# watsonx.ai credentials\nWATSONX_APIKEY = get_credential(\"WATSONX_APIKEY\", \"Enter your watsonx.ai API key\")\nPROJECT_ID = get_credential(\"PROJECT_ID\", \"Enter your watsonx.ai project ID\")\nWATSONX_URL = os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\")\n\n# Elasticsearch credentials (optional - can use Chroma instead)\nUSE_ELASTICSEARCH = input(\"Use Elasticsearch? (y/n, default=n): \").lower() == 'y'\n\nif USE_ELASTICSEARCH:\n    ES_URL = get_credential(\"ES_URL\", \"Enter Elasticsearch URL\")\n    ES_USER = get_credential(\"ES_USER\", \"Enter Elasticsearch username\")\n    ES_PASSWORD = get_credential(\"ES_PASSWORD\", \"Enter Elasticsearch password\")\n\nCONFIG = {\n    \"corpus_path\": \"data/corpus\",  # Reuse from Lab 2.1\n    \"chunk_size\": 1000,\n    \"chunk_overlap\": 200,\n    \"embedding_model\": \"ibm/slate-30m-english-rtrvr\",\n    \"llm_model\": ModelTypes.GRANITE_13B_CHAT_V2,\n    \"index_name\": \"watsonx_rag_index\",\n    \"top_k\": 5\n}\n\nprint(\"\\n\u2713 Configuration loaded\")\n\n# Cell 3: Initialize watsonx.ai Client\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INITIALIZING WATSONX.AI CLIENT\")\nprint(\"=\" * 80)\n\ncredentials = Credentials(\n    url=WATSONX_URL,\n    api_key=WATSONX_APIKEY\n)\n\napi_client = APIClient(\n    credentials=credentials,\n    project_id=PROJECT_ID\n)\n\nprint(\"\u2713 watsonx.ai client initialized\")\nprint(f\"  URL: {WATSONX_URL}\")\nprint(f\"  Project ID: {PROJECT_ID[:8]}...\")\n\n# Cell 4: Setup Embeddings\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SETTING UP EMBEDDINGS\")\nprint(\"=\" * 80)\n\nembeddings = WatsonxEmbeddings(\n    model_id=CONFIG[\"embedding_model\"],\n    url=WATSONX_URL,\n    apikey=WATSONX_APIKEY,\n    project_id=PROJECT_ID\n)\n\n# Test embedding\ntest_text = \"Granite is IBM's foundation model\"\ntest_embedding = embeddings.embed_query(test_text)\n\nprint(f\"\u2713 watsonx embeddings initialized\")\nprint(f\"  Model: {CONFIG['embedding_model']}\")\nprint(f\"  Embedding dimension: {len(test_embedding)}\")\n\n# Cell 5: Load and Chunk Documents\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LOADING AND CHUNKING DOCUMENTS\")\nprint(\"=\" * 80)\n\n# Load documents (reuse corpus from Lab 2.1)\nloader = DirectoryLoader(\n    CONFIG[\"corpus_path\"],\n    glob=\"**/*.txt\",\n    loader_cls=TextLoader\n)\n\ndocuments = loader.load()\nprint(f\"\u2713 Loaded {len(documents)} documents\")\n\n# Chunk documents\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=CONFIG[\"chunk_size\"],\n    chunk_overlap=CONFIG[\"chunk_overlap\"]\n)\n\nchunks = text_splitter.split_documents(documents)\nprint(f\"\u2713 Created {len(chunks)} chunks\")\n\n# Cell 6: Build Vector Store\nprint(\"\\n\" + \"=\" * 80)\nprint(\"BUILDING VECTOR STORE\")\nprint(\"=\" * 80)\n\nif USE_ELASTICSEARCH:\n    # Elasticsearch setup\n    vectorstore = ElasticsearchStore(\n        es_url=ES_URL,\n        index_name=CONFIG[\"index_name\"],\n        embedding=embeddings,\n        es_user=ES_USER,\n        es_password=ES_PASSWORD\n    )\n    vectorstore.add_documents(chunks)\n    print(f\"\u2713 Elasticsearch vector store created\")\n    print(f\"  Index: {CONFIG['index_name']}\")\nelse:\n    # Fallback to Chroma for local testing\n    from langchain_chroma import Chroma\n    vectorstore = Chroma.from_documents(\n        documents=chunks,\n        embedding=embeddings,\n        persist_directory=\"./chroma_watsonx_db\"\n    )\n    print(\"\u2713 Chroma vector store created (local)\")\n\n# Cell 7: Initialize Granite LLM\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INITIALIZING GRANITE MODEL\")\nprint(\"=\" * 80)\n\n# Model parameters\nmodel_parameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 200,\n    GenParams.STOP_SEQUENCES: [\"&lt;|endoftext|&gt;\"]\n}\n\n# Initialize Granite\ngranite_llm = WatsonxLLM(\n    model_id=CONFIG[\"llm_model\"].value,\n    url=WATSONX_URL,\n    apikey=WATSONX_APIKEY,\n    project_id=PROJECT_ID,\n    params=model_parameters\n)\n\n# Test Granite\ntest_response = granite_llm(\"What is 2+2? Answer with just the number.\")\nprint(\"\u2713 Granite model initialized\")\nprint(f\"  Model: {CONFIG['llm_model'].value}\")\nprint(f\"  Test response: {test_response}\")\n\n# Cell 8: Build RAG Pipeline\nprint(\"\\n\" + \"=\" * 80)\nprint(\"BUILDING RAG PIPELINE\")\nprint(\"=\" * 80)\n\n# Custom prompt for Granite\nprompt_template = \"\"\"&lt;|system|&gt;\nYou are a helpful AI assistant. Use the provided context to answer questions accurately.\nIf the answer is not in the context, say \"I don't have enough information to answer that.\"\n&lt;|endofsystem|&gt;\n\n&lt;|user|&gt;\nContext:\n{context}\n\nQuestion: {question}\n&lt;|endofuser|&gt;\n\n&lt;|assistant|&gt;\n\"\"\"\n\nPROMPT = PromptTemplate(\n    template=prompt_template,\n    input_variables=[\"context\", \"question\"]\n)\n\n# Create QA chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=granite_llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(\n        search_kwargs={\"k\": CONFIG[\"top_k\"]}\n    ),\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": PROMPT}\n)\n\nprint(\"\u2713 RAG pipeline with Granite built successfully\")\n\n# Cell 9: Test Pipeline\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TESTING WATSONX.AI RAG PIPELINE\")\nprint(\"=\" * 80)\n\ndef answer_with_granite(query: str):\n    \"\"\"Answer using watsonx.ai RAG pipeline\"\"\"\n    start_time = time.time()\n\n    result = qa_chain({\"query\": query})\n\n    elapsed_time = time.time() - start_time\n\n    print(f\"\\n{'='*80}\")\n    print(f\"Question: {query}\")\n    print(f\"{'='*80}\")\n    print(f\"\\nGranite Answer:\\n{result['result']}\")\n    print(f\"\\nSources:\")\n    for i, doc in enumerate(result['source_documents'], 1):\n        print(f\"  {i}. {doc.metadata['source']}\")\n    print(f\"\\nResponse time: {elapsed_time:.2f}s\")\n\n    return {\n        \"question\": query,\n        \"answer\": result['result'],\n        \"sources\": [d.metadata['source'] for d in result['source_documents']],\n        \"response_time\": elapsed_time,\n        \"model\": \"granite-13b-chat-v2\"\n    }\n\n# Test questions\ntest_questions = [\n    \"What is RAG and why is it useful?\",\n    \"Explain the different chunking strategies\",\n    \"Compare Chroma and Elasticsearch vector stores\"\n]\n\nwatsonx_results = []\nfor question in test_questions:\n    result = answer_with_granite(question)\n    watsonx_results.append(result)\n\n# Cell 10: Save Results\nresults_df = pd.DataFrame(watsonx_results)\nresults_df.to_csv(\"lab_2.2_watsonx_results.csv\", index=False)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LAB 2.2 COMPLETE! \u2705\")\nprint(\"=\" * 80)\nprint(\"\\nKey Achievements:\")\nprint(\"  \u2713 Connected to watsonx.ai\")\nprint(\"  \u2713 Used watsonx embeddings\")\nprint(\"  \u2713 Integrated Granite model\")\nprint(\"  \u2713 Built enterprise RAG pipeline\")\nprint(\"\\nNext: Lab 2.3 - Compare Ollama vs watsonx\")\n</code></pre>"},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#lab-23-solution-twin-rag-pipelines","title":"Lab 2.3 Solution: Twin RAG Pipelines","text":""},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#complete-comparison-framework","title":"Complete Comparison Framework","text":"<pre><code># ============================================================================\n# LAB 2.3: TWIN RAG PIPELINES - COMPLETE SOLUTION\n# ============================================================================\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict\nimport time\n\n# Cell 1: Setup Comparison Framework\nprint(\"=\" * 80)\nprint(\"TWIN RAG PIPELINE COMPARISON\")\nprint(\"=\" * 80)\n\n# Assume qa_chain_ollama and qa_chain_granite are already initialized\n# from Labs 2.1 and 2.2\n\n# Cell 2: Define Test Queries\nevaluation_queries = [\n    {\n        \"query\": \"What is Retrieval Augmented Generation?\",\n        \"category\": \"definition\",\n        \"expected_concepts\": [\"retrieval\", \"generation\", \"LLM\", \"knowledge\"]\n    },\n    {\n        \"query\": \"What are the benefits of RAG?\",\n        \"category\": \"benefits\",\n        \"expected_concepts\": [\"hallucination\", \"current\", \"domain-specific\"]\n    },\n    {\n        \"query\": \"Explain chunking strategies in detail\",\n        \"category\": \"technical\",\n        \"expected_concepts\": [\"fixed-size\", \"semantic\", \"overlap\"]\n    },\n    {\n        \"query\": \"When should I use Chroma vs Elasticsearch?\",\n        \"category\": \"comparison\",\n        \"expected_concepts\": [\"local\", \"production\", \"scalable\"]\n    },\n    {\n        \"query\": \"What is the weather today?\",  # Out of domain\n        \"category\": \"out-of-domain\",\n        \"expected_concepts\": []\n    }\n]\n\n# Cell 3: Comparison Function\ndef compare_rag_backends(\n    query: str,\n    ollama_chain,\n    granite_chain\n) -&gt; Dict:\n    \"\"\"\n    Compare responses from both RAG backends\n\n    Returns:\n        dict with results from both systems\n    \"\"\"\n    print(f\"\\nComparing: {query}\")\n    print(\"-\" * 80)\n\n    # Ollama RAG\n    start = time.time()\n    ollama_result = ollama_chain({\"query\": query})\n    ollama_time = time.time() - start\n\n    # watsonx Granite RAG\n    start = time.time()\n    granite_result = granite_chain({\"query\": query})\n    granite_time = time.time() - start\n\n    comparison = {\n        \"query\": query,\n        \"ollama_answer\": ollama_result['result'],\n        \"ollama_time\": ollama_time,\n        \"ollama_sources\": len(ollama_result['source_documents']),\n        \"granite_answer\": granite_result['result'],\n        \"granite_time\": granite_time,\n        \"granite_sources\": len(granite_result['source_documents']),\n        \"time_difference\": abs(ollama_time - granite_time),\n        \"faster_backend\": \"Ollama\" if ollama_time &lt; granite_time else \"Granite\"\n    }\n\n    print(f\"Ollama: {ollama_time:.2f}s ({comparison['ollama_sources']} sources)\")\n    print(f\"Granite: {granite_time:.2f}s ({comparison['granite_sources']} sources)\")\n\n    return comparison\n\n# Cell 4: Run Comparisons\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RUNNING COMPARISONS\")\nprint(\"=\" * 80)\n\ncomparisons = []\nfor eval_query in evaluation_queries:\n    comparison = compare_rag_backends(\n        eval_query[\"query\"],\n        qa_chain_ollama,  # from Lab 2.1\n        qa_chain_granite   # from Lab 2.2\n    )\n    comparison[\"category\"] = eval_query[\"category\"]\n    comparisons.append(comparison)\n\n# Cell 5: Analyze Results\nresults_df = pd.DataFrame(comparisons)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COMPARISON ANALYSIS\")\nprint(\"=\" * 80)\n\nprint(\"\\nResponse Time Comparison:\")\nprint(f\"  Ollama average: {results_df['ollama_time'].mean():.2f}s\")\nprint(f\"  Granite average: {results_df['granite_time'].mean():.2f}s\")\nprint(f\"  Winner: {results_df['faster_backend'].mode()[0]}\")\n\nprint(\"\\nSource Retrieval:\")\nprint(f\"  Ollama average sources: {results_df['ollama_sources'].mean():.1f}\")\nprint(f\"  Granite average sources: {results_df['granite_sources'].mean():.1f}\")\n\n# Cell 6: Visualizations\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Response time comparison\nax1 = axes[0]\nx = range(len(results_df))\nwidth = 0.35\nax1.bar([i - width/2 for i in x], results_df['ollama_time'], width, label='Ollama', alpha=0.8)\nax1.bar([i + width/2 for i in x], results_df['granite_time'], width, label='Granite', alpha=0.8)\nax1.set_xlabel('Query')\nax1.set_ylabel('Response Time (seconds)')\nax1.set_title('Response Time Comparison')\nax1.set_xticks(x)\nax1.set_xticklabels(range(1, len(results_df)+1))\nax1.legend()\nax1.grid(axis='y', alpha=0.3)\n\n# Sources retrieved\nax2 = axes[1]\nax2.bar([i - width/2 for i in x], results_df['ollama_sources'], width, label='Ollama', alpha=0.8)\nax2.bar([i + width/2 for i in x], results_df['granite_sources'], width, label='Granite', alpha=0.8)\nax2.set_xlabel('Query')\nax2.set_ylabel('Number of Sources')\nax2.set_title('Sources Retrieved')\nax2.set_xticks(x)\nax2.set_xticklabels(range(1, len(results_df)+1))\nax2.legend()\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('lab_2.3_comparison.png', dpi=300, bbox_inches='tight')\nprint(\"\\n\u2713 Visualization saved to lab_2.3_comparison.png\")\n\n# Cell 7: Side-by-Side Answer Comparison\ndef display_comparison(query_idx: int):\n    \"\"\"Display side-by-side comparison for a specific query\"\"\"\n    comp = comparisons[query_idx]\n\n    print(f\"\\n{'='*80}\")\n    print(f\"QUERY {query_idx + 1}: {comp['query']}\")\n    print(f\"{'='*80}\")\n\n    print(f\"\\n{'OLLAMA':^40} | {'GRANITE':^40}\")\n    print(f\"{'-'*40}|{'-'*40}\")\n\n    # Split answers into lines for side-by-side display\n    ollama_lines = comp['ollama_answer'].split('\\n')\n    granite_lines = comp['granite_answer'].split('\\n')\n\n    max_lines = max(len(ollama_lines), len(granite_lines))\n\n    for i in range(max_lines):\n        ollama_line = ollama_lines[i] if i &lt; len(ollama_lines) else \"\"\n        granite_line = granite_lines[i] if i &lt; len(granite_lines) else \"\"\n\n        # Truncate lines that are too long\n        if len(ollama_line) &gt; 38:\n            ollama_line = ollama_line[:35] + \"...\"\n        if len(granite_line) &gt; 38:\n            granite_line = granite_line[:35] + \"...\"\n\n        print(f\"{ollama_line:&lt;40} | {granite_line:&lt;40}\")\n\n    print(f\"\\n{'-'*80}\")\n    print(f\"{'Time: ' + str(comp['ollama_time']) + 's':&lt;40} | {'Time: ' + str(comp['granite_time']) + 's':&lt;40}\")\n    print(f\"{'Sources: ' + str(comp['ollama_sources']):&lt;40} | {'Sources: ' + str(comp['granite_sources']):&lt;40}\")\n\n# Display comparisons\nfor i in range(len(comparisons)):\n    display_comparison(i)\n\n# Cell 8: Qualitative Analysis\nprint(\"\\n\" + \"=\" * 80)\nprint(\"QUALITATIVE ANALYSIS\")\nprint(\"=\" * 80)\n\nobservations = {\n    \"verbosity\": {\n        \"ollama\": \"More conversational and detailed\",\n        \"granite\": \"More concise and factual\"\n    },\n    \"accuracy\": {\n        \"ollama\": \"Good for general knowledge\",\n        \"granite\": \"Better for technical accuracy\"\n    },\n    \"handling_out_of_domain\": {\n        \"ollama\": \"May attempt to answer anyway\",\n        \"granite\": \"Better at refusing gracefully\"\n    },\n    \"speed\": {\n        \"ollama\": f\"{results_df['ollama_time'].mean():.2f}s average\",\n        \"granite\": f\"{results_df['granite_time'].mean():.2f}s average\"\n    }\n}\n\nfor aspect, comparison in observations.items():\n    print(f\"\\n{aspect.replace('_', ' ').title()}:\")\n    for model, observation in comparison.items():\n        print(f\"  {model.capitalize()}: {observation}\")\n\n# Cell 9: Save Complete Results\nresults_df.to_csv(\"lab_2.3_twin_comparison.csv\", index=False)\n\n# Create detailed report\nwith open(\"lab_2.3_comparison_report.txt\", \"w\") as f:\n    f.write(\"TWIN RAG PIPELINE COMPARISON REPORT\\n\")\n    f.write(\"=\" * 80 + \"\\n\\n\")\n\n    for i, comp in enumerate(comparisons):\n        f.write(f\"Query {i+1}: {comp['query']}\\n\")\n        f.write(\"-\" * 80 + \"\\n\")\n        f.write(f\"Category: {comp['category']}\\n\\n\")\n\n        f.write(\"OLLAMA:\\n\")\n        f.write(f\"  Answer: {comp['ollama_answer']}\\n\")\n        f.write(f\"  Time: {comp['ollama_time']:.2f}s\\n\")\n        f.write(f\"  Sources: {comp['ollama_sources']}\\n\\n\")\n\n        f.write(\"GRANITE:\\n\")\n        f.write(f\"  Answer: {comp['granite_answer']}\\n\")\n        f.write(f\"  Time: {comp['granite_time']:.2f}s\\n\")\n        f.write(f\"  Sources: {comp['granite_sources']}\\n\")\n        f.write(\"\\n\" + \"=\" * 80 + \"\\n\\n\")\n\nprint(\"\\n\u2713 Detailed report saved to lab_2.3_comparison_report.txt\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LAB 2.3 COMPLETE! \u2705\")\nprint(\"=\" * 80)\nprint(\"\\nKey Findings:\")\nprint(f\"  \u2022 Average response time difference: {results_df['time_difference'].mean():.2f}s\")\nprint(f\"  \u2022 Faster system: {results_df['faster_backend'].mode()[0]}\")\nprint(f\"  \u2022 Both systems retrieved similar number of sources\")\nprint(\"\\nNext: Lab 2.4 - Build evaluation harness\")\n</code></pre>"},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#lab-24-solution-rag-evaluation-harness","title":"Lab 2.4 Solution: RAG Evaluation Harness","text":""},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#complete-evaluation-framework","title":"Complete Evaluation Framework","text":"<pre><code># ============================================================================\n# LAB 2.4: RAG EVALUATION HARNESS - COMPLETE SOLUTION\n# ============================================================================\n\nimport pandas as pd\nimport numpy as np\nfrom rouge_score import rouge_scorer\nfrom typing import List, Dict, Tuple\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Cell 1: Define Ground Truth\nprint(\"=\" * 80)\nprint(\"RAG EVALUATION HARNESS\")\nprint(\"=\" * 80)\n\nground_truth = [\n    {\n        \"query\": \"What is Retrieval Augmented Generation?\",\n        \"gold_answer\": \"RAG is a technique that enhances LLMs by retrieving relevant external knowledge from a document corpus to ground responses in factual data.\",\n        \"relevant_doc_ids\": [\"rag_overview.txt\"],\n        \"must_include_concepts\": [\"retrieval\", \"generation\", \"external knowledge\", \"LLM\"]\n    },\n    {\n        \"query\": \"What are the benefits of RAG?\",\n        \"gold_answer\": \"RAG reduces hallucinations, provides access to current information, enables domain-specific knowledge without retraining, and offers source attribution.\",\n        \"relevant_doc_ids\": [\"rag_overview.txt\"],\n        \"must_include_concepts\": [\"hallucination\", \"current information\", \"domain-specific\"]\n    },\n    {\n        \"query\": \"Explain chunking strategies\",\n        \"gold_answer\": \"Chunking strategies include fixed-size splitting by tokens, semantic chunking by paragraphs, and recursive chunking that tries multiple separators.\",\n        \"relevant_doc_ids\": [\"chunking_strategies.txt\"],\n        \"must_include_concepts\": [\"fixed-size\", \"semantic\", \"recursive\"]\n    }\n]\n\nprint(f\"\u2713 Created ground truth with {len(ground_truth)} test cases\")\n\n# Cell 2: Retrieval Metrics\ndef calculate_retrieval_metrics(\n    retrieved_doc_ids: List[str],\n    relevant_doc_ids: List[str]\n) -&gt; Dict:\n    \"\"\"\n    Calculate retrieval quality metrics\n\n    Metrics:\n    - Precision: What % of retrieved docs are relevant?\n    - Recall: What % of relevant docs were retrieved?\n    - F1: Harmonic mean of precision and recall\n    - Hit Rate: Was at least one relevant doc retrieved?\n    \"\"\"\n    retrieved_set = set(retrieved_doc_ids)\n    relevant_set = set(relevant_doc_ids)\n\n    true_positives = len(retrieved_set &amp; relevant_set)\n\n    precision = true_positives / len(retrieved_set) if retrieved_set else 0\n    recall = true_positives / len(relevant_set) if relevant_set else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n    hit_rate = 1 if true_positives &gt; 0 else 0\n\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"hit_rate\": hit_rate,\n        \"retrieved_count\": len(retrieved_set),\n        \"relevant_count\": len(relevant_set)\n    }\n\n# Test retrieval metrics\ntest_retrieved = [\"rag_overview.txt\", \"chunking_strategies.txt\"]\ntest_relevant = [\"rag_overview.txt\"]\ntest_metrics = calculate_retrieval_metrics(test_retrieved, test_relevant)\n\nprint(\"\\nRetrieval Metrics Example:\")\nfor metric, value in test_metrics.items():\n    print(f\"  {metric}: {value:.3f}\" if isinstance(value, float) else f\"  {metric}: {value}\")\n\n# Cell 3: Answer Quality Metrics\ndef calculate_answer_metrics(\n    generated_answer: str,\n    gold_answer: str,\n    must_include_concepts: List[str]\n) -&gt; Dict:\n    \"\"\"\n    Calculate answer quality metrics\n\n    Metrics:\n    - ROUGE-L: Longest common subsequence similarity\n    - Concept Coverage: % of required concepts present\n    - Answer Length: Number of words\n    \"\"\"\n    # ROUGE-L score\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    rouge_scores = scorer.score(gold_answer, generated_answer)\n    rouge_l = rouge_scores['rougeL'].fmeasure\n\n    # Concept coverage\n    answer_lower = generated_answer.lower()\n    concepts_found = [c for c in must_include_concepts if c.lower() in answer_lower]\n    concept_coverage = len(concepts_found) / len(must_include_concepts) if must_include_concepts else 0\n\n    # Length metrics\n    word_count = len(generated_answer.split())\n\n    return {\n        \"rouge_l\": rouge_l,\n        \"concept_coverage\": concept_coverage,\n        \"concepts_found\": concepts_found,\n        \"concepts_missing\": [c for c in must_include_concepts if c not in concepts_found],\n        \"word_count\": word_count\n    }\n\n# Test answer metrics\ntest_answer = \"RAG combines retrieval and generation using external knowledge for LLMs\"\ntest_gold = \"RAG enhances LLMs with retrieved external knowledge\"\ntest_concepts = [\"retrieval\", \"generation\", \"external knowledge\"]\ntest_answer_metrics = calculate_answer_metrics(test_answer, test_gold, test_concepts)\n\nprint(\"\\nAnswer Quality Metrics Example:\")\nfor metric, value in test_answer_metrics.items():\n    if isinstance(value, float):\n        print(f\"  {metric}: {value:.3f}\")\n    elif isinstance(value, list):\n        print(f\"  {metric}: {value}\")\n    else:\n        print(f\"  {metric}: {value}\")\n\n# Cell 4: Evaluate RAG System\ndef evaluate_rag_system(\n    rag_chain,\n    test_cases: List[Dict],\n    backend_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Comprehensive RAG system evaluation\n\n    Args:\n        rag_chain: The RAG QA chain to evaluate\n        test_cases: List of test cases with ground truth\n        backend_name: Name of the backend (e.g., \"Ollama\", \"Granite\")\n\n    Returns:\n        DataFrame with evaluation results\n    \"\"\"\n    print(f\"\\nEvaluating {backend_name} RAG system...\")\n    print(\"-\" * 80)\n\n    results = []\n\n    for i, test_case in enumerate(test_cases, 1):\n        query = test_case[\"query\"]\n        print(f\"\\n{i}. {query}\")\n\n        # Get RAG response\n        response = rag_chain({\"query\": query})\n        generated_answer = response['result']\n        retrieved_docs = response['source_documents']\n\n        # Extract doc IDs from retrieved documents\n        retrieved_doc_ids = [\n            doc.metadata['source'].split('/')[-1]\n            for doc in retrieved_docs\n        ]\n\n        # Calculate retrieval metrics\n        retrieval_metrics = calculate_retrieval_metrics(\n            retrieved_doc_ids,\n            test_case[\"relevant_doc_ids\"]\n        )\n\n        # Calculate answer metrics\n        answer_metrics = calculate_answer_metrics(\n            generated_answer,\n            test_case[\"gold_answer\"],\n            test_case[\"must_include_concepts\"]\n        )\n\n        # Combine results\n        result = {\n            \"backend\": backend_name,\n            \"query\": query,\n            \"generated_answer\": generated_answer,\n            **retrieval_metrics,\n            **answer_metrics\n        }\n\n        results.append(result)\n\n        # Print summary\n        print(f\"   Retrieval F1: {retrieval_metrics['f1']:.3f}\")\n        print(f\"   Concept Coverage: {answer_metrics['concept_coverage']:.1%}\")\n        print(f\"   ROUGE-L: {answer_metrics['rouge_l']:.3f}\")\n\n    return pd.DataFrame(results)\n\n# Cell 5: Run Evaluations\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RUNNING EVALUATIONS\")\nprint(\"=\" * 80)\n\n# Evaluate Ollama\nollama_eval_results = evaluate_rag_system(\n    qa_chain_ollama,  # from Lab 2.1\n    ground_truth,\n    \"Ollama\"\n)\n\n# Evaluate Granite\ngranite_eval_results = evaluate_rag_system(\n    qa_chain_granite,  # from Lab 2.2\n    ground_truth,\n    \"Granite\"\n)\n\n# Combine results\nall_eval_results = pd.concat([ollama_eval_results, granite_eval_results])\n\n# Cell 6: Aggregate Analysis\nprint(\"\\n\" + \"=\" * 80)\nprint(\"EVALUATION SUMMARY\")\nprint(\"=\" * 80)\n\nsummary_metrics = all_eval_results.groupby('backend').agg({\n    'precision': 'mean',\n    'recall': 'mean',\n    'f1': 'mean',\n    'hit_rate': 'mean',\n    'rouge_l': 'mean',\n    'concept_coverage': 'mean',\n    'word_count': 'mean'\n}).round(3)\n\nprint(\"\\nAverage Metrics by Backend:\")\nprint(summary_metrics)\n\n# Cell 7: Visualizations\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Retrieval metrics\nax1 = axes[0, 0]\nretrieval_metrics = ['precision', 'recall', 'f1']\nx = np.arange(len(retrieval_metrics))\nwidth = 0.35\nax1.bar(x - width/2, summary_metrics.loc['Ollama', retrieval_metrics], \n        width, label='Ollama', alpha=0.8)\nax1.bar(x + width/2, summary_metrics.loc['Granite', retrieval_metrics],\n        width, label='Granite', alpha=0.8)\nax1.set_ylabel('Score')\nax1.set_title('Retrieval Metrics')\nax1.set_xticks(x)\nax1.set_xticklabels(retrieval_metrics)\nax1.legend()\nax1.set_ylim([0, 1])\nax1.grid(axis='y', alpha=0.3)\n\n# Answer quality\nax2 = axes[0, 1]\nanswer_metrics = ['rouge_l', 'concept_coverage']\nx = np.arange(len(answer_metrics))\nax2.bar(x - width/2, summary_metrics.loc['Ollama', answer_metrics],\n        width, label='Ollama', alpha=0.8)\nax2.bar(x + width/2, summary_metrics.loc['Granite', answer_metrics],\n        width, label='Granite', alpha=0.8)\nax2.set_ylabel('Score')\nax2.set_title('Answer Quality Metrics')\nax2.set_xticks(x)\nax2.set_xticklabels(answer_metrics)\nax2.legend()\nax2.set_ylim([0, 1])\nax2.grid(axis='y', alpha=0.3)\n\n# Per-query F1 scores\nax3 = axes[1, 0]\nquery_numbers = range(1, len(ground_truth) + 1)\nollama_f1 = ollama_eval_results['f1'].values\ngranite_f1 = granite_eval_results['f1'].values\nx = np.arange(len(query_numbers))\nax3.plot(x, ollama_f1, 'o-', label='Ollama', linewidth=2, markersize=8)\nax3.plot(x, granite_f1, 's-', label='Granite', linewidth=2, markersize=8)\nax3.set_xlabel('Query Number')\nax3.set_ylabel('F1 Score')\nax3.set_title('Per-Query Retrieval F1')\nax3.set_xticks(x)\nax3.set_xticklabels(query_numbers)\nax3.legend()\nax3.grid(alpha=0.3)\nax3.set_ylim([0, 1.1])\n\n# Per-query concept coverage\nax4 = axes[1, 1]\nollama_coverage = ollama_eval_results['concept_coverage'].values\ngranite_coverage = granite_eval_results['concept_coverage'].values\nax4.plot(x, ollama_coverage, 'o-', label='Ollama', linewidth=2, markersize=8)\nax4.plot(x, granite_coverage, 's-', label='Granite', linewidth=2, markersize=8)\nax4.set_xlabel('Query Number')\nax4.set_ylabel('Concept Coverage')\nax4.set_title('Per-Query Concept Coverage')\nax4.set_xticks(x)\nax4.set_xticklabels(query_numbers)\nax4.legend()\nax4.grid(alpha=0.3)\nax4.set_ylim([0, 1.1])\n\nplt.tight_layout()\nplt.savefig('lab_2.4_evaluation_results.png', dpi=300, bbox_inches='tight')\nprint(\"\\n\u2713 Evaluation visualizations saved\")\n\n# Cell 8: Design eval_small.py Script\neval_script_design = \"\"\"\n# accelerator/tools/eval_small.py\n\nimport argparse\nimport pandas as pd\nfrom pathlib import Path\nimport requests\nimport json\n\ndef load_test_cases(csv_path: str) -&gt; pd.DataFrame:\n    '''Load test queries and ground truth from CSV'''\n    return pd.read_csv(csv_path)\n\ndef call_rag_api(endpoint: str, query: str) -&gt; dict:\n    '''Call the RAG API endpoint'''\n    response = requests.post(\n        f\"{endpoint}/ask\",\n        json={\"query\": query}\n    )\n    return response.json()\n\ndef evaluate_response(\n    generated: str,\n    gold: str,\n    relevant_docs: list\n) -&gt; dict:\n    '''Calculate metrics for a single response'''\n    # Implementation from above\n    pass\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--test-data', required=True)\n    parser.add_argument('--api-endpoint', required=True)\n    parser.add_argument('--output', default='eval_results.json')\n    args = parser.parse_args()\n\n    # Load test cases\n    test_cases = load_test_cases(args.test_data)\n\n    # Run evaluation\n    results = []\n    for _, test in test_cases.iterrows():\n        response = call_rag_api(args.api_endpoint, test['query'])\n        metrics = evaluate_response(\n            response['answer'],\n            test['gold_answer'],\n            test['relevant_docs'].split(',')\n        )\n        results.append(metrics)\n\n    # Save results\n    with open(args.output, 'w') as f:\n        json.dump(results, f, indent=2)\n\n    # Print summary\n    df = pd.DataFrame(results)\n    print(df.describe())\n\nif __name__ == '__main__':\n    main()\n\"\"\"\n\nwith open(\"eval_small.py.design\", \"w\") as f:\n    f.write(eval_script_design)\n\nprint(\"\\n\u2713 Script design saved to eval_small.py.design\")\n\n# Cell 9: Save All Results\nall_eval_results.to_csv(\"lab_2.4_full_evaluation.csv\", index=False)\nsummary_metrics.to_csv(\"lab_2.4_summary_metrics.csv\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"LAB 2.4 COMPLETE! \u2705\")\nprint(\"=\" * 80)\nprint(\"\\nKey Achievements:\")\nprint(\"  \u2713 Defined ground truth test cases\")\nprint(\"  \u2713 Implemented retrieval metrics (P/R/F1)\")\nprint(\"  \u2713 Implemented answer quality metrics (ROUGE, concept coverage)\")\nprint(\"  \u2713 Evaluated both RAG backends\")\nprint(\"  \u2713 Created visualizations\")\nprint(\"  \u2713 Designed eval_small.py script\")\nprint(\"\\nAll Labs Complete! Ready for production integration.\")\n</code></pre>"},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#production-deployment-code","title":"Production Deployment Code","text":""},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#accelerator-integration-examples","title":"Accelerator Integration Examples","text":"<pre><code># accelerator/rag/pipeline.py\n\nfrom typing import Dict, List\nfrom langchain.chains import RetrievalQA\nfrom .retriever import create_retriever\nfrom .prompt import get_rag_prompt\n\ndef answer_question(\n    query: str,\n    vectorstore,\n    llm,\n    top_k: int = 5\n) -&gt; Dict:\n    \"\"\"\n    Production RAG pipeline\n\n    Args:\n        query: User question\n        vectorstore: Vector database instance\n        llm: Language model instance\n        top_k: Number of documents to retrieve\n\n    Returns:\n        {\n            \"answer\": str,\n            \"chunks\": List[dict],\n            \"metadata\": dict\n        }\n    \"\"\"\n    # Create retriever\n    retriever = create_retriever(vectorstore, k=top_k)\n\n    # Get prompt template\n    prompt = get_rag_prompt()\n\n    # Build QA chain\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt}\n    )\n\n    # Generate answer\n    result = qa_chain({\"query\": query})\n\n    # Format response\n    return {\n        \"answer\": result['result'],\n        \"chunks\": [\n            {\n                \"id\": doc.metadata.get('id'),\n                \"text\": doc.page_content,\n                \"source\": doc.metadata.get('source'),\n                \"score\": doc.metadata.get('score', 0)\n            }\n            for doc in result['source_documents']\n        ],\n        \"metadata\": {\n            \"model_id\": llm.model_id if hasattr(llm, 'model_id') else \"unknown\",\n            \"num_chunks\": len(result['source_documents']),\n            \"retrieval_k\": top_k\n        }\n    }\n\n# accelerator/service/api.py\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom ..rag import pipeline\nfrom ..rag import retriever\n\napp = FastAPI(title=\"RAG Service\")\n\nclass QuestionRequest(BaseModel):\n    query: str\n    top_k: int = 5\n\nclass AnswerResponse(BaseModel):\n    answer: str\n    citations: List[Dict]\n    metadata: Dict\n\n@app.post(\"/ask\", response_model=AnswerResponse)\nasync def ask_question(request: QuestionRequest):\n    \"\"\"\n    Answer a question using RAG\n\n    Example:\n        POST /ask\n        {\n            \"query\": \"What is RAG?\",\n            \"top_k\": 5\n        }\n    \"\"\"\n    try:\n        result = pipeline.answer_question(\n            query=request.query,\n            vectorstore=app.state.vectorstore,\n            llm=app.state.llm,\n            top_k=request.top_k\n        )\n\n        return AnswerResponse(\n            answer=result[\"answer\"],\n            citations=result[\"chunks\"],\n            metadata=result[\"metadata\"]\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n</code></pre>"},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#common-issues","title":"Common Issues","text":"<pre><code># Issue 1: Ollama not responding\n# Solution:\n\"\"\"\n1. Check if Ollama is running:\n   ps aux | grep ollama\n\n2. Start Ollama:\n   ollama serve\n\n3. Test connection:\n   curl http://localhost:11434/api/tags\n\"\"\"\n\n# Issue 2: watsonx.ai authentication errors\n# Solution:\n\"\"\"\n1. Verify API key:\n   echo $WATSONX_APIKEY\n\n2. Check project ID:\n   # Should be in format: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n\n3. Test credentials:\n   from ibm_watsonx_ai import APIClient\n   client = APIClient(credentials=creds, project_id=project_id)\n   client.foundation_models.get_model_specs()\n\"\"\"\n\n# Issue 3: Slow retrieval\n# Solution:\n\"\"\"\n1. Reduce top_k parameter\n2. Use smaller embedding model\n3. Implement caching\n4. Consider approximate nearest neighbor search\n\"\"\"\n\n# Issue 4: Out of memory\n# Solution:\n\"\"\"\n1. Process documents in batches\n2. Use smaller chunk size\n3. Clear unused variables:\n   import gc\n   gc.collect()\n4. Use generators instead of lists\n\"\"\"\n</code></pre>"},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#summary","title":"Summary","text":""},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#workshop-completion-checklist","title":"Workshop Completion Checklist","text":"<ul> <li> Understand RAG architecture and components</li> <li> Implement local RAG with Ollama and Chroma</li> <li> Build enterprise RAG with watsonx.ai and Elasticsearch</li> <li> Compare multiple RAG implementations</li> <li> Create automated evaluation harness</li> <li> Design production deployment patterns</li> <li> Map concepts to AI Accelerator framework</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Complete_Solutions_Guide/#next-steps","title":"Next Steps","text":"<ol> <li>Day 3 Integration:</li> <li>Implement accelerator components</li> <li>Deploy FastAPI service</li> <li>Build Streamlit UI</li> <li> <p>Set up monitoring</p> </li> <li> <p>Advanced Topics:</p> </li> <li>Multi-modal RAG (images, tables)</li> <li>Agentic RAG systems</li> <li>Hybrid search (keyword + semantic)</li> <li> <p>Advanced evaluation metrics</p> </li> <li> <p>Production Considerations:</p> </li> <li>Scaling strategies</li> <li>Security and authentication</li> <li>Cost optimization</li> <li>User feedback loops</li> </ol> <p>Congratulations on completing Day 2! \ud83c\udf89</p> <p>You now have production-ready RAG implementation skills and are prepared to build enterprise AI applications.</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/","title":"Day 2 RAG Workshop - Instructor Guide &amp; Quick Reference","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#workshop-overview-for-instructors","title":"\ud83c\udfaf Workshop Overview for Instructors","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#target-audience","title":"Target Audience","text":"<ul> <li>Software engineers with Python experience</li> <li>Data scientists exploring LLM applications</li> <li>AI engineers building production systems</li> <li>Technical professionals interested in RAG</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#prerequisites-check-day-1","title":"Prerequisites Check (Day 1)","text":"<ul> <li>\u2705 Students completed Day 1: LLM basics, prompting, basic API usage</li> <li>\u2705 Python 3.10+ installed</li> <li>\u2705 Jupyter notebooks working</li> <li>\u2705 Basic understanding of embeddings and vectors</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#daily-schedule","title":"\ud83d\udcc5 Daily Schedule","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#morning-session-theory-4-hours","title":"Morning Session: Theory (4 hours)","text":"<p>9:00 - 10:00: Module 1 - RAG Architecture Overview (60 min) - Introduction to RAG (15 min)   - Definition and motivation   - Real-world use cases   - Demo: Show working RAG system</p> <ul> <li>Core Components (30 min)</li> <li>Document store</li> <li>Chunking strategies</li> <li>Embeddings and vector stores</li> <li> <p>Retriever and LLM integration</p> </li> <li> <p>RAG Pipeline Flow (15 min)</p> </li> <li>Live diagram walkthrough</li> <li>Ingestion \u2192 Indexing \u2192 Retrieval \u2192 Generation</li> </ul> <p>10:00 - 10:45: Module 2 - Embedding Models (45 min) - Understanding embeddings (20 min)   - Dense vector representations   - Semantic similarity   - Demo: Visualize embeddings in 2D</p> <ul> <li>Model comparison (25 min)</li> <li>sentence-transformers vs commercial</li> <li>Dimensions and trade-offs</li> <li>Hands-on: Test different models</li> </ul> <p>10:45 - 11:00: \u2615 Break (15 min)</p> <p>11:00 - 12:00: Module 3 - Vector Databases (60 min) - Introduction to vector stores (20 min)   - Why specialized databases?   - Similarity search algorithms</p> <ul> <li>Tool comparison (30 min)</li> <li>Chroma: Local and simple</li> <li>Elasticsearch: Production-grade</li> <li>FAISS: High-performance</li> <li> <p>Pinecone: Managed service</p> </li> <li> <p>Hands-on demo (10 min)</p> </li> <li>Build a simple vector store</li> <li>Run similarity searches</li> </ul> <p>12:00 - 1:00: Module 4 - Accelerator Architecture (60 min) - Accelerator components (30 min)   - Code walkthrough   - tools/ directory overview   - rag/ directory overview</p> <ul> <li>Production patterns (20 min)</li> <li>API design</li> <li>Error handling</li> <li> <p>Monitoring considerations</p> </li> <li> <p>Q&amp;A and prep for labs (10 min)</p> </li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#afternoon-session-labs-4-hours","title":"Afternoon Session: Labs (4 hours)","text":"<p>2:00 - 3:00: Lab 2.1 - Local RAG with Ollama (60 min)</p> <p>Instructor Prep: <pre><code># Test Ollama is running\nollama serve\nollama pull llama2\n\n# Verify sample corpus exists\nls data/corpus/\n\n# Test reference solution\njupyter nbconvert --execute lab_2.1_solution.ipynb\n</code></pre></p> <p>Walkthrough (15 min): 1. Show completed notebook running 2. Explain key sections 3. Point out common pitfalls 4. Answer setup questions</p> <p>Independent Work (35 min): - Students work through lab - Circulate to help with issues - Monitor for blockers</p> <p>Review (10 min): - Show solution - Discuss variations - Preview next lab</p> <p>Common Issues: <pre><code># Ollama not starting\n# Solution: Check if port 11434 is free\nlsof -i :11434\n\n# Slow embedding\n# Solution: Reduce chunk count or use GPU\nCONFIG[\"chunk_size\"] = 500  # Smaller chunks\n\n# Out of memory\n# Solution: Process in batches\nfor batch in chunks[::100]:\n    vectorstore.add_documents(batch)\n</code></pre></p> <p>3:00 - 4:00: Lab 2.2 - RAG with watsonx.ai (60 min)</p> <p>Instructor Prep: <pre><code># Verify credentials work\nexport WATSONX_APIKEY=\"your_key\"\nexport PROJECT_ID=\"your_project_id\"\n\n# Test API connection\npython test_watsonx_connection.py\n\n# Prepare Elasticsearch alternative if needed\ndocker run -p 9200:9200 elasticsearch:8.11.0\n</code></pre></p> <p>Walkthrough (15 min): 1. Credentials setup walkthrough 2. Show Granite model in action 3. Explain watsonx-specific patterns 4. Elasticsearch optional setup</p> <p>Independent Work (35 min): - Students implement enterprise RAG - Help with credentials - Troubleshoot API issues</p> <p>Review (10 min): - Compare Ollama vs watsonx - Discuss production considerations</p> <p>Common Issues: <pre><code># Authentication errors\n# Check: API key format, project permissions\n\n# Slow responses\n# Solution: Reduce max_new_tokens\nparameters = {\n    GenParams.MAX_NEW_TOKENS: 100  # Instead of 500\n}\n\n# Elasticsearch connection timeout\n# Solution: Use Chroma fallback\nif USE_ELASTICSEARCH:\n    try:\n        vectorstore = ElasticsearchStore(...)\n    except:\n        vectorstore = Chroma.from_documents(...)\n</code></pre></p> <p>4:00 - 4:15: \u2615 Break (15 min)</p> <p>4:15 - 5:15: Lab 2.3 - Twin RAG Pipelines (60 min)</p> <p>Instructor Prep: <pre><code># Ensure both RAG systems are working\npython -c \"from lab_2_1_solution import qa_chain_ollama\"\npython -c \"from lab_2_2_solution import qa_chain_granite\"\n\n# Prepare comparison template\njupyter notebook lab_2.3_starter.ipynb\n</code></pre></p> <p>Walkthrough (10 min): 1. Show comparison framework 2. Explain metrics 3. Demonstrate visualization</p> <p>Independent Work (40 min): - Students build comparison - Collect results - Analyze differences</p> <p>Review (10 min): - Discuss findings - Identify patterns - Production insights</p> <p>5:15 - 6:00: Lab 2.4 - Evaluation Harness (45 min)</p> <p>Instructor Prep: <pre><code># Prepare ground truth data\ncat &gt; ground_truth.csv &lt;&lt; EOF\nquery,gold_answer,relevant_docs\n\"What is RAG?\",\"RAG enhances LLMs...\",\"rag_overview.txt\"\nEOF\n\n# Test evaluation functions\npython test_evaluation.py\n</code></pre></p> <p>Walkthrough (10 min): 1. Importance of evaluation 2. Metrics explanation 3. Ground truth creation</p> <p>Independent Work (25 min): - Implement eval functions - Run evaluations - Visualize results</p> <p>Review (10 min): - Discuss metrics - eval_small.py design - Governance integration</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#learning-objectives-checklist","title":"\ud83c\udf93 Learning Objectives Checklist","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#knowledge-can-explain","title":"Knowledge (Can explain)","text":"<ul> <li> What is RAG and why it's useful</li> <li> How embeddings represent semantic meaning</li> <li> Difference between vector stores</li> <li> Trade-offs in RAG design</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#skills-can-implement","title":"Skills (Can implement)","text":"<ul> <li> Load and chunk documents</li> <li> Create embeddings and vector stores</li> <li> Build retrieval functions</li> <li> Integrate LLMs into RAG</li> <li> Evaluate RAG systems</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#application-can-build","title":"Application (Can build)","text":"<ul> <li> Working local RAG pipeline</li> <li> Enterprise RAG with watsonx</li> <li> Comparison framework</li> <li> Evaluation harness</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#assessment-grading","title":"\ud83d\udcca Assessment &amp; Grading","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#lab-completion-40-points","title":"Lab Completion (40 points)","text":"<ul> <li>Lab 2.1: 10 points (notebook runs, answers correct)</li> <li>Lab 2.2: 10 points (watsonx integration works)</li> <li>Lab 2.3: 10 points (comparison complete)</li> <li>Lab 2.4: 10 points (evaluation implemented)</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#quality-30-points","title":"Quality (30 points)","text":"<ul> <li>Code organization and comments: 10 points</li> <li>Error handling: 10 points</li> <li>Documentation: 10 points</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#understanding-30-points","title":"Understanding (30 points)","text":"<ul> <li>Wrap-up questions: 15 points</li> <li>Accelerator mapping: 15 points</li> </ul> <p>Total: 100 points</p> <p>Grading Rubric: - 90-100: Excellent - All labs working, clear understanding - 80-89: Good - Minor issues, solid grasp of concepts - 70-79: Satisfactory - Some components working, basic understanding - 60-69: Needs Improvement - Significant gaps - &lt;60: Not Passing - Incomplete work</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#technical-setup-verification","title":"\ud83d\udd27 Technical Setup Verification","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#pre-workshop-checklist","title":"Pre-Workshop Checklist","text":"<p>Send to students 1 week before: <pre><code># Install Python packages\npip install -r requirements.txt\n\n# Install Ollama\ncurl https://ollama.ai/install.sh | sh\nollama pull llama2\n\n# Test imports\npython -c \"import langchain; import chromadb; import sentence_transformers\"\n\n# Verify watsonx credentials (share template)\npython verify_credentials.py\n</code></pre></p> <p>Day-of setup (30 minutes before): <pre><code># Start Ollama\nollama serve &amp;\n\n# Verify all systems\n./verify_workshop_setup.sh\n\n# Create sample data\npython create_sample_corpus.py\n\n# Test reference solutions\npytest test_lab_solutions.py\n</code></pre></p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#teaching-tips","title":"\ud83d\udca1 Teaching Tips","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#for-each-module","title":"For Each Module","text":"<p>1. Start with \"Why\" - Don't jump into code - Explain the problem being solved - Show real-world example first</p> <p>2. Use Progressive Disclosure <pre><code># First show simple version\nvectorstore = Chroma.from_documents(docs, embeddings)\n\n# Then add complexity\nvectorstore = Chroma.from_documents(\n    documents=docs,\n    embedding=embeddings,\n    persist_directory=\"./db\",\n    collection_metadata={\"hnsw:space\": \"cosine\"}\n)\n</code></pre></p> <p>3. Encourage Experimentation - \"What happens if you change chunk_size to 500?\" - \"Try different embedding models\" - \"Compare with/without overlap\"</p> <p>4. Relate to Accelerator - \"This notebook code will become retriever.py\" - \"Notice how we're separating concerns\" - \"Think about error handling for production\"</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#handling-questions","title":"Handling Questions","text":"<p>Common Questions &amp; Answers:</p> <p>Q: \"Why not just use GPT-4 for everything?\" A: \"Cost, latency, privacy, and control over knowledge\"</p> <p>Q: \"When should I use RAG vs fine-tuning?\" A: \"RAG: frequently updated info. Fine-tuning: style/behavior\"</p> <p>Q: \"Which vector store should I use?\" A: \"Dev: Chroma. Production: Elasticsearch/Pinecone\"</p> <p>Q: \"How do I handle multi-language?\" A: \"Use multilingual embedding models or separate stores\"</p> <p>Q: \"What about images/tables?\" A: \"Multi-modal RAG - covered in advanced topics\"</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#pacing-strategies","title":"Pacing Strategies","text":"<p>If Running Behind: - Skip optional demos - Provide pre-built components - Focus on key concepts - Extend lab time, reduce review</p> <p>If Running Ahead: - Deep dive into advanced topics - Extra optimization exercises - Production deployment discussion - Preview Day 3 content</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#troubleshooting-guide-for-instructors","title":"\ud83d\udc1b Troubleshooting Guide for Instructors","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#student-environment-issues","title":"Student Environment Issues","text":"<p>Issue: \"Ollama won't start\" <pre><code># Check if already running\nps aux | grep ollama\n\n# Check port availability\nlsof -i :11434\n\n# Kill existing process\npkill ollama\n\n# Restart\nollama serve\n</code></pre></p> <p>Issue: \"Out of memory during indexing\" <pre><code># Solution: Batch processing\nbatch_size = 100\nfor i in range(0, len(chunks), batch_size):\n    batch = chunks[i:i+batch_size]\n    vectorstore.add_documents(batch)\n</code></pre></p> <p>Issue: \"watsonx authentication fails\" <pre><code># Debug credentials\nprint(f\"API Key length: {len(WATSONX_APIKEY)}\")\nprint(f\"Project ID format: {len(PROJECT_ID) == 36}\")\n\n# Test simple call\nfrom ibm_watsonx_ai import APIClient\nclient = APIClient(credentials, project_id)\nprint(client.foundation_models.get_model_specs()[:2])\n</code></pre></p> <p>Issue: \"Elasticsearch connection timeout\" <pre><code># Provide Chroma fallback\nprint(\"Elasticsearch unavailable, using Chroma...\")\nvectorstore = Chroma.from_documents(\n    documents=chunks,\n    embedding=embeddings\n)\n</code></pre></p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#content-delivery-issues","title":"Content Delivery Issues","text":"<p>Students Lost in Theory: - Pause for questions every 10 minutes - Use more diagrams/visualizations - Relate to familiar concepts - Provide analogy: \"Like a librarian finding relevant books\"</p> <p>Labs Too Fast/Slow: - Have extension exercises ready - Provide partial solutions for struggling students - Pair programming for mixed skill levels</p> <p>Technical Difficulties: - Always have backup: cloud notebooks (Colab/SageMaker) - Pre-recorded demos as fallback - Offline documentation</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#additional-resources-for-instructors","title":"\ud83d\udcda Additional Resources for Instructors","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#reference-materials","title":"Reference Materials","text":"<p>Papers: 1. \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis et al., 2020) 2. \"Dense Passage Retrieval for Open-Domain Question Answering\" (Karpukhin et al., 2020)</p> <p>Documentation: - LangChain RAG Guide: https://python.langchain.com/docs/use_cases/question_answering/ - watsonx.ai Docs: https://www.ibm.com/docs/en/watsonx-as-a-service - Chroma Docs: https://docs.trychroma.com/</p> <p>Videos (for students): - \"RAG Explained\" - Andrej Karpathy - \"Building Production RAG Systems\" - LangChain YouTube</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#extension-activities","title":"Extension Activities","text":"<p>For Advanced Students: 1. Hybrid Search Implementation    - Combine keyword and semantic search    - Implement BM25 + vector similarity</p> <ol> <li>Multi-Query RAG</li> <li>Generate multiple versions of query</li> <li> <p>Retrieve from each, combine results</p> </li> <li> <p>Agentic RAG</p> </li> <li>Add routing logic</li> <li>Implement query planning</li> <li> <p>Self-correction loops</p> </li> <li> <p>Custom Evaluators</p> </li> <li>Domain-specific metrics</li> <li>Human-in-the-loop evaluation</li> <li>A/B testing framework</li> </ol>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#workshop-closing-545-600-pm","title":"\ud83c\udfac Workshop Closing (5:45 - 6:00 PM)","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#wrap-up-activity-10-min","title":"Wrap-Up Activity (10 min)","text":"<p>Quick Reflection: 1. \"What was the most surprising thing you learned?\" 2. \"What will you try in your work?\" 3. \"What questions remain?\"</p> <p>Key Takeaways Reinforcement: - RAG = Retrieval + Generation for grounded answers - Components: Docs \u2192 Chunks \u2192 Embeddings \u2192 Vector Store \u2192 Retriever \u2192 LLM - Evaluation is critical for production systems - Accelerator provides production patterns</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#preview-day-3-5-min","title":"Preview Day 3 (5 min)","text":"<ul> <li>Agentic AI systems</li> <li>Governed AI tooling</li> <li>Advanced evaluation</li> <li>Deployment and monitoring</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#logistics","title":"Logistics","text":"<ul> <li>Share: Completion certificates</li> <li>Collect: Feedback surveys</li> <li>Provide: Additional resources document</li> <li>Remind: Office hours schedule</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#post-workshop-instructor-checklist","title":"\ud83d\udcdd Post-Workshop Instructor Checklist","text":""},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#immediate-same-day","title":"Immediate (Same Day)","text":"<ul> <li> Collect attendance</li> <li> Grade lab submissions (if applicable)</li> <li> Note common issues for next time</li> <li> Share solution notebooks</li> <li> Send follow-up email with resources</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#within-week","title":"Within Week","text":"<ul> <li> Review feedback surveys</li> <li> Update materials based on feedback</li> <li> Grade final submissions</li> <li> Provide individual feedback</li> <li> Schedule follow-up office hours</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#for-next-workshop","title":"For Next Workshop","text":"<ul> <li> Update prerequisites based on issues</li> <li> Refine pacing based on feedback</li> <li> Add/remove content as needed</li> <li> Update code for latest library versions</li> <li> Create FAQ document</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#success-metrics","title":"\ud83c\udfaf Success Metrics","text":"<p>Workshop is Successful If: - \u2705 80%+ students complete all labs - \u2705 90%+ understand RAG concepts - \u2705 70%+ can build basic RAG pipeline independently - \u2705 Average satisfaction score &gt; \u2158 - \u2705 Students can explain trade-offs in RAG design</p> <p>Track These Metrics: - Completion rates per lab - Time spent per module - Number of questions per topic - Common error patterns - Post-workshop survey results</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#support-resources","title":"\ud83d\udcde Support Resources","text":"<p>During Workshop: - Slack: #day2-rag-workshop - TA support: 2-3 TAs for 20+ students - Backup instructor for technical issues</p> <p>After Workshop: - Office hours: 2x per week for 2 weeks - Discussion forum: for ongoing questions - Code review: optional for project work</p>"},{"location":"tracks/day2-rag/Day2_RAG_Instructor_Guide/#bonus-quick-command-reference","title":"\ud83c\udfc6 Bonus: Quick Command Reference","text":"<pre><code># === Environment Setup ===\n# Install all dependencies\npip install -r requirements.txt\n\n# Start Ollama\nollama serve\n\n# Pull models\nollama pull llama2\nollama pull mistral\n\n# === Data Preparation ===\n# Create sample corpus\npython scripts/create_sample_corpus.py\n\n# Download workshop data\nwget https://raw.github.com/.../data.zip\nunzip data.zip\n\n# === Testing ===\n# Test all labs\npytest tests/test_labs.py\n\n# Test specific lab\npytest tests/test_lab_2_1.py -v\n\n# Run solution notebook\njupyter nbconvert --execute lab_2_1_solution.ipynb\n\n# === Common Fixes ===\n# Reset Chroma database\nrm -rf ./chroma_db\n\n# Clear Python cache\nfind . -type d -name __pycache__ -exec rm -r {} +\n\n# Restart Jupyter kernel\n# Kernel &gt; Restart &amp; Clear Output\n\n# === Monitoring ===\n# Watch Ollama logs\nollama logs -f\n\n# Check vector store size\ndu -sh ./chroma_db\n\n# Monitor memory usage\nwatch -n 1 free -h\n</code></pre> <p>Good luck with the workshop! \ud83d\ude80</p> <p>Remember: The goal is learning, not perfection. Encourage experimentation and questions!</p> <p>Document Version: 1.0 Last Updated: January 2025 Contact: workshop-support@example.com</p>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/","title":"Day 2 \u2013 RAG Workshop: Complete Guide","text":""},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#workshop-overview","title":"Workshop Overview","text":"<p>Duration: 8 hours total - Morning Session (4 hours): Theory and Concepts - Afternoon Session (4 hours): Hands-on Labs</p> <p>Objective: Master Retrieval Augmented Generation (RAG) implementation using local models (Ollama) and watsonx.ai, with evaluation and production deployment patterns.</p>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#workshop-structure","title":"Workshop Structure","text":""},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#part-1-theory-4-hours","title":"\ud83d\udcda Part 1: Theory (4 hours)","text":""},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#module-1-rag-architecture-overview-1-hour","title":"Module 1: RAG Architecture Overview (1 hour)","text":"<ul> <li>What is RAG?</li> <li>Definition and motivation</li> <li>Reducing hallucinations through grounding</li> <li> <p>Real-world use cases</p> </li> <li> <p>Core Components</p> </li> <li>Document store and corpus management</li> <li>Chunking strategies</li> <li>Embedding models</li> <li>Vector databases</li> <li>Retrieval mechanisms</li> <li> <p>LLM integration</p> </li> <li> <p>RAG Pipeline Flow</p> </li> <li>Ingestion phase</li> <li>Indexing phase</li> <li>Retrieval phase</li> <li> <p>Generation phase</p> </li> <li> <p>Trade-offs and Considerations</p> </li> <li>Latency vs accuracy</li> <li>Embedding model selection</li> <li>Index size and refresh strategies</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#module-2-embedding-models-45-minutes","title":"Module 2: Embedding Models (45 minutes)","text":"<ul> <li>Understanding dense vector representations</li> <li>Popular embedding models (sentence-transformers, OpenAI, watsonx)</li> <li>Embedding dimensions and trade-offs</li> <li>Semantic similarity metrics</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#module-3-vector-databases-1-hour","title":"Module 3: Vector Databases (1 hour)","text":"<ul> <li>Introduction to vector stores</li> <li>Chroma: Local lightweight option</li> <li>Elasticsearch: Production-grade distributed search</li> <li>FAISS: High-performance similarity search</li> <li>Index types and search strategies</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#module-4-accelerator-architecture-1-hour-15-minutes","title":"Module 4: Accelerator Architecture (1 hour 15 minutes)","text":"<ul> <li>Ingestion tools (<code>chunk.py</code>, <code>extract.py</code>, <code>embed_index.py</code>)</li> <li>Retrieval components (<code>retriever.py</code>)</li> <li>Pipeline orchestration (<code>pipeline.py</code>)</li> <li>Prompt templates (<code>prompt.py</code>)</li> <li>Production deployment patterns</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#part-2-hands-on-labs-4-hours","title":"\ud83d\udd2c Part 2: Hands-on Labs (4 hours)","text":""},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#lab-21-local-rag-with-ollama-1-hour","title":"Lab 2.1: Local RAG with Ollama (1 hour)","text":"<p>Objective: Build a complete RAG pipeline using local Ollama models</p> <p>What You'll Build: - Document corpus preparation - Local embedding generation - Chroma vector store setup - Query-response pipeline - Source attribution</p> <p>Skills Learned: - Chunking strategies - Local vector store management - Ollama model integration - End-to-end RAG flow</p>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#lab-22-rag-with-watsonxai-1-hour","title":"Lab 2.2: RAG with watsonx.ai (1 hour)","text":"<p>Objective: Implement enterprise-grade RAG using IBM watsonx.ai</p> <p>What You'll Build: - watsonx.ai authentication setup - Granite model integration - Elasticsearch vector store - Production-ready retrieval - Performance comparison</p> <p>Skills Learned: - watsonx.ai API usage - Enterprise vector store setup - Prompt engineering for Granite - Scalable RAG patterns</p>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#lab-23-twin-rag-pipelines-1-hour","title":"Lab 2.3: Twin RAG Pipelines (1 hour)","text":"<p>Objective: Compare Ollama and watsonx implementations side-by-side</p> <p>What You'll Build: - Unified comparison framework - Test query suite - Results aggregation - Qualitative analysis</p> <p>Skills Learned: - Multi-backend orchestration - Comparative evaluation - Response quality assessment - Trade-off analysis</p>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#lab-24-rag-evaluation-harness-1-hour","title":"Lab 2.4: RAG Evaluation Harness (1 hour)","text":"<p>Objective: Build automated evaluation pipeline for RAG systems</p> <p>What You'll Build: - Ground truth dataset - Retrieval metrics (hit rate, precision@k) - Answer quality metrics (ROUGE, semantic similarity) - Automated scoring pipeline - Visualization dashboard</p> <p>Skills Learned: - RAG evaluation methodologies - Metrics computation - Automated testing - Performance tracking</p>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#prerequisites","title":"Prerequisites","text":""},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#required-knowledge","title":"Required Knowledge","text":"<ul> <li>Python programming (intermediate level)</li> <li>Basic understanding of LLMs</li> <li>Familiarity with APIs</li> <li>Command line basics</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#technical-setup","title":"Technical Setup","text":""},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#software-requirements","title":"Software Requirements","text":"<pre><code># Python 3.10 or higher\npython --version\n\n# Install core dependencies\npip install langchain langchain-community\npip install sentence-transformers\npip install chromadb\npip install ollama\npip install ibm-watsonx-ai\npip install elasticsearch\npip install pandas numpy\npip install rouge-score evaluate\n</code></pre>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#environment-setup","title":"Environment Setup","text":"<ol> <li> <p>Ollama Installation (for local labs)    <pre><code># Install Ollama\ncurl https://ollama.ai/install.sh | sh\n\n# Pull models\nollama pull llama2\nollama pull mistral\n</code></pre></p> </li> <li> <p>watsonx.ai Access (for enterprise labs)</p> </li> <li>IBM Cloud account</li> <li>watsonx.ai service instance</li> <li> <p>API key generation</p> </li> <li> <p>Vector Store Setup</p> </li> <li>Chroma: Included with Python install</li> <li>Elasticsearch: Docker or cloud instance</li> </ol>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#credentials-configuration","title":"Credentials Configuration","text":"<p>Create <code>.env</code> file: <pre><code># watsonx.ai credentials\nWATSONX_APIKEY=your_api_key_here\nWATSONX_PROJECT_ID=your_project_id\nWATSONX_URL=https://us-south.ml.cloud.ibm.com\n\n# Elasticsearch (if using)\nELASTICSEARCH_URL=your_elasticsearch_url\nELASTICSEARCH_USER=your_username\nELASTICSEARCH_PASSWORD=your_password\n\n# OpenAI (if using)\nOPENAI_API_KEY=your_openai_key\n</code></pre></p>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#workshop-materials-structure","title":"Workshop Materials Structure","text":"<pre><code>day2-rag-workshop/\n\u251c\u2500\u2500 theory/\n\u2502   \u251c\u2500\u2500 01_rag_architecture_overview.md\n\u2502   \u251c\u2500\u2500 02_embedding_models.md\n\u2502   \u251c\u2500\u2500 03_vector_databases.md\n\u2502   \u2514\u2500\u2500 04_accelerator_architecture.md\n\u2502\n\u251c\u2500\u2500 labs/\n\u2502   \u251c\u2500\u2500 lab_2.1_local_rag_ollama/\n\u2502   \u2502   \u251c\u2500\u2500 lab_instructions.md\n\u2502   \u2502   \u251c\u2500\u2500 starter_notebook.ipynb\n\u2502   \u2502   \u2514\u2500\u2500 solution_notebook.ipynb\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 lab_2.2_rag_watsonx/\n\u2502   \u2502   \u251c\u2500\u2500 lab_instructions.md\n\u2502   \u2502   \u251c\u2500\u2500 starter_notebook.ipynb\n\u2502   \u2502   \u2514\u2500\u2500 solution_notebook.ipynb\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 lab_2.3_twin_pipelines/\n\u2502   \u2502   \u251c\u2500\u2500 lab_instructions.md\n\u2502   \u2502   \u251c\u2500\u2500 starter_notebook.ipynb\n\u2502   \u2502   \u2514\u2500\u2500 solution_notebook.ipynb\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 lab_2.4_evaluation_harness/\n\u2502       \u251c\u2500\u2500 lab_instructions.md\n\u2502       \u251c\u2500\u2500 starter_notebook.ipynb\n\u2502       \u2514\u2500\u2500 solution_notebook.ipynb\n\u2502\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 sample_documents/\n\u2502   \u251c\u2500\u2500 test_queries.csv\n\u2502   \u2514\u2500\u2500 ground_truth.json\n\u2502\n\u251c\u2500\u2500 reference_notebooks/\n\u2502   \u251c\u2500\u2500 elasticsearch_watsonx_example.ipynb\n\u2502   \u251c\u2500\u2500 chroma_langchain_example.ipynb\n\u2502   \u2514\u2500\u2500 evaluation_studio_example.ipynb\n\u2502\n\u2514\u2500\u2500 accelerator_integration/\n    \u251c\u2500\u2500 chunk.py\n    \u251c\u2500\u2500 embed_index.py\n    \u251c\u2500\u2500 retriever.py\n    \u251c\u2500\u2500 pipeline.py\n    \u2514\u2500\u2500 prompt.py\n</code></pre>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this workshop, you will be able to:</p> <ol> <li>\u2705 Explain RAG architecture and components</li> <li>\u2705 Implement local RAG pipelines with Ollama</li> <li>\u2705 Build enterprise RAG systems with watsonx.ai</li> <li>\u2705 Choose appropriate embedding models and vector stores</li> <li>\u2705 Implement effective chunking strategies</li> <li>\u2705 Evaluate RAG system performance</li> <li>\u2705 Compare different RAG implementations</li> <li>\u2705 Deploy production-grade RAG services</li> <li>\u2705 Integrate with the AI Accelerator framework</li> <li>\u2705 Monitor and improve RAG systems</li> </ol>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#workshop-timeline","title":"Workshop Timeline","text":""},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#morning-session-900-am-100-pm","title":"Morning Session (9:00 AM - 1:00 PM)","text":"Time Activity Duration 9:00 - 10:00 Module 1: RAG Architecture Overview 60 min 10:00 - 10:45 Module 2: Embedding Models 45 min 10:45 - 11:00 \u2615 Break 15 min 11:00 - 12:00 Module 3: Vector Databases 60 min 12:00 - 1:00 Module 4: Accelerator Architecture 60 min"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#afternoon-session-200-pm-600-pm","title":"Afternoon Session (2:00 PM - 6:00 PM)","text":"Time Activity Duration 2:00 - 3:00 Lab 2.1: Local RAG with Ollama 60 min 3:00 - 4:00 Lab 2.2: RAG with watsonx.ai 60 min 4:00 - 4:15 \u2615 Break 15 min 4:15 - 5:15 Lab 2.3: Twin RAG Pipelines 60 min 5:15 - 6:00 Lab 2.4: Evaluation Harness 45 min"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#additional-resources","title":"Additional Resources","text":""},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#documentation","title":"Documentation","text":"<ul> <li>LangChain Documentation</li> <li>watsonx.ai Documentation</li> <li>Chroma Documentation</li> <li>Elasticsearch Vector Search</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#example-repositories","title":"Example Repositories","text":"<ul> <li>IBM Watson ML Samples: GitHub</li> <li>LangChain Templates: GitHub</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#research-papers","title":"Research Papers","text":"<ul> <li>\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis et al., 2020)</li> <li>\"Dense Passage Retrieval for Open-Domain Question Answering\" (Karpukhin et al., 2020)</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#support-and-troubleshooting","title":"Support and Troubleshooting","text":""},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#common-issues","title":"Common Issues","text":"<ol> <li>Ollama Connection Errors</li> <li>Ensure Ollama service is running: <code>ollama serve</code></li> <li> <p>Check model is pulled: <code>ollama list</code></p> </li> <li> <p>watsonx.ai Authentication</p> </li> <li>Verify API key is correct</li> <li>Check project/space permissions</li> <li> <p>Ensure region matches your instance</p> </li> <li> <p>Vector Store Issues</p> </li> <li>Elasticsearch connection timeouts: Check network/firewall</li> <li>Chroma persistence: Verify directory permissions</li> <li> <p>Out of memory: Reduce batch size or chunk count</p> </li> <li> <p>Performance Issues</p> </li> <li>Slow embedding: Use smaller models or GPU acceleration</li> <li>Large index size: Implement incremental updates</li> <li>Query latency: Optimize retrieval parameters (top_k, similarity threshold)</li> </ol>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#getting-help","title":"Getting Help","text":"<ul> <li>Workshop facilitators: Available during lab sessions</li> <li>Slack channel: #day2-rag-workshop</li> <li>Office hours: Schedule via workshop portal</li> </ul>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#next-steps-after-workshop","title":"Next Steps After Workshop","text":"<ol> <li>Implement Your Use Case</li> <li>Identify your domain documents</li> <li>Customize chunking strategy</li> <li> <p>Fine-tune retrieval parameters</p> </li> <li> <p>Production Deployment</p> </li> <li>Review accelerator deployment patterns</li> <li>Set up monitoring and logging</li> <li> <p>Implement user feedback loops</p> </li> <li> <p>Advanced Topics (Day 3)</p> </li> <li>Multi-modal RAG</li> <li>Agentic RAG systems</li> <li>Governed AI tooling</li> <li>Advanced evaluation techniques</li> </ol>"},{"location":"tracks/day2-rag/Day2_RAG_Workshop_README/#feedback","title":"Feedback","text":"<p>Please complete the workshop survey: [Survey Link]</p> <p>Your feedback helps us improve future workshops!</p> <p>Workshop Version: 1.0 Last Updated: January 2025 Maintained by: IBM watsonx.ai Education Team</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/","title":"Day 2 Supplementary Material: Accelerator Integration &amp; IBM Tooling","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#overview","title":"\ud83d\udccb Overview","text":"<p>This supplementary guide covers additional topics not included in the main Day 2 workshop materials:</p> <ol> <li>Accelerator Project Structure - How labs map to production code</li> <li>IBM Reference Notebooks - Detailed examples from labs-src/ and accelerator/assets/</li> <li>Docker Environment Setup - Container-based development</li> <li>watsonx_quickstart.ipynb - Initial setup notebook</li> <li>Governed Agentic Catalog - Custom tool registration and usage</li> <li>Evaluation Studio Integration - Experiment tracking with multiple LLMs</li> <li>Advanced Examples - Banking Assistant and HR Assistant with LangGraph</li> </ol>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#1-connection-to-the-accelerator-project","title":"1. Connection to the Accelerator Project","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#11-understanding-the-accelerator-structure","title":"1.1 Understanding the Accelerator Structure","text":"<p>The <code>watsonx-workshop/accelerator/</code> directory contains production-grade RAG service components:</p> <pre><code>accelerator/\n\u251c\u2500\u2500 rag/                    # Core RAG logic\n\u2502   \u251c\u2500\u2500 retriever.py        # Vector store queries\n\u2502   \u251c\u2500\u2500 pipeline.py         # End-to-end RAG orchestration\n\u2502   \u2514\u2500\u2500 prompt.py           # Prompt templates\n\u2502\n\u251c\u2500\u2500 service/                # API layer\n\u2502   \u251c\u2500\u2500 api.py             # FastAPI app with /ask endpoint\n\u2502   \u2514\u2500\u2500 deps.py            # Configuration &amp; dependencies\n\u2502\n\u251c\u2500\u2500 tools/                  # Batch processing scripts\n\u2502   \u251c\u2500\u2500 chunk.py           # Document chunking\n\u2502   \u251c\u2500\u2500 extract.py         # Text extraction\n\u2502   \u251c\u2500\u2500 embed_index.py     # Embedding &amp; indexing\n\u2502   \u2514\u2500\u2500 eval_small.py      # Evaluation harness\n\u2502\n\u251c\u2500\u2500 ui/                     # User interface\n\u2502   \u2514\u2500\u2500 app.py             # Streamlit front-end\n\u2502\n\u2514\u2500\u2500 assets/                 # Reference materials\n    \u251c\u2500\u2500 notebook/          # Example notebooks\n    \u2514\u2500\u2500 data_asset/        # Helper functions\n        \u2514\u2500\u2500 rag_helper_functions.py\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#12-mapping-labs-to-accelerator-components","title":"1.2 Mapping Labs to Accelerator Components","text":"Lab Component Becomes Accelerator File Document loading Batch tool <code>tools/extract.py</code> Chunking logic Batch tool <code>tools/chunk.py</code> Embedding &amp; indexing Batch tool <code>tools/embed_index.py</code> Retrieval function Core RAG <code>rag/retriever.py</code> RAG pipeline Core RAG <code>rag/pipeline.py</code> Prompt templates Core RAG <code>rag/prompt.py</code> Answer endpoint API <code>service/api.py</code> Evaluation Testing <code>tools/eval_small.py</code>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#13-from-notebook-to-production","title":"1.3 From Notebook to Production","text":"<p>Lab 2.\u00bd.2 Pattern: <pre><code># In notebook (prototype)\ndef retrieve(query: str):\n    docs = vectorstore.similarity_search(query, k=4)\n    return docs\n\ndef answer(query: str):\n    docs = retrieve(query)\n    prompt = build_prompt(query, docs)\n    answer = llm(prompt)\n    return {\"answer\": answer, \"chunks\": docs}\n</code></pre></p> <p>Accelerator Pattern: <pre><code># accelerator/rag/retriever.py (production)\nclass Retriever:\n    def __init__(self, vectorstore_config: dict):\n        self.vectorstore = self._init_vectorstore(vectorstore_config)\n\n    def retrieve(self, query: str, k: int = 4) -&gt; List[Chunk]:\n        \"\"\"Retrieve relevant chunks with error handling &amp; logging\"\"\"\n        try:\n            results = self.vectorstore.similarity_search(query, k=k)\n            return [self._format_chunk(r) for r in results]\n        except Exception as e:\n            logger.error(f\"Retrieval failed: {e}\")\n            raise\n\n# accelerator/rag/pipeline.py (production)\nclass RAGPipeline:\n    def __init__(self, retriever: Retriever, llm: BaseLLM):\n        self.retriever = retriever\n        self.llm = llm\n\n    def answer_question(self, query: str) -&gt; dict:\n        \"\"\"End-to-end RAG with metrics &amp; error handling\"\"\"\n        start_time = time.time()\n\n        chunks = self.retriever.retrieve(query)\n        prompt = self.prompt_builder.build(query, chunks)\n        answer = self.llm.generate(prompt)\n\n        return {\n            \"answer\": answer,\n            \"chunks\": [c.to_dict() for c in chunks],\n            \"model_id\": self.llm.model_id,\n            \"latency_ms\": int((time.time() - start_time) * 1000)\n        }\n</code></pre></p> <p>Key Differences: - \u2705 Configuration management via <code>deps.py</code> - \u2705 Error handling and logging - \u2705 Structured response format - \u2705 Metrics collection - \u2705 Type hints and validation</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#14-service-configuration-depspy","title":"1.4 Service Configuration (deps.py)","text":"<pre><code># accelerator/service/deps.py\nfrom pydantic import BaseSettings\n\nclass Settings(BaseSettings):\n    # watsonx.ai\n    WATSONX_URL: str\n    WATSONX_APIKEY: str\n    WATSONX_PROJECT_ID: str\n    LLM_MODEL_ID: str = \"ibm/granite-3-3-8b-instruct\"\n\n    # Vector store\n    VECTOR_STORE_TYPE: str = \"elasticsearch\"  # or \"chroma\"\n    ES_URL: str = \"\"\n    ES_INDEX: str = \"rag_index\"\n\n    # RAG parameters\n    RETRIEVAL_K: int = 5\n    MAX_CONTEXT_TOKENS: int = 2000\n\n    class Config:\n        env_file = \".env\"\n\nsettings = Settings()\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#15-fastapi-endpoint-apipy","title":"1.5 FastAPI Endpoint (api.py)","text":"<pre><code># accelerator/service/api.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\nfrom accelerator.rag.pipeline import RAGPipeline\nfrom accelerator.service.deps import settings, get_pipeline\n\napp = FastAPI(title=\"RAG Service\")\n\nclass QuestionRequest(BaseModel):\n    question: str\n    k: int = 5\n\nclass AnswerResponse(BaseModel):\n    answer: str\n    citations: List[dict]\n    model_id: str\n    latency_ms: int\n\n@app.post(\"/ask\", response_model=AnswerResponse)\nasync def ask_question(request: QuestionRequest):\n    \"\"\"\n    Answer a question using RAG\n\n    Example:\n        POST /ask\n        {\n            \"question\": \"What is RAG?\",\n            \"k\": 5\n        }\n    \"\"\"\n    try:\n        pipeline = get_pipeline()\n        result = pipeline.answer_question(\n            query=request.question,\n            k=request.k\n        )\n\n        return AnswerResponse(\n            answer=result[\"answer\"],\n            citations=result[\"chunks\"],\n            model_id=result[\"model_id\"],\n            latency_ms=result[\"latency_ms\"]\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#2-reference-notebooks-deep-dive","title":"2. Reference Notebooks Deep Dive","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#21-labs-src-notebooks-ibm-examples","title":"2.1 Labs-src/ Notebooks (IBM Examples)","text":"<p>These notebooks demonstrate different RAG patterns with IBM tooling:</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#elasticsearch-langchain-pattern","title":"Elasticsearch + LangChain Pattern","text":"<p>File: <code>labs-src/use-watsonx-elasticsearch-and-langchain-to-answer-questions-rag.ipynb</code></p> <p>Key Learnings: - Using LangChain's <code>ElasticsearchStore</code> abstraction - Configuring Elasticsearch with watsonx embeddings - Chain construction with <code>RetrievalQA</code></p> <p>Code Pattern: <pre><code>from langchain_elasticsearch import ElasticsearchStore\nfrom langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n\n# Setup embeddings\nembeddings = WatsonxEmbeddings(\n    model_id=\"ibm/slate-30m-english-rtrvr\",\n    url=watsonx_url,\n    apikey=watsonx_apikey,\n    project_id=project_id\n)\n\n# Setup vector store\nvectorstore = ElasticsearchStore(\n    es_url=elasticsearch_url,\n    index_name=\"my_index\",\n    embedding=embeddings,\n    es_user=es_user,\n    es_password=es_password\n)\n\n# Create retrieval chain\nllm = WatsonxLLM(model_id=\"ibm/granite-13b-chat-v2\", ...)\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    retriever=vectorstore.as_retriever(),\n    return_source_documents=True\n)\n\n# Query\nresult = qa_chain({\"query\": \"What is RAG?\"})\n</code></pre></p> <p>When to Use: Production deployments requiring scalable vector search</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#elasticsearch-python-sdk-pattern","title":"Elasticsearch Python SDK Pattern","text":"<p>File: <code>labs-src/use-watsonx-and-elasticsearch-python-sdk-to-answer-questions-rag.ipynb</code></p> <p>Key Learnings: - Direct Elasticsearch client usage (no LangChain) - Manual embedding and indexing - Custom similarity search queries</p> <p>Code Pattern: <pre><code>from elasticsearch import Elasticsearch\nfrom sentence_transformers import SentenceTransformer\n\n# Direct ES client\nes_client = Elasticsearch(\n    [f\"https://{es_host}:{es_port}\"],\n    basic_auth=(es_user, es_password),\n    ssl_assert_fingerprint=ssl_fingerprint\n)\n\n# Manual embedding\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nquery_vector = model.encode(query)\n\n# KNN search\nresponse = es_client.search(\n    index=index_name,\n    knn={\n        \"field\": \"embedding\",\n        \"query_vector\": query_vector,\n        \"k\": 5,\n        \"num_candidates\": 50\n    }\n)\n</code></pre></p> <p>When to Use: Maximum control over Elasticsearch, custom search logic</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#chroma-langchain-pattern","title":"Chroma + LangChain Pattern","text":"<p>File: <code>labs-src/use-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb</code></p> <p>Key Learnings: - Local development with Chroma - Persistent vs in-memory stores - Integration with Granite models</p> <p>Code Pattern: <pre><code>from langchain_chroma import Chroma\nfrom langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n\n# Chroma setup\nvectorstore = Chroma.from_documents(\n    documents=chunks,\n    embedding=WatsonxEmbeddings(model_id=\"ibm/slate-30m-english-rtrvr\"),\n    persist_directory=\"./chroma_db\"\n)\n\n# Granite LLM\nllm = WatsonxLLM(\n    model_id=\"ibm/granite-13b-chat-v2\",\n    url=watsonx_url,\n    apikey=watsonx_apikey,\n    project_id=project_id,\n    params={\n        \"decoding_method\": \"greedy\",\n        \"max_new_tokens\": 200\n    }\n)\n\n# Query\nqa = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())\nresult = qa({\"query\": \"Explain RAG\"})\n</code></pre></p> <p>When to Use: Prototyping, local development, small-scale deployments</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#22-accelerator-notebooks","title":"2.2 Accelerator Notebooks","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#ingestion-notebooks","title":"Ingestion Notebooks","text":"<p>Process_and_Ingest_Data_into_Vector_DB.ipynb - Complete ingestion pipeline - Error handling and validation - Batch processing strategies - Metadata management</p> <p>Process_and_Ingest_Data_from_COS_into_vector_DB.ipynb - Reading from IBM Cloud Object Storage - Handling multiple file formats - Incremental updates</p> <p>Ingestion_of_Expert_Profile_data_to_Vector_DB.ipynb - Structured data ingestion - Custom metadata fields - Domain-specific chunking</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#rag-qa-notebooks","title":"RAG Q&amp;A Notebooks","text":"<p>QnA_with_RAG.ipynb - End-to-end RAG example - Multiple query patterns - Response formatting</p> <p>Create_and_Deploy_QnA_AI_Service.ipynb - Packaging RAG as a service - Deployment patterns - Testing endpoints</p> <p>Test_Queries_for_Vector_DB.ipynb - Vector store validation - Retrieval quality checks - Performance benchmarking</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#evaluation-notebooks","title":"Evaluation Notebooks","text":"<p>Analyze_Log_and_Feedback.ipynb - Log analysis patterns - User feedback integration - Continuous improvement loop</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#3-docker-environment-setup","title":"3. Docker Environment Setup","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#31-dockerfile-for-watsonx-environment","title":"3.1 Dockerfile for watsonx Environment","text":"<pre><code># Dockerfile\nFROM python:3.11-slim\n\nWORKDIR /workspace\n\n# System dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    curl \\\n    build-essential \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Jupyter setup\nRUN pip install jupyter jupyterlab\n\n# Create notebook directory\nRUN mkdir -p /workspace/notebooks\n\nEXPOSE 8888\n\nCMD [\"jupyter\", \"lab\", \"--ip=0.0.0.0\", \"--port=8888\", \"--no-browser\", \"--allow-root\"]\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#32-docker-compose-setup","title":"3.2 Docker Compose Setup","text":"<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  watsonx-workspace:\n    build: .\n    ports:\n      - \"8888:8888\"\n    volumes:\n      - ./notebooks:/workspace/notebooks\n      - ./data:/workspace/data\n      - ./accelerator:/workspace/accelerator\n    env_file:\n      - .env\n    environment:\n      - JUPYTER_ENABLE_LAB=yes\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#33-makefile-commands","title":"3.3 Makefile Commands","text":"<pre><code># Makefile\n.PHONY: help install build run stop clean\n\nhelp:\n    @echo \"Available commands:\"\n    @echo \"  make install  - Install dependencies locally\"\n    @echo \"  make build    - Build Docker image\"\n    @echo \"  make run      - Start Docker container\"\n    @echo \"  make stop     - Stop Docker container\"\n    @echo \"  make clean    - Clean up containers and images\"\n\ninstall:\n    python -m venv venv\n    . venv/bin/activate &amp;&amp; pip install -r requirements.txt\n\nbuild:\n    docker-compose build\n\nrun:\n    docker-compose up -d\n    @echo \"Jupyter Lab running at http://localhost:8888\"\n\nstop:\n    docker-compose down\n\nclean:\n    docker-compose down -v\n    docker system prune -f\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#34-using-docker-for-labs","title":"3.4 Using Docker for Labs","text":"<pre><code># Build environment\nmake build\n\n# Start container\nmake run\n\n# Access Jupyter Lab\n# Navigate to http://localhost:8888\n# Token will be displayed in console\n\n# Run notebooks inside container\ndocker-compose exec watsonx-workspace bash\ncd notebooks\njupyter nbconvert --execute rag_local_ollama.ipynb\n\n# Stop when done\nmake stop\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#4-watsonx_quickstartipynb-initial-setup-notebook","title":"4. watsonx_quickstart.ipynb - Initial Setup Notebook","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#41-purpose","title":"4.1 Purpose","text":"<p>The quickstart notebook verifies your watsonx.ai setup before starting labs.</p> <p>Location: <code>simple-watsonx-environment/notebooks/watsonx_quickstart.ipynb</code></p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#42-complete-quickstart-code","title":"4.2 Complete Quickstart Code","text":"<pre><code># %% [markdown]\n# # watsonx.ai Quickstart - Environment Verification\n\n# %% [markdown]\n# ## 1. Install Dependencies\n\n# %%\n!pip install -U \"ibm-watsonx-ai&gt;=1.1.22\" python-dotenv\n\n# %% [markdown]\n# ## 2. Load Credentials\n\n# %%\nimport os\nfrom dotenv import load_dotenv\n\n# Load from .env file\nload_dotenv()\n\n# Verify credentials are set\nWATSONX_URL = os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\")\nWATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\nWATSONX_PROJECT_ID = os.getenv(\"WATSONX_PROJECT_ID\")\n\nif not WATSONX_APIKEY:\n    WATSONX_APIKEY = input(\"Enter your watsonx.ai API key: \")\n\nif not WATSONX_PROJECT_ID:\n    WATSONX_PROJECT_ID = input(\"Enter your watsonx.ai project ID: \")\n\nprint(\"\u2713 Credentials loaded\")\n\n# %% [markdown]\n# ## 3. Initialize watsonx.ai Client\n\n# %%\nfrom ibm_watsonx_ai import Credentials, APIClient\n\ncredentials = Credentials(\n    url=WATSONX_URL,\n    api_key=WATSONX_APIKEY\n)\n\napi_client = APIClient(\n    credentials=credentials,\n    project_id=WATSONX_PROJECT_ID\n)\n\nprint(\"\u2713 API client initialized\")\n\n# %% [markdown]\n# ## 4. List Available Models\n\n# %%\n# Get foundation models\nmodels = api_client.foundation_models.get_model_specs()\n\nprint(f\"\u2713 Found {len(models)} available models\\n\")\nprint(\"Sample models:\")\nfor model in models[:5]:\n    print(f\"  - {model['model_id']}\")\n\n# %% [markdown]\n# ## 5. Test Granite Model\n\n# %%\nfrom ibm_watsonx_ai.foundation_models import Model\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n\n# Model parameters\nparams = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n    GenParams.MAX_NEW_TOKENS: 100,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.TEMPERATURE: 0.0,\n}\n\n# Initialize model\nmodel = Model(\n    model_id=\"ibm/granite-3-3-8b-instruct\",\n    credentials=credentials,\n    project_id=WATSONX_PROJECT_ID,\n    params=params\n)\n\nprint(\"\u2713 Granite model initialized\")\n\n# %% [markdown]\n# ## 6. Test Generation\n\n# %%\ntest_prompt = \"What is artificial intelligence? Answer in one sentence.\"\n\nresponse = model.generate_text(prompt=test_prompt)\n\nprint(\"Test Prompt:\")\nprint(f\"  {test_prompt}\")\nprint(\"\\nGranite Response:\")\nprint(f\"  {response}\")\n\n# %% [markdown]\n# ## 7. Test with Structured Output\n\n# %%\nstructured_prompt = \"\"\"List three benefits of RAG systems in JSON format:\n{\n  \"benefits\": [\n    {\"name\": \"...\", \"description\": \"...\"},\n    {\"name\": \"...\", \"description\": \"...\"},\n    {\"name\": \"...\", \"description\": \"...\"}\n  ]\n}\"\"\"\n\nresponse = model.generate_text(prompt=structured_prompt)\nprint(\"Structured Response:\")\nprint(response)\n\n# %% [markdown]\n# ## 8. Verify Embeddings Access\n\n# %%\nfrom ibm_watsonx_ai.foundation_models import Embeddings\n\nembeddings = Embeddings(\n    model_id=\"ibm/slate-30m-english-rtrvr\",\n    credentials=credentials,\n    project_id=WATSONX_PROJECT_ID\n)\n\ntest_texts = [\"Hello world\", \"RAG is useful\"]\nvectors = embeddings.embed_documents(test_texts)\n\nprint(f\"\u2713 Embedded {len(test_texts)} texts\")\nprint(f\"  Embedding dimension: {len(vectors[0])}\")\nprint(f\"  Sample values: {vectors[0][:5]}\")\n\n# %% [markdown]\n# ## 9. Environment Summary\n\n# %%\nprint(\"\\n\" + \"=\"*80)\nprint(\"ENVIRONMENT VERIFICATION SUMMARY\")\nprint(\"=\"*80)\nprint(f\"\u2713 watsonx.ai URL: {WATSONX_URL}\")\nprint(f\"\u2713 API Key: {'*' * 20} (hidden)\")\nprint(f\"\u2713 Project ID: {WATSONX_PROJECT_ID}\")\nprint(f\"\u2713 Available models: {len(models)}\")\nprint(f\"\u2713 Granite model: Working\")\nprint(f\"\u2713 Embeddings: Working\")\nprint(\"\\nReady for Day 2 labs! \ud83d\ude80\")\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#43-troubleshooting-checklist","title":"4.3 Troubleshooting Checklist","text":"<p>Use this when <code>watsonx_quickstart.ipynb</code> fails:</p> <pre><code># Troubleshooting cell\ndef diagnose_setup():\n    issues = []\n\n    # Check API key\n    if not WATSONX_APIKEY or len(WATSONX_APIKEY) &lt; 20:\n        issues.append(\"\u274c API key seems invalid or too short\")\n    else:\n        print(\"\u2713 API key format looks valid\")\n\n    # Check project ID format\n    if not WATSONX_PROJECT_ID or len(WATSONX_PROJECT_ID) != 36:\n        issues.append(\"\u274c Project ID should be 36 characters (UUID format)\")\n    else:\n        print(\"\u2713 Project ID format looks valid\")\n\n    # Check network connectivity\n    try:\n        import requests\n        response = requests.get(WATSONX_URL, timeout=5)\n        print(f\"\u2713 Can reach {WATSONX_URL}\")\n    except Exception as e:\n        issues.append(f\"\u274c Network error: {e}\")\n\n    # Check authentication\n    try:\n        api_client.foundation_models.get_model_specs()\n        print(\"\u2713 Authentication successful\")\n    except Exception as e:\n        issues.append(f\"\u274c Authentication failed: {e}\")\n\n    # Summary\n    if issues:\n        print(\"\\n\u26a0\ufe0f  Issues found:\")\n        for issue in issues:\n            print(f\"  {issue}\")\n    else:\n        print(\"\\n\u2705 All checks passed!\")\n\ndiagnose_setup()\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#5-governed-agentic-catalog","title":"5. Governed Agentic Catalog","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#51-overview","title":"5.1 Overview","text":"<p>The Governed Agentic Catalog allows you to: - Register custom tools with code - Use out-of-the-box IBM tools - Build governed AI agents - Track tool usage and governance</p> <p>Access: https://dataplatform.cloud.ibm.com/aigov/modelinventory/ai-tools?context=wx</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#52-registering-custom-tools","title":"5.2 Registering Custom Tools","text":"<pre><code># Register a custom RAG retrieval tool\nfrom ibm_watsonx_gov.tools.clients import register_tool, ToolRegistrationPayload\n\ntool_code = \"\"\"\ndef rag_retriever(query: str, k: int = 5) -&gt; str:\n    '''Retrieve relevant documents for a query'''\n    import os\n    from langchain_chroma import Chroma\n    from langchain_huggingface import HuggingFaceEmbeddings\n\n    # Load vector store\n    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    vectorstore = Chroma(\n        persist_directory=os.getenv(\"CHROMA_PATH\"),\n        embedding_function=embeddings\n    )\n\n    # Retrieve\n    docs = vectorstore.similarity_search(query, k=k)\n    context = \"\\\\n\\\\n\".join([doc.page_content for doc in docs])\n\n    return context if context else \"No relevant documents found.\"\n\"\"\"\n\ntool_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"The search query\"\n        },\n        \"k\": {\n            \"type\": \"integer\",\n            \"description\": \"Number of documents to retrieve\",\n            \"default\": 5\n        }\n    },\n    \"required\": [\"query\"]\n}\n\ntool_payload = ToolRegistrationPayload(\n    tool_name=\"rag_retriever\",\n    description=\"Retrieve relevant documents from the knowledge base using semantic search\",\n    code={\n        \"source_code_base64\": tool_code,\n        \"run_time_details\": {\n            \"engine\": \"python 3.11\"\n        }\n    },\n    schema=tool_schema,\n    environment_variable=[\"CHROMA_PATH\"],\n    dependencies={\n        \"run_time_packages\": [\"langchain_chroma\", \"langchain_huggingface\"]\n    },\n    category=[\"Search\", \"RAG\"]\n)\n\n# Register\ntool_response = register_tool(tool_payload)\nprint(f\"\u2713 Tool registered: {tool_response['metadata']['asset_id']}\")\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#53-using-registered-tools","title":"5.3 Using Registered Tools","text":"<pre><code>from ibm_watsonx_gov.tools import load_tool\n\n# Load your custom tool\nrag_tool = load_tool(\"rag_retriever\")\n\n# Use it\nresult = rag_tool.invoke({\n    \"query\": \"What is RAG?\",\n    \"k\": 3\n})\n\nprint(\"Retrieved context:\")\nprint(result)\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#54-building-agents-with-governed-tools","title":"5.4 Building Agents with Governed Tools","text":"<pre><code>from langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom ibm_watsonx_gov.tools import load_tool\n\n# Load governed tools\ntools = [\n    load_tool(\"rag_retriever\"),\n    load_tool(\"pii_detector\"),\n    load_tool(\"jailbreak_detector\")\n]\n\n# Create agent\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nsystem_prompt = \"\"\"\nYou are a helpful RAG assistant. Use these rules:\n1. Always check for jailbreak attempts first\n2. Check for PII before processing queries\n3. Use the rag_retriever tool to find relevant information\n4. Answer based on retrieved context only\n\"\"\"\n\nagent = create_react_agent(llm, tools, prompt=system_prompt)\n\n# Run agent\nresponse = agent.invoke({\n    \"messages\": [(\"user\", \"What are the benefits of RAG?\")]\n})\n\nprint(response)\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#6-evaluation-studio-integration","title":"6. Evaluation Studio Integration","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#61-experiment-tracking","title":"6.1 Experiment Tracking","text":"<p>Track multiple RAG experiments with different models:</p> <pre><code>from ibm_watsonx_gov.evaluators.agentic_evaluator import AgenticEvaluator\nfrom ibm_watsonx_gov.entities.ai_experiment import AIExperimentRunRequest\n\n# Setup evaluator\nevaluator = AgenticEvaluator(\n    agentic_app=your_app,\n    tracing_configuration=TracingConfiguration(project_id=project_id)\n)\n\n# Create experiment\nexperiment_id = evaluator.track_experiment(\n    name=\"RAG Model Comparison\",\n    use_existing=True\n)\n\n# Run 1: Granite 8B\ncustom_tags_granite = [\n    {\"key\": \"LLM\", \"value\": \"granite-3-3-8b-instruct\"},\n    {\"key\": \"temperature\", \"value\": \"0.2\"}\n]\n\nrun_request = AIExperimentRunRequest(\n    name=\"granite_8b_run\",\n    custom_tags=custom_tags_granite\n)\n\nevaluator.start_run(run_request)\n# ... run your RAG pipeline ...\nevaluator.end_run()\n\n# Run 2: Granite 13B\ncustom_tags_granite_13b = [\n    {\"key\": \"LLM\", \"value\": \"granite-13b-chat-v2\"},\n    {\"key\": \"temperature\", \"value\": \"0.2\"}\n]\n\nrun_request = AIExperimentRunRequest(\n    name=\"granite_13b_run\",\n    custom_tags=custom_tags_granite_13b\n)\n\nevaluator.start_run(run_request)\n# ... run your RAG pipeline ...\nevaluator.end_run()\n\n# Compare results\nevaluator.compare_ai_experiments(\n    ai_experiments=[AIExperiment(asset_id=experiment_id, runs=[])]\n)\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#62-custom-metrics","title":"6.2 Custom Metrics","text":"<pre><code>from ibm_watsonx_gov.metrics import ContextRelevanceMetric, FaithfulnessMetric, AnswerRelevanceMetric\n\n# Define metrics for each node\nretrieval_node = Node(\n    name=\"Document Retrieval\",\n    metrics_configurations=[\n        MetricsConfiguration(\n            configuration=AgenticAIConfiguration(\n                input_fields=[\"query\"],\n                context_fields=[\"retrieved_docs\"]\n            ),\n            metrics=[ContextRelevanceMetric()]\n        )\n    ]\n)\n\ngeneration_node = Node(\n    name=\"Answer Generation\",\n    metrics_configurations=[\n        MetricsConfiguration(\n            configuration=AgenticAIConfiguration(\n                input_fields=[\"query\"],\n                context_fields=[\"retrieved_docs\"],\n                output_fields=[\"answer\"]\n            ),\n            metrics=[FaithfulnessMetric()]\n        )\n    ]\n)\n\n# Agent-level metrics\nagentic_app = AgenticApp(\n    name=\"RAG Assistant\",\n    metrics_configuration=MetricsConfiguration(\n        metrics=[AnswerRelevanceMetric()],\n        metric_groups=[MetricGroup.CONTENT_SAFETY]\n    ),\n    nodes=[retrieval_node, generation_node]\n)\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#7-advanced-example-banking-assistant-with-langgraph","title":"7. Advanced Example: Banking Assistant with LangGraph","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#71-complete-implementation","title":"7.1 Complete Implementation","text":"<pre><code># Banking Assistant with Experiment Tracking\nfrom langgraph.graph import START, END, StateGraph\nfrom typing_extensions import TypedDict\nfrom langchain_openai import ChatOpenAI\nfrom langchain_chroma import Chroma\n\n# State definition\nclass BankingState(TypedDict):\n    input_text: str\n    context: list[str]\n    generated_text: str\n\n# Retrieval node\ndef retrieval_node(state: BankingState, config) -&gt; dict:\n    vectorstore = config[\"configurable\"][\"vectorstore\"]\n    retriever = vectorstore.as_retriever(\n        search_type=\"similarity_score_threshold\",\n        search_kwargs={\"k\": 3, \"score_threshold\": 0.1}\n    )\n\n    context = retriever.invoke(state[\"input_text\"])\n    context_text = [doc.page_content for doc in context]\n\n    return {\"context\": context_text}\n\n# Generation node\ndef generation_node(state: BankingState, config) -&gt; dict:\n    llm = config[\"configurable\"][\"llm\"]\n\n    prompt = f\"\"\"You are a banking assistant. Answer using only the context:\n\nContext: {chr(10).join(state['context'])}\n\nQuestion: {state['input_text']}\n\nAnswer:\"\"\"\n\n    result = llm.invoke(prompt)\n    return {\"generated_text\": result.content}\n\n# Build graph\ndef build_banking_app():\n    graph = StateGraph(BankingState)\n\n    graph.add_node(\"Bank VectorDb Retrieval\", retrieval_node)\n    graph.add_node(\"Bank VectorDb Answer Generation\", generation_node)\n\n    graph.add_edge(START, \"Bank VectorDb Retrieval\")\n    graph.add_edge(\"Bank VectorDb Retrieval\", \"Bank VectorDb Answer Generation\")\n    graph.add_edge(\"Bank VectorDb Answer Generation\", END)\n\n    return graph.compile()\n\n# Run with experiment tracking\nbanking_app = build_banking_app()\n\n# Test data\ntest_queries = [\n    {\"input_text\": \"How do I open a savings account?\"},\n    {\"input_text\": \"What is the overdraft protection policy?\"},\n    {\"input_text\": \"How long does a wire transfer take?\"}\n]\n\n# Run experiment\nevaluator.start_run(run_request)\n\nfor query in test_queries:\n    result = banking_app.invoke(\n        query,\n        config={\n            \"configurable\": {\n                \"llm\": ChatOpenAI(model=\"gpt-4o-mini\"),\n                \"vectorstore\": vectorstore\n            }\n        }\n    )\n    print(f\"Q: {query['input_text']}\")\n    print(f\"A: {result['generated_text']}\\n\")\n\nevaluator.end_run()\n\n# View results\nresults_df = evaluator.get_result().to_df()\nprint(results_df)\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#8-troubleshooting-guide","title":"8. Troubleshooting Guide","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#81-authentication-issues","title":"8.1 Authentication Issues","text":"<p>Problem: 401 Unauthorized <pre><code># Diagnose\nimport requests\n\ndef test_auth():\n    url = f\"{WATSONX_URL}/ml/v4/foundation_model_specs\"\n    headers = {\n        \"Authorization\": f\"Bearer {WATSONX_APIKEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    response = requests.get(f\"{url}?version=2024-03-19\", headers=headers)\n\n    if response.status_code == 401:\n        print(\"\u274c API key is invalid or expired\")\n        print(\"   Check: https://cloud.ibm.com/iam/apikeys\")\n    elif response.status_code == 403:\n        print(\"\u274c API key is valid but lacks permissions\")\n        print(\"   Check project access in watsonx.ai UI\")\n    else:\n        print(f\"\u2713 Authentication working (status: {response.status_code})\")\n\ntest_auth()\n</code></pre></p> <p>Solution: 1. Regenerate API key: https://cloud.ibm.com/iam/apikeys 2. Verify project permissions 3. Check API key hasn't expired</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#82-project-not-found-404","title":"8.2 Project Not Found (404)","text":"<p>Problem: Invalid project ID <pre><code>def verify_project():\n    try:\n        api_client.projects.get_details(WATSONX_PROJECT_ID)\n        print(f\"\u2713 Project {WATSONX_PROJECT_ID} exists and is accessible\")\n    except Exception as e:\n        print(f\"\u274c Project error: {e}\")\n        print(\"\\nHow to find your project ID:\")\n        print(\"1. Go to watsonx.ai\")\n        print(\"2. Open your project\")\n        print(\"3. Click 'Manage' \u2192 'General' \u2192 'Details'\")\n        print(\"4. Copy the 'Project ID' (36 characters)\")\n\nverify_project()\n</code></pre></p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#83-environment-file-not-loading","title":"8.3 Environment File Not Loading","text":"<p>Problem: <code>.env</code> variables not available <pre><code>import os\nfrom pathlib import Path\n\ndef diagnose_env():\n    env_path = Path(\".env\")\n\n    if not env_path.exists():\n        print(\"\u274c .env file not found\")\n        print(f\"   Expected location: {env_path.absolute()}\")\n        print(\"\\nCreate .env file with:\")\n        print(\"\"\"\nWATSONX_URL=https://us-south.ml.cloud.ibm.com\nWATSONX_APIKEY=your_key_here\nWATSONX_PROJECT_ID=your_project_id_here\n        \"\"\")\n        return\n\n    print(f\"\u2713 .env file found at {env_path.absolute()}\")\n\n    # Check if dotenv is loaded\n    from dotenv import load_dotenv\n    load_dotenv()\n\n    required_vars = [\"WATSONX_URL\", \"WATSONX_APIKEY\", \"WATSONX_PROJECT_ID\"]\n    missing = [v for v in required_vars if not os.getenv(v)]\n\n    if missing:\n        print(f\"\u274c Missing variables: {missing}\")\n    else:\n        print(\"\u2713 All required variables loaded\")\n\ndiagnose_env()\n</code></pre></p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#84-docker-issues","title":"8.4 Docker Issues","text":"<p>Problem: Port already in use <pre><code># Check what's using port 8888\nlsof -i :8888\n\n# Kill the process\nkill -9 &lt;PID&gt;\n\n# Or use different port\ndocker-compose run -p 8889:8888 watsonx-workspace\n</code></pre></p> <p>Problem: Container won't start <pre><code># Check logs\ndocker-compose logs watsonx-workspace\n\n# Rebuild from scratch\ndocker-compose down -v\ndocker-compose build --no-cache\ndocker-compose up\n</code></pre></p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#9-quick-reference","title":"9. Quick Reference","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#91-file-paths-cheat-sheet","title":"9.1 File Paths Cheat Sheet","text":"<pre><code>watsonx-workshop/\n\u251c\u2500\u2500 simple-watsonx-environment/\n\u2502   \u251c\u2500\u2500 .env                          # Credentials (YOU CREATE)\n\u2502   \u251c\u2500\u2500 notebooks/\n\u2502   \u2502   \u251c\u2500\u2500 watsonx_quickstart.ipynb  # Start here\n\u2502   \u2502   \u251c\u2500\u2500 rag_watsonx.ipynb         # Lab 2.2\n\u2502   \u2502   \u2514\u2500\u2500 rag_twin_compare.ipynb    # Lab 2.3\n\u2502   \u2514\u2500\u2500 data/\n\u2502       \u2514\u2500\u2500 corpus/                   # Your documents\n\u2502\n\u251c\u2500\u2500 labs-src/                         # IBM reference notebooks\n\u2502   \u251c\u2500\u2500 use-watsonx-elasticsearch-and-langchain-to-answer-questions-rag.ipynb\n\u2502   \u251c\u2500\u2500 use-watsonx-and-elasticsearch-python-sdk-to-answer-questions-rag.ipynb\n\u2502   \u2514\u2500\u2500 use-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb\n\u2502\n\u2514\u2500\u2500 accelerator/                      # Production code\n    \u251c\u2500\u2500 rag/\n    \u2502   \u251c\u2500\u2500 retriever.py\n    \u2502   \u251c\u2500\u2500 pipeline.py\n    \u2502   \u2514\u2500\u2500 prompt.py\n    \u251c\u2500\u2500 service/\n    \u2502   \u251c\u2500\u2500 api.py\n    \u2502   \u2514\u2500\u2500 deps.py\n    \u251c\u2500\u2500 tools/\n    \u2502   \u251c\u2500\u2500 chunk.py\n    \u2502   \u251c\u2500\u2500 extract.py\n    \u2502   \u251c\u2500\u2500 embed_index.py\n    \u2502   \u2514\u2500\u2500 eval_small.py\n    \u2514\u2500\u2500 assets/\n        \u2514\u2500\u2500 notebook/                 # More IBM examples\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#92-command-cheat-sheet","title":"9.2 Command Cheat Sheet","text":"<pre><code># Environment setup\npython -m venv venv\nsource venv/bin/activate  # or: venv\\Scripts\\activate on Windows\npip install -r requirements.txt\n\n# Docker\nmake build\nmake run\nmake stop\n\n# Run notebook\njupyter notebook notebooks/watsonx_quickstart.ipynb\n\n# Run accelerator service\ncd accelerator\npython -m service.api\n\n# Run evaluation\npython -m tools.eval_small \\\n    --api-url http://localhost:8000/ask \\\n    --input tests/eval_input.csv \\\n    --output results/eval_results.csv\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#93-essential-imports","title":"9.3 Essential Imports","text":"<pre><code># watsonx.ai basics\nfrom ibm_watsonx_ai import Credentials, APIClient\nfrom ibm_watsonx_ai.foundation_models import Model, Embeddings\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n\n# LangChain with watsonx\nfrom langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n\n# Vector stores\nfrom langchain_chroma import Chroma\nfrom langchain_elasticsearch import ElasticsearchStore\n\n# Governance\nfrom ibm_watsonx_gov.tools import load_tool\nfrom ibm_watsonx_gov.evaluators.agentic_evaluator import AgenticEvaluator\n\n# LangGraph\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.prebuilt import create_react_agent\n</code></pre>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#10-next-steps","title":"10. Next Steps","text":"<p>After completing supplementary materials:</p> <p>Immediate: 1. \u2705 Run <code>watsonx_quickstart.ipynb</code> successfully 2. \u2705 Register at least one custom tool 3. \u2705 Review one reference notebook from labs-src/</p> <p>This Week: 1. \u2705 Implement Docker-based workflow 2. \u2705 Set up experiment tracking 3. \u2705 Build a simple governed agent</p> <p>This Month: 1. \u2705 Integrate accelerator components 2. \u2705 Deploy RAG service with FastAPI 3. \u2705 Set up continuous evaluation pipeline</p>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#11-additional-resources","title":"11. Additional Resources","text":""},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#documentation","title":"Documentation","text":"<ul> <li>watsonx.ai Docs: https://www.ibm.com/docs/en/watsonx-as-a-service</li> <li>watsonx.governance: https://www.ibm.com/docs/en/watsonx/saas?topic=governance-overview</li> <li>LangGraph: https://langchain-ai.github.io/langgraph/</li> <li>FastAPI: https://fastapi.tiangolo.com/</li> </ul>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#ibm-specific","title":"IBM-Specific","text":"<ul> <li>Tool Catalog: https://dataplatform.cloud.ibm.com/aigov/modelinventory/ai-tools</li> <li>Evaluation Studio: Access via watsonx.governance UI</li> <li>Cloud API Keys: https://cloud.ibm.com/iam/apikeys</li> </ul>"},{"location":"tracks/day2-rag/Day2_Supplementary_Material_Accelerator_IBM_Tooling/#community","title":"Community","text":"<ul> <li>IBM Community: https://community.ibm.com/community/user/watsonx/home</li> <li>Stack Overflow: Tag <code>ibm-watsonx</code></li> </ul> <p>Version: 1.0 Last Updated: January 2025 Maintained by: IBM watsonx.ai Education Team</p> <p>This supplementary guide complements the main Day 2 RAG workshop materials with IBM-specific tooling and production deployment patterns.</p>"},{"location":"tracks/day2-rag/START_HERE/","title":"\ud83c\udfaf Day 2 RAG Workshop - START HERE","text":""},{"location":"tracks/day2-rag/START_HERE/#welcome","title":"Welcome!","text":"<p>You've received a complete workshop package for teaching Retrieval Augmented Generation (RAG).</p> <p>Package includes: - \u2705 Complete theory documentation - \u2705 4 hands-on labs with solutions - \u2705 Instructor guide with timing and tips - \u2705 All code examples tested and working - \u2705 Production deployment patterns</p> <p>Duration: 8 hours (4h theory + 4h labs)</p>"},{"location":"tracks/day2-rag/START_HERE/#which-document-should-i-read-first","title":"\ud83d\udcd6 Which Document Should I Read First?","text":""},{"location":"tracks/day2-rag/START_HERE/#if-youre-an-instructor-preparing-to-teach","title":"If you're an INSTRUCTOR preparing to teach:","text":"<p>\u2192 Read First: <code>Day2_RAG_Instructor_Guide.md</code> - Complete session plans - Teaching tips and timing - Common issues and solutions - Assessment rubrics</p> <p>\u2192 Read Second: <code>Theory_01_RAG_Architecture_Overview.md</code> - Theory content you'll teach - Concepts and examples - Demos to prepare</p> <p>\u2192 Keep Handy: <code>Day2_RAG_Complete_Solutions_Guide.md</code> - All lab solutions - Quick reference during workshop - Troubleshooting guide</p>"},{"location":"tracks/day2-rag/START_HERE/#if-youre-a-student-taking-the-workshop","title":"If you're a STUDENT taking the workshop:","text":"<p>\u2192 Read First: <code>Day2_RAG_Workshop_README.md</code> - Workshop overview - Prerequisites and setup - Daily schedule - What to expect</p> <p>\u2192 During Theory: <code>Theory_01_RAG_Architecture_Overview.md</code> - Follow along with instructor - Take notes on key concepts</p> <p>\u2192 During Labs:  1. <code>labs/lab_2.1_local_rag_ollama/Lab_2.1_Instructions.md</code> 2. Reference <code>Day2_RAG_Complete_Solutions_Guide.md</code> when stuck</p>"},{"location":"tracks/day2-rag/START_HERE/#if-youre-self-studying","title":"If you're SELF-STUDYING:","text":"<p>Day 1: 1. Read <code>Day2_RAG_Workshop_README.md</code> (30 min) 2. Complete technical setup (1 hour) 3. Study <code>Theory_01_RAG_Architecture_Overview.md</code> (2 hours)</p> <p>Day 2: 1. Complete Lab 2.1 (90 min) 2. Complete Lab 2.2 (90 min)</p> <p>Day 3: 1. Complete Lab 2.3 (90 min) 2. Complete Lab 2.4 (60 min) 3. Review solutions and extend</p>"},{"location":"tracks/day2-rag/START_HERE/#complete-file-list","title":"\ud83d\udcc1 Complete File List","text":"<pre><code>outputs/\n\u251c\u2500\u2500 START_HERE.md                          \u2190 You are here!\n\u251c\u2500\u2500 Package_Summary_and_Navigation.md      \u2190 Detailed navigation guide\n\u251c\u2500\u2500 Day2_RAG_Workshop_README.md            \u2190 Student overview\n\u251c\u2500\u2500 Day2_RAG_Instructor_Guide.md           \u2190 Complete instructor manual\n\u251c\u2500\u2500 Theory_01_RAG_Architecture_Overview.md \u2190 Theory content\n\u251c\u2500\u2500 Day2_RAG_Complete_Solutions_Guide.md   \u2190 All labs + solutions\n\u2514\u2500\u2500 labs/\n    \u2514\u2500\u2500 lab_2.1_local_rag_ollama/\n        \u2514\u2500\u2500 Lab_2.1_Instructions.md        \u2190 Lab 2.1 detailed instructions\n</code></pre>"},{"location":"tracks/day2-rag/START_HERE/#quick-links","title":"\u26a1 Quick Links","text":"I want to... Go to this file Section Understand what RAG is Theory_01_RAG_Architecture_Overview.md Section 1 See complete Lab 2.1 code Day2_RAG_Complete_Solutions_Guide.md Lab 2.1 Solution Know workshop schedule Day2_RAG_Workshop_README.md Workshop Timeline Find teaching tips Day2_RAG_Instructor_Guide.md Teaching Tips Get setup instructions Day2_RAG_Workshop_README.md Technical Setup See evaluation rubric Day2_RAG_Instructor_Guide.md Assessment &amp; Grading Troubleshoot issues Day2_RAG_Complete_Solutions_Guide.md Troubleshooting Map to accelerator Theory_01_RAG_Architecture_Overview.md Section 5"},{"location":"tracks/day2-rag/START_HERE/#what-youll-learn","title":"\ud83c\udf93 What You'll Learn","text":""},{"location":"tracks/day2-rag/START_HERE/#theory-4-hours","title":"Theory (4 hours)","text":"<ul> <li>\u2705 RAG architecture and components</li> <li>\u2705 Embedding models and vector stores</li> <li>\u2705 Trade-offs in RAG design</li> <li>\u2705 Production deployment patterns</li> </ul>"},{"location":"tracks/day2-rag/START_HERE/#labs-4-hours","title":"Labs (4 hours)","text":"<ul> <li>\u2705 Lab 2.1: Build local RAG with Ollama + Chroma</li> <li>\u2705 Lab 2.2: Build enterprise RAG with watsonx.ai</li> <li>\u2705 Lab 2.3: Compare multiple RAG backends</li> <li>\u2705 Lab 2.4: Create automated evaluation harness</li> </ul>"},{"location":"tracks/day2-rag/START_HERE/#prerequisites-check","title":"\u2705 Prerequisites Check","text":"<p>Before starting, ensure you have:</p> <p>Knowledge: - [ ] Python programming (intermediate) - [ ] Basic understanding of LLMs - [ ] Completed Day 1 workshop (or equivalent)</p> <p>Software: - [ ] Python 3.10+ - [ ] Jupyter notebooks - [ ] Ollama installed - [ ] Required Python packages</p> <p>Credentials: - [ ] watsonx.ai API key (for Lab 2.2) - [ ] watsonx.ai project ID - [ ] (Optional) Elasticsearch access</p> <p>Setup verification command: <pre><code>python -c \"import langchain; import chromadb; print('\u2713 Ready!')\"\n</code></pre></p>"},{"location":"tracks/day2-rag/START_HERE/#getting-started-in-3-steps","title":"\ud83d\ude80 Getting Started in 3 Steps","text":""},{"location":"tracks/day2-rag/START_HERE/#step-1-choose-your-path","title":"Step 1: Choose Your Path","text":"<ul> <li>Teaching? \u2192 Read <code>Day2_RAG_Instructor_Guide.md</code></li> <li>Learning? \u2192 Read <code>Day2_RAG_Workshop_README.md</code></li> <li>Just browsing? \u2192 Read <code>Package_Summary_and_Navigation.md</code></li> </ul>"},{"location":"tracks/day2-rag/START_HERE/#step-2-set-up-environment","title":"Step 2: Set Up Environment","text":"<pre><code># Install dependencies\npip install langchain langchain-community chromadb\npip install sentence-transformers ibm-watsonx-ai\n\n# Install Ollama\ncurl https://ollama.ai/install.sh | sh\nollama pull llama2\n</code></pre>"},{"location":"tracks/day2-rag/START_HERE/#step-3-start-learning","title":"Step 3: Start Learning!","text":"<ul> <li>Open your first document</li> <li>Follow along with examples</li> <li>Complete labs at your own pace</li> </ul>"},{"location":"tracks/day2-rag/START_HERE/#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"<p>For Best Results: 1. Don't skip theory - Understanding concepts makes labs easier 2. Type the code - Don't just copy/paste, understand each line 3. Experiment - Try different parameters and observe results 4. Map to accelerator - Think about production patterns 5. Ask questions - Use provided support channels</p> <p>If You Get Stuck: 1. Check the troubleshooting section 2. Review the relevant theory section 3. Look at the complete solution 4. Ask for help (don't struggle alone!)</p>"},{"location":"tracks/day2-rag/START_HERE/#whats-included","title":"\ud83d\udcca What's Included","text":""},{"location":"tracks/day2-rag/START_HERE/#documentation-statistics","title":"Documentation Statistics","text":"<ul> <li>Total lines: ~4,350 lines</li> <li>Code examples: 50+ complete examples</li> <li>Exercises: 20+ hands-on activities</li> <li>Diagrams: 5+ conceptual diagrams</li> </ul>"},{"location":"tracks/day2-rag/START_HERE/#content-breakdown","title":"Content Breakdown","text":"<ul> <li>Theory modules: 4 comprehensive modules</li> <li>Hands-on labs: 4 progressive labs</li> <li>Solutions: Complete working code</li> <li>Reference: Production patterns</li> </ul>"},{"location":"tracks/day2-rag/START_HERE/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this workshop, you will be able to:</p> <ol> <li>\u2705 Explain RAG architecture and its benefits</li> <li>\u2705 Implement document chunking and embedding</li> <li>\u2705 Build and query vector stores</li> <li>\u2705 Create end-to-end RAG pipelines</li> <li>\u2705 Compare different RAG implementations</li> <li>\u2705 Evaluate RAG system performance</li> <li>\u2705 Deploy production-ready RAG services</li> <li>\u2705 Integrate with AI Accelerator framework</li> </ol>"},{"location":"tracks/day2-rag/START_HERE/#success-criteria","title":"\ud83c\udfc6 Success Criteria","text":"<p>You'll know you're successful when you can: - [ ] Build a working RAG pipeline from scratch - [ ] Explain trade-offs in RAG design - [ ] Choose appropriate components for your use case - [ ] Evaluate RAG performance with metrics - [ ] Deploy a RAG service to production</p>"},{"location":"tracks/day2-rag/START_HERE/#need-help","title":"\ud83d\udcde Need Help?","text":""},{"location":"tracks/day2-rag/START_HERE/#finding-information","title":"Finding Information","text":"<p>Can't find something? \u2192 Check <code>Package_Summary_and_Navigation.md</code> - comprehensive index</p> <p>Need teaching tips? \u2192 Check <code>Day2_RAG_Instructor_Guide.md</code> - section-by-section guidance</p> <p>Code not working? \u2192 Check <code>Day2_RAG_Complete_Solutions_Guide.md</code> - troubleshooting section</p>"},{"location":"tracks/day2-rag/START_HERE/#support-channels","title":"Support Channels","text":"<ul> <li>Workshop Slack: #day2-rag-workshop</li> <li>Office hours: Schedule via workshop portal</li> <li>Documentation: All materials in this package</li> </ul>"},{"location":"tracks/day2-rag/START_HERE/#highlights","title":"\ud83c\udf1f Highlights","text":""},{"location":"tracks/day2-rag/START_HERE/#why-this-package-is-great","title":"Why This Package is Great:","text":"<ol> <li>Complete - Everything you need in one place</li> <li>Tested - All code examples work</li> <li>Flexible - Adapt for different audiences</li> <li>Production-Ready - Real-world patterns</li> <li>Comprehensive - Theory + practice + solutions</li> </ol>"},{"location":"tracks/day2-rag/START_HERE/#typical-workshop-flow","title":"\ud83d\udcc5 Typical Workshop Flow","text":"<pre><code>9:00 AM  - Welcome &amp; Setup\n9:15 AM  - Module 1: RAG Architecture\n10:15 AM - Module 2: Embeddings\n11:00 AM - BREAK\n11:15 AM - Module 3: Vector Stores\n12:15 PM - Module 4: Accelerator\n1:00 PM  - LUNCH\n2:00 PM  - Lab 2.1: Local RAG\n3:00 PM  - Lab 2.2: watsonx RAG\n4:00 PM  - BREAK\n4:15 PM  - Lab 2.3: Comparison\n5:15 PM  - Lab 2.4: Evaluation\n6:00 PM  - Wrap-up\n</code></pre>"},{"location":"tracks/day2-rag/START_HERE/#next-steps-after-this-workshop","title":"\ud83c\udf93 Next Steps After This Workshop","text":"<p>Immediate: 1. Complete any unfinished labs 2. Review solutions and compare with yours 3. Try extension exercises</p> <p>This Week: 1. Apply RAG to your own use case 2. Experiment with different configurations 3. Share learnings with team</p> <p>This Month: 1. Deploy a production RAG system 2. Implement evaluation pipeline 3. Integrate with accelerator framework</p>"},{"location":"tracks/day2-rag/START_HERE/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<p>Documentation: - LangChain: https://python.langchain.com/ - watsonx.ai: https://www.ibm.com/docs/en/watsonx-as-a-service - Chroma: https://docs.trychroma.com/</p> <p>Papers: - \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" - \"Dense Passage Retrieval for Open-Domain Question Answering\"</p> <p>Community: - LangChain Discord - r/MachineLearning - AI/ML meetups</p>"},{"location":"tracks/day2-rag/START_HERE/#ready-to-begin","title":"\u2728 Ready to Begin?","text":"<p>Your next step:</p> <p>If teaching \u2192 Open <code>Day2_RAG_Instructor_Guide.md</code> If learning \u2192 Open <code>Day2_RAG_Workshop_README.md</code> If browsing \u2192 Open <code>Package_Summary_and_Navigation.md</code></p> <p>Good luck! \ud83d\ude80</p> <p>Questions? Check the documentation or ask for help!</p> <p>Package Version: 1.0 Last Updated: January 2025 License: MIT Maintained by: IBM watsonx.ai Education Team</p>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/","title":"Day 2 Supplementary Materials - Quick Start Guide","text":""},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#whats-in-the-supplementary-package","title":"\ud83c\udfaf What's in the Supplementary Package?","text":"<p>This supplementary guide covers IBM-specific tooling and production patterns not included in the main workshop:</p>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#topics-covered","title":"\ud83d\udce6 Topics Covered","text":"<ol> <li>Accelerator Project Integration - Production code structure</li> <li>IBM Reference Notebooks - Examples from labs-src/ and accelerator/assets/</li> <li>Docker Setup - Container-based development</li> <li>watsonx_quickstart.ipynb - Environment verification</li> <li>Governed Agentic Catalog - Custom tool registration</li> <li>Evaluation Studio - Experiment tracking</li> <li>Advanced Examples - Banking &amp; HR assistants</li> </ol>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#quick-navigation","title":"\ud83d\ude80 Quick Navigation","text":""},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#i-want-to","title":"I want to...","text":"Goal Go to Section Time Understand accelerator structure Section 1 10 min See IBM notebook examples Section 2 15 min Set up Docker environment Section 3 20 min Verify watsonx setup Section 4 10 min Register custom tools Section 5 15 min Track experiments Section 6 20 min Build LangGraph agent Section 7 30 min"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#prerequisites","title":"\u2705 Prerequisites","text":"<p>Before using supplementary materials:</p> <p>Required: - [x] Completed main Day 2 workshop (or Labs 2.1-2.4) - [x] watsonx.ai account and credentials - [x] Basic understanding of RAG concepts</p> <p>Optional: - [ ] Docker installed (for Section 3) - [ ] watsonx.governance access (for Sections 5-6) - [ ] Elasticsearch instance (for reference notebooks)</p>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#learning-path","title":"\ud83c\udf93 Learning Path","text":""},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#path-1-quick-integration-30-minutes","title":"Path 1: Quick Integration (30 minutes)","text":"<pre><code>1. Section 1.2: Lab-to-Accelerator Mapping (10 min)\n2. Section 4: Run watsonx_quickstart.ipynb (10 min)\n3. Section 1.5: Review FastAPI endpoint (10 min)\n</code></pre>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#path-2-ibm-tooling-deep-dive-2-hours","title":"Path 2: IBM Tooling Deep Dive (2 hours)","text":"<pre><code>1. Section 2.1: Review reference notebooks (30 min)\n2. Section 5: Register custom tool (30 min)\n3. Section 6: Set up experiment tracking (30 min)\n4. Section 7: Build example agent (30 min)\n</code></pre>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#path-3-production-deployment-4-hours","title":"Path 3: Production Deployment (4 hours)","text":"<pre><code>1. Section 1: Full accelerator integration (1 hour)\n2. Section 3: Docker setup (1 hour)\n3. Section 2.2: Accelerator notebooks (1 hour)\n4. Section 8: Troubleshooting (1 hour)\n</code></pre>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#file-structure","title":"\ud83d\udcc1 File Structure","text":"<pre><code>supplementary-materials/\n\u251c\u2500\u2500 SUPPLEMENTARY_QUICK_START.md              \u2190 You are here\n\u2514\u2500\u2500 Day2_Supplementary_Material_Accelerator_IBM_Tooling.md  \u2190 Full guide\n\naccelerator/                                   \u2190 Production code\n\u251c\u2500\u2500 rag/\n\u2502   \u251c\u2500\u2500 retriever.py                          \u2190 From Lab 2.1/2.2\n\u2502   \u251c\u2500\u2500 pipeline.py                           \u2190 From Lab 2.2/2.3\n\u2502   \u2514\u2500\u2500 prompt.py                             \u2190 From Lab 2.2\n\u251c\u2500\u2500 service/\n\u2502   \u251c\u2500\u2500 api.py                                \u2190 FastAPI endpoints\n\u2502   \u2514\u2500\u2500 deps.py                               \u2190 Configuration\n\u2514\u2500\u2500 tools/\n    \u251c\u2500\u2500 eval_small.py                         \u2190 From Lab 2.4\n\nsimple-watsonx-environment/\n\u251c\u2500\u2500 .env                                      \u2190 YOUR CREDENTIALS\n\u2514\u2500\u2500 notebooks/\n    \u2514\u2500\u2500 watsonx_quickstart.ipynb              \u2190 START HERE\n</code></pre>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#quick-setup","title":"\ud83d\udd27 Quick Setup","text":""},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#1-verify-watsonxai-access-5-minutes","title":"1. Verify watsonx.ai Access (5 minutes)","text":"<pre><code># Clone if needed\ngit clone &lt;your-repo&gt;\ncd simple-watsonx-environment\n\n# Create .env file\ncat &gt; .env &lt;&lt; 'ENVEOF'\nWATSONX_URL=https://us-south.ml.cloud.ibm.com\nWATSONX_APIKEY=your_key_here\nWATSONX_PROJECT_ID=your_project_id_here\nENVEOF\n\n# Run quickstart\njupyter notebook notebooks/watsonx_quickstart.ipynb\n</code></pre> <p>Expected Output: <pre><code>\u2713 Credentials loaded\n\u2713 API client initialized\n\u2713 Found 50+ available models\n\u2713 Granite model: Working\n\u2713 Embeddings: Working\nReady for Day 2 labs! \ud83d\ude80\n</code></pre></p>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#2-try-a-reference-notebook-10-minutes","title":"2. Try a Reference Notebook (10 minutes)","text":"<pre><code># Navigate to reference notebooks\ncd labs-src/\n\n# Open Chroma example (easiest to start)\njupyter notebook use-watsonx-chroma-and-langchain-to-answer-questions-rag.ipynb\n\n# Follow along - it demonstrates:\n# - Chroma setup with watsonx embeddings\n# - Granite model integration\n# - Full RAG pipeline\n</code></pre>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#3-register-your-first-tool-15-minutes","title":"3. Register Your First Tool (15 minutes)","text":"<pre><code># In a notebook\nfrom ibm_watsonx_gov.tools.clients import register_tool, ToolRegistrationPayload\n\n# Simple example tool\ntool_code = \"\"\"\ndef hello_world(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\"\"\"\n\ntool_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"}\n    },\n    \"required\": [\"name\"]\n}\n\npayload = ToolRegistrationPayload(\n    tool_name=\"hello_world\",\n    description=\"A simple greeting tool\",\n    code={\n        \"source_code_base64\": tool_code,\n        \"run_time_details\": {\"engine\": \"python 3.11\"}\n    },\n    schema=tool_schema,\n    category=[\"Example\"]\n)\n\nresponse = register_tool(payload)\nprint(f\"\u2713 Registered: {response['metadata']['asset_id']}\")\n</code></pre>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#key-concepts","title":"\ud83d\udca1 Key Concepts","text":""},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#accelerator-pattern","title":"Accelerator Pattern","text":"<pre><code>Labs (Prototype)          \u2192    Accelerator (Production)\n------------------------       ---------------------------\nNotebook cells            \u2192    Python modules\nInline functions          \u2192    Classes with error handling\nManual testing            \u2192    Automated tests\nSingle file               \u2192    Structured project\nLocal only                \u2192    Docker + cloud deployment\n</code></pre>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#ibm-reference-notebooks","title":"IBM Reference Notebooks","text":"<p>Three Types:</p> <ol> <li>labs-src/ - RAG patterns (Elasticsearch, Chroma, LangChain)</li> <li>accelerator/assets/notebook/ - Ingestion, deployment, evaluation</li> <li>Your notebooks - Custom implementations</li> </ol> <p>When to Use Each: - Starting out? \u2192 Use labs-src/ examples - Building ingestion? \u2192 Use accelerator ingestion notebooks - Deploying? \u2192 Use Create_and_Deploy notebooks - Testing? \u2192 Use Test_Queries notebooks</p>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#governed-tools","title":"Governed Tools","text":"<p>Concept: Tools in a central catalog with: - \u2705 Version control - \u2705 Access governance - \u2705 Usage tracking - \u2705 Audit logs</p> <p>Benefits: - Reuse across projects - Consistent tool behavior - Governance compliance - Easy updates</p>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#common-tasks","title":"\ud83c\udfaf Common Tasks","text":""},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#task-1-map-lab-code-to-accelerator","title":"Task 1: Map Lab Code to Accelerator","text":"<pre><code># Your Lab 2.2 notebook\ndef retrieve(query):\n    return vectorstore.similarity_search(query, k=4)\n\ndef answer(query):\n    docs = retrieve(query)\n    prompt = build_prompt(query, docs)\n    return llm(prompt)\n\n# Becomes: accelerator/rag/retriever.py\nclass Retriever:\n    def retrieve(self, query: str, k: int = 4) -&gt; List[Chunk]:\n        # Production version with error handling\n        pass\n\n# And: accelerator/rag/pipeline.py\nclass RAGPipeline:\n    def answer_question(self, query: str) -&gt; dict:\n        # Production version with metrics\n        pass\n</code></pre>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#task-2-run-reference-notebook","title":"Task 2: Run Reference Notebook","text":"<pre><code># 1. Navigate to labs-src\ncd labs-src/\n\n# 2. Copy credentials\ncp ../simple-watsonx-environment/.env .\n\n# 3. Install extra deps if needed\npip install elasticsearch  # for ES examples\n\n# 4. Run notebook\njupyter notebook use-watsonx-elasticsearch-and-langchain-to-answer-questions-rag.ipynb\n\n# 5. Follow along and note patterns\n</code></pre>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#task-3-set-up-docker","title":"Task 3: Set Up Docker","text":"<p>```bash</p>"},{"location":"tracks/day2-rag/SUPPLEMENTARY_QUICK_START/#1-create-dockerfile-see-section-31","title":"1. Create Dockerfile (see Section 3.1)","text":"<p>cat &gt; Dockerfile &lt;&lt; 'EOF' FROM python:3.11-slim WORKDIR /workspace COPY requirements.txt . RUN pip install -r requirements.txt CMD [\"jupyter\", \"lab\", \"--ip=0.0.0.0\", \"--allow-root\"]</p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/","title":"Module 1: RAG Architecture Overview","text":""},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#learning-objectives","title":"\ud83d\udcda Learning Objectives","text":"<p>By the end of this module, you will be able to: - Define Retrieval Augmented Generation and explain its value - Identify the core components of a RAG system - Describe the typical RAG flow from ingestion to generation - Understand trade-offs in RAG system design - Map RAG concepts to the AI Accelerator framework</p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#1-what-is-rag","title":"1. What is RAG?","text":""},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#definition","title":"Definition","text":"<p>Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge retrieved from a document corpus during the generation process.</p> <pre><code>Traditional LLM:           RAG-Enhanced LLM:\n\nUser Query                User Query\n    \u2193                          \u2193\n   LLM  \u2192  Answer          Retriever \u2192 Relevant Docs\n                                \u2193\n                            LLM + Context  \u2192  Grounded Answer\n</code></pre>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#the-problem-rag-solves","title":"The Problem RAG Solves","text":"<p>Challenge: LLMs have several limitations: 1. Knowledge Cutoff: Training data is frozen at a point in time 2. Hallucinations: Models may generate plausible but incorrect information 3. Domain Specificity: Lack of specialized or proprietary knowledge 4. Attribution: Difficulty in citing sources for generated content</p> <p>Solution: RAG addresses these by: - Retrieving up-to-date, relevant information at query time - Grounding responses in actual source documents - Enabling domain-specific knowledge without retraining - Providing source attribution for transparency</p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#real-world-use-cases","title":"Real-World Use Cases","text":"<ol> <li>Customer Support Chatbots</li> <li>Query: \"How do I reset my password?\"</li> <li>RAG retrieves: Latest help documentation</li> <li> <p>Response: Current, accurate reset procedure</p> </li> <li> <p>Legal Document Analysis</p> </li> <li>Query: \"What are precedents for contract disputes?\"</li> <li>RAG retrieves: Relevant case law and contracts</li> <li> <p>Response: Contextual legal analysis with citations</p> </li> <li> <p>Medical Information Systems</p> </li> <li>Query: \"Treatment options for condition X?\"</li> <li>RAG retrieves: Latest medical literature and guidelines</li> <li> <p>Response: Evidence-based treatment recommendations</p> </li> <li> <p>Enterprise Knowledge Management</p> </li> <li>Query: \"What's our policy on remote work?\"</li> <li>RAG retrieves: Company policy documents</li> <li>Response: Accurate policy information with references</li> </ol>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#2-core-components-of-rag","title":"2. Core Components of RAG","text":""},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#21-document-store-corpus","title":"2.1 Document Store / Corpus","text":"<p>Purpose: The knowledge base containing documents to be searched</p> <p>Characteristics: - Format: PDFs, HTML, markdown, plain text, structured data - Scale: From hundreds to millions of documents - Domain: General knowledge, domain-specific, or proprietary data</p> <p>Example: <pre><code># Document structure\ndocument = {\n    \"id\": \"doc_12345\",\n    \"title\": \"Employee Handbook 2024\",\n    \"text\": \"Company policies and procedures...\",\n    \"metadata\": {\n        \"author\": \"HR Department\",\n        \"date\": \"2024-01-15\",\n        \"category\": \"policies\"\n    }\n}\n</code></pre></p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#22-chunking-preprocessing","title":"2.2 Chunking / Preprocessing","text":"<p>Purpose: Split large documents into manageable, semantic units</p> <p>Why Chunking Matters: - Embedding models have token limits (e.g., 512 tokens) - Smaller chunks = more precise retrieval - Balanced chunk size improves relevance</p> <p>Chunking Strategies:</p> <ol> <li> <p>Fixed-Size Chunking <pre><code>chunk_size = 500  # characters or tokens\nchunk_overlap = 50  # overlap for context continuity\n</code></pre></p> </li> <li> <p>Semantic Chunking</p> </li> <li>Split by paragraphs, sections, or sentences</li> <li>Preserve natural text boundaries</li> <li> <p>Better semantic coherence</p> </li> <li> <p>Recursive Chunking <pre><code># Try to split by:\n# 1. Double newline (paragraphs)\n# 2. Single newline (sentences)\n# 3. Spaces (words)\n# 4. Characters (last resort)\n</code></pre></p> </li> </ol> <p>Best Practices: - Include some overlap between chunks - Preserve document metadata in each chunk - Test different chunk sizes for your use case</p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#23-embeddings","title":"2.3 Embeddings","text":"<p>Purpose: Convert text into dense vector representations for semantic similarity</p> <p>How Embeddings Work: <pre><code>Text: \"The cat sat on the mat\"\n       \u2193\nEmbedding Model\n       \u2193\nVector: [0.23, -0.45, 0.67, ..., 0.12]  # 384-1536 dimensions\n</code></pre></p> <p>Popular Embedding Models:</p> Model Dimensions Use Case Provider all-MiniLM-L6-v2 384 Fast, lightweight HuggingFace all-mpnet-base-v2 768 Balanced performance HuggingFace text-embedding-ada-002 1536 High quality OpenAI slate-30m-english-rtrvr 384 Retrieval-optimized watsonx.ai <p>Key Concepts: - Semantic Similarity: Similar meanings = similar vectors - Cosine Similarity: Measures angle between vectors (0-1) - Dense vs Sparse: Dense = all dimensions used; Sparse = mostly zeros</p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#24-vector-store","title":"2.4 Vector Store","text":"<p>Purpose: Specialized database for storing and searching embeddings</p> <p>Why Not Regular Databases? - Traditional DBs: Exact match (WHERE clause) - Vector DBs: Similarity search (nearest neighbors)</p> <p>Popular Vector Stores:</p> <ol> <li> <p>Chroma - Local, lightweight    <pre><code>from langchain_chroma import Chroma\nvectorstore = Chroma.from_documents(docs, embeddings)\n</code></pre></p> </li> <li> <p>Elasticsearch - Enterprise, scalable    <pre><code>from langchain_elasticsearch import ElasticsearchStore\nvectorstore = ElasticsearchStore(\n    elasticsearch_url=url,\n    index_name=\"my_index\"\n)\n</code></pre></p> </li> <li> <p>FAISS - High-performance, in-memory    <pre><code>from langchain_community.vectorstores import FAISS\nvectorstore = FAISS.from_documents(docs, embeddings)\n</code></pre></p> </li> </ol> <p>Vector Store Operations: <pre><code># Add documents\nvectorstore.add_documents(documents)\n\n# Similarity search\nresults = vectorstore.similarity_search(\n    query=\"What is the refund policy?\",\n    k=5  # return top 5 results\n)\n\n# Similarity search with scores\nresults = vectorstore.similarity_search_with_score(\n    query=\"What is the refund policy?\",\n    k=5\n)\n</code></pre></p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#25-retriever","title":"2.5 Retriever","text":"<p>Purpose: Interface to query the vector store and return relevant documents</p> <p>Retrieval Strategies:</p> <ol> <li> <p>Similarity Search <pre><code>retriever = vectorstore.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 4}\n)\n</code></pre></p> </li> <li> <p>MMR (Maximum Marginal Relevance)</p> </li> <li>Balances relevance and diversity</li> <li> <p>Avoids redundant results    <pre><code>retriever = vectorstore.as_retriever(\n    search_type=\"mmr\",\n    search_kwargs={\"k\": 4, \"fetch_k\": 20}\n)\n</code></pre></p> </li> <li> <p>Similarity with Threshold <pre><code>retriever = vectorstore.as_retriever(\n    search_type=\"similarity_score_threshold\",\n    search_kwargs={\"score_threshold\": 0.7, \"k\": 4}\n)\n</code></pre></p> </li> </ol>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#26-llm-prompt","title":"2.6 LLM + Prompt","text":"<p>Purpose: Generate answers using retrieved context</p> <p>Prompt Pattern: <pre><code>prompt_template = \"\"\"\nUse the following context to answer the question.\nIf you don't know the answer, say you don't know.\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:\n\"\"\"\n</code></pre></p> <p>LLM Options: - Local: Ollama (llama2, mistral) - Cloud: watsonx.ai (Granite), OpenAI (GPT), Anthropic (Claude)</p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#3-typical-rag-flow","title":"3. Typical RAG Flow","text":""},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#complete-rag-pipeline","title":"Complete RAG Pipeline","text":"<pre><code>graph LR\n    A[Raw Documents] --&gt;|1. Extract| B[Text Content]\n    B --&gt;|2. Chunk| C[Document Chunks]\n    C --&gt;|3. Embed| D[Vectors]\n    D --&gt;|4. Index| E[Vector Store]\n\n    F[User Query] --&gt;|5. Embed| G[Query Vector]\n    G --&gt;|6. Search| E\n    E --&gt;|7. Retrieve| H[Relevant Chunks]\n    H --&gt;|8. Context| I[LLM]\n    F --&gt;|9. Question| I\n    I --&gt;|10. Generate| J[Answer]</code></pre>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#phase-1-ingestion-one-time-setup","title":"Phase 1: Ingestion (One-Time Setup)","text":"<p>Step 1: Extract <pre><code># Extract text from various formats\nfrom langchain.document_loaders import (\n    PyPDFLoader,\n    TextLoader,\n    UnstructuredHTMLLoader\n)\n\nloader = PyPDFLoader(\"employee_handbook.pdf\")\ndocuments = loader.load()\n</code></pre></p> <p>Step 2: Chunk <pre><code>from langchain.text_splitters import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\nchunks = splitter.split_documents(documents)\n</code></pre></p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#phase-2-indexing-one-time-setup","title":"Phase 2: Indexing (One-Time Setup)","text":"<p>Step 3: Embed <pre><code>from langchain_community.embeddings import HuggingFaceEmbeddings\n\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"all-MiniLM-L6-v2\"\n)\n</code></pre></p> <p>Step 4: Index <pre><code>from langchain_chroma import Chroma\n\nvectorstore = Chroma.from_documents(\n    documents=chunks,\n    embedding=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n</code></pre></p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#phase-3-retrieval-per-query","title":"Phase 3: Retrieval (Per Query)","text":"<p>Step 5-7: Query &amp; Retrieve <pre><code># User asks a question\nquery = \"What is the vacation policy?\"\n\n# Retrieve relevant chunks\nretriever = vectorstore.as_retriever(k=4)\nrelevant_docs = retriever.get_relevant_documents(query)\n\n# Each doc contains:\n# - page_content: The text chunk\n# - metadata: Document info\n</code></pre></p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#phase-4-generation-per-query","title":"Phase 4: Generation (Per Query)","text":"<p>Step 8-10: Generate Answer <pre><code>from langchain.chains import RetrievalQA\nfrom langchain_community.llms import Ollama\n\nllm = Ollama(model=\"llama2\")\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    retriever=retriever,\n    return_source_documents=True\n)\n\nresult = qa_chain({\"query\": query})\nprint(result['result'])  # Answer\nprint(result['source_documents'])  # Sources\n</code></pre></p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#4-trade-offs-in-rag-design","title":"4. Trade-Offs in RAG Design","text":""},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#41-latency-vs-accuracy","title":"4.1 Latency vs Accuracy","text":"<p>Latency Factors: - Embedding model size - Vector store query time - Number of retrieved chunks (k) - LLM size and parameters</p> <p>Optimization Strategies: <pre><code># Faster (lower latency)\nembeddings = HuggingFaceEmbeddings(\"all-MiniLM-L6-v2\")  # 384 dim\nk = 3  # fewer chunks\nllm = Ollama(\"llama2:7b\")  # smaller model\n\n# More accurate (higher latency)\nembeddings = HuggingFaceEmbeddings(\"all-mpnet-base-v2\")  # 768 dim\nk = 10  # more chunks\nllm = WatsonxLLM(\"ibm/granite-13b-chat-v2\")  # larger model\n</code></pre></p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#42-embedding-model-selection","title":"4.2 Embedding Model Selection","text":"<p>Considerations: - Domain: General vs specialized (legal, medical, etc.) - Language: Multilingual support - Dimensions: Higher = more nuance, slower - License: Open-source vs proprietary</p> <p>Comparison: | Criteria | Small Model | Large Model | |----------|-------------|-------------| | Speed | \u26a1\u26a1\u26a1 | \u26a1 | | Accuracy | \u2b50\u2b50 | \u2b50\u2b50\u2b50 | | Memory | \ud83d\udcbe | \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe | | Cost | $ | $$$ |</p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#43-index-size-and-refresh","title":"4.3 Index Size and Refresh","text":"<p>Index Size Trade-offs: - Large Index:    - \u2705 More comprehensive knowledge   - \u274c Slower queries, more storage</p> <ul> <li>Small Index:</li> <li>\u2705 Faster queries, less storage</li> <li>\u274c May miss relevant information</li> </ul> <p>Refresh Strategies:</p> <ol> <li> <p>Batch Refresh (Daily/Weekly)    <pre><code># Full reindex\nvectorstore.delete_collection()\nvectorstore = Chroma.from_documents(new_docs, embeddings)\n</code></pre></p> </li> <li> <p>Incremental Updates <pre><code># Add new documents\nvectorstore.add_documents(new_docs)\n\n# Update existing\nvectorstore.delete(ids=[\"doc_123\"])\nvectorstore.add_documents([updated_doc])\n</code></pre></p> </li> <li> <p>Hot/Cold Separation</p> </li> <li>Frequently accessed: In-memory, fast index</li> <li>Archival: Disk-based, slower but comprehensive</li> </ol>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#5-rag-in-the-ai-accelerator","title":"5. RAG in the AI Accelerator","text":""},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#accelerator-architecture","title":"Accelerator Architecture","text":"<p>The AI Accelerator provides production-ready RAG components:</p> <pre><code>accelerator/\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 chunk.py          # Document chunking\n\u2502   \u251c\u2500\u2500 extract.py        # Text extraction\n\u2502   \u2514\u2500\u2500 embed_index.py    # Embedding &amp; indexing\n\u251c\u2500\u2500 rag/\n\u2502   \u251c\u2500\u2500 retriever.py      # Retrieval logic\n\u2502   \u251c\u2500\u2500 pipeline.py       # End-to-end RAG\n\u2502   \u2514\u2500\u2500 prompt.py         # Prompt templates\n\u2514\u2500\u2500 service/\n    \u2514\u2500\u2500 api.py            # FastAPI endpoints\n</code></pre>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#component-details","title":"Component Details","text":""},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#1-ingestion-indexing","title":"1. Ingestion &amp; Indexing","text":"<p><code>tools/chunk.py</code> - Document Chunking <pre><code>def chunk_documents(\n    documents: List[Document],\n    chunk_size: int = 1000,\n    chunk_overlap: int = 200\n) -&gt; List[Document]:\n    \"\"\"Split documents into chunks\"\"\"\n    pass\n</code></pre></p> <p><code>tools/extract.py</code> - Text Extraction <pre><code>def extract_text(\n    file_path: str,\n    file_type: str\n) -&gt; str:\n    \"\"\"Extract text from PDF, HTML, etc.\"\"\"\n    pass\n</code></pre></p> <p><code>tools/embed_index.py</code> - Embedding &amp; Indexing <pre><code>def embed_and_index(\n    chunks: List[Document],\n    vector_store_config: dict\n) -&gt; VectorStore:\n    \"\"\"Embed chunks and store in vector DB\"\"\"\n    pass\n</code></pre></p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#2-retrieval-generation","title":"2. Retrieval &amp; Generation","text":"<p><code>rag/retriever.py</code> - Retrieval Function <pre><code>def retrieve(\n    query: str,\n    vectorstore: VectorStore,\n    k: int = 5\n) -&gt; List[Document]:\n    \"\"\"Retrieve relevant documents\"\"\"\n    pass\n</code></pre></p> <p><code>rag/pipeline.py</code> - RAG Pipeline <pre><code>def answer_question(\n    query: str,\n    retriever: Retriever,\n    llm: BaseLLM\n) -&gt; dict:\n    \"\"\"\n    Returns:\n        {\n            \"answer\": str,\n            \"chunks\": List[dict],\n            \"metadata\": dict\n        }\n    \"\"\"\n    pass\n</code></pre></p> <p><code>rag/prompt.py</code> - Prompt Templates <pre><code>SYSTEM_PROMPT = \"\"\"\nYou are a helpful assistant that answers questions\nbased on the provided context.\n\"\"\"\n\nUSER_TEMPLATE = \"\"\"\nContext: {context}\n\nQuestion: {question}\n\nAnswer:\n\"\"\"\n</code></pre></p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#reference-notebooks","title":"Reference Notebooks","text":"<p>Located in <code>accelerator/assets/notebook/</code>:</p> <ol> <li><code>Process_and_Ingest_Data_into_Vector_DB.ipynb</code></li> <li>Complete ingestion pipeline</li> <li>Multiple data sources</li> <li> <p>Error handling</p> </li> <li> <p><code>QnA_with_RAG.ipynb</code></p> </li> <li>Query answering examples</li> <li>Different retrieval strategies</li> <li> <p>Response formatting</p> </li> <li> <p><code>Test_Queries_for_Vector_DB.ipynb</code></p> </li> <li>Testing retrieval quality</li> <li>Performance benchmarks</li> <li>Query debugging</li> </ol>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#6-how-day-2-labs-map-to-rag-components","title":"6. How Day 2 Labs Map to RAG Components","text":""},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#lab-progression","title":"Lab Progression","text":"<pre><code>Lab 2.1 (Local RAG)\n    \u2193\nLearn: Basic RAG flow, Ollama integration, Chroma usage\nMaps to: chunk.py, embed_index.py basics\n\nLab 2.2 (watsonx RAG)\n    \u2193\nLearn: Enterprise RAG, Elasticsearch, watsonx.ai\nMaps to: retriever.py, pipeline.py\n\nLab 2.3 (Twin Pipelines)\n    \u2193\nLearn: Multi-backend orchestration, comparison\nMaps to: Service layer design, api.py\n\nLab 2.4 (Evaluation)\n    \u2193\nLearn: Metrics, testing, optimization\nMaps to: tools/eval_small.py, monitoring\n</code></pre>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#progressive-implementation","title":"Progressive Implementation","text":"<p>Each lab builds toward a production RAG service:</p> <ol> <li>Lab 2.1: Core concepts in notebook</li> <li>Lab 2.2: Enterprise patterns</li> <li>Lab 2.3: Service design</li> <li>Lab 2.4: Quality assurance</li> </ol> <p>By end of Day 2: - \u2705 Understand all RAG components - \u2705 Implement multiple RAG backends - \u2705 Ready to integrate into accelerator - \u2705 Evaluate and optimize RAG systems</p>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#summary","title":"Summary","text":""},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>RAG = Retrieval + Generation</li> <li>Grounds LLM responses in external knowledge</li> <li> <p>Reduces hallucinations through factual context</p> </li> <li> <p>Core Pipeline: Ingest \u2192 Index \u2192 Retrieve \u2192 Generate</p> </li> <li> <p>Critical Components:</p> </li> <li>Chunking strategy</li> <li>Embedding model</li> <li>Vector store</li> <li>Retrieval mechanism</li> <li> <p>LLM integration</p> </li> <li> <p>Trade-offs Matter:</p> </li> <li>Latency vs accuracy</li> <li>Index size vs freshness</li> <li> <p>Cost vs performance</p> </li> <li> <p>Production-Ready:</p> </li> <li>Use accelerator patterns</li> <li>Implement evaluation</li> <li>Monitor and iterate</li> </ol>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#next-steps","title":"Next Steps","text":"<ol> <li>Review this theory document</li> <li>Explore reference notebooks</li> <li>Begin Lab 2.1: Local RAG implementation</li> <li>Map your learning to accelerator architecture</li> </ol>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#additional-resources","title":"Additional Resources","text":""},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#documentation","title":"Documentation","text":"<ul> <li>LangChain RAG Guide</li> <li>watsonx.ai RAG Patterns</li> <li>Vector Database Comparison</li> </ul>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#papers","title":"Papers","text":"<ul> <li>\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"</li> <li>\"Dense Passage Retrieval for Open-Domain Question Answering\"</li> </ul>"},{"location":"tracks/day2-rag/Theory_01_RAG_Architecture_Overview/#tools","title":"Tools","text":"<ul> <li>LangSmith - RAG debugging</li> <li>Weights &amp; Biases - Experiment tracking</li> </ul> <p>Theory Module Complete! \u2705</p> <p>Proceed to Lab 2.1 when ready.</p>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/","title":"3.0 Agentic AI &amp; Orchestration Overview","text":""},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of Day 3 (morning theory block), you should be able to:</p> <ul> <li>Explain what Agentic AI is and how it differs from \u201csingle-shot\u201d LLM calls.</li> <li>Describe common agent patterns: tool calling, ReAct, plan\u2013act, multi-agent and graph-based flows.</li> <li>Map these patterns onto concrete frameworks:</li> <li>CrewAI \u2013 multi-agent orchestration in Python.</li> <li>Langflow \u2013 visual builder for LangChain-style flows.</li> <li>LangGraph \u2013 graph-based orchestration for complex workflows.</li> <li>watsonx Orchestrate \u2013 IBM\u2019s production-grade agent platform.</li> <li>Understand how your RAG accelerator turns into a reusable tool for agents.</li> </ul>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#31-what-is-agentic-ai","title":"3.1 What Is Agentic AI?","text":"<p>So far we mostly used LLMs like functions:</p> <p>Prompt in \u2192 answer out</p> <p>Agentic AI adds reasoning + action:</p> <ul> <li>The LLM doesn\u2019t just answer \u2014 it:</li> <li>Decides which tools to use (APIs, RAG services, calculators\u2026).</li> <li>Plans a sequence of steps.</li> <li>Executes tools, reads results, and continues reasoning.</li> <li>The result is an agent:</li> <li>Has a goal (\u201chelp user with workshop questions\u201d).</li> <li>Has capabilities (tools, collaborators, knowledge bases).</li> <li>Uses an LLM for planning and reflection.</li> </ul> <p>Agentic AI is powerful when:</p> <ul> <li>You need multi-step workflows (e.g. search \u2192 filter \u2192 call RAG \u2192 summarize).</li> <li>You need to integrate with existing systems (ticketing, CRM, data lakes).</li> <li>You want traceability: which tools were used, which docs were read, etc.</li> </ul>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#32-core-agent-patterns","title":"3.2 Core Agent Patterns","text":"<p>We\u2019ll refer to these patterns throughout Day 3:</p>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#1-tool-calling-agent","title":"1. Tool-Calling Agent","text":"<ul> <li>LLM chooses one tool at a time based on user input.</li> <li>Example:</li> <li><code>rag_service_tool(question)</code> \u2192 calls your accelerator <code>/ask</code> endpoint.</li> <li><code>calculator_tool(expression)</code> \u2192 safe arithmetic evaluation.</li> <li>Agent picks a tool, calls it, and formats the result back to the user.</li> </ul>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#2-react-reason-act","title":"2. ReAct (Reason + Act)","text":"<ul> <li>LLM alternates between thinking and acting:</li> <li>Thought: \u201cI should call the RAG service to get context.\u201d</li> <li>Action: Use tool <code>rag_service_tool</code>.</li> <li>Observation: Tool output.</li> <li>Repeat\u2026</li> </ul> <p>Many frameworks (CrewAI, LangGraph, watsonx Orchestrate \u201creact style\u201d) build on this idea.</p>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#3-planact","title":"3. Plan\u2013Act","text":"<ul> <li>LLM plans a sequence of steps, then executes them:</li> <li>Plan: \u201c1) Retrieve relevant docs. 2) Summarize. 3) Run calculator. 4) Draft email.\u201d</li> <li>Tools are then called to fulfill the plan.</li> <li>Good for longer-running workflows and orchestrated flows.</li> </ul>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#4-multi-agent-crew","title":"4. Multi-Agent / Crew","text":"<ul> <li>Several agents with distinct roles:</li> <li>Researcher, Writer, Reviewer, Ops Agent, etc.</li> <li>A supervisor (or \u201cCrew\u201d) coordinates them:</li> <li>Delegate tasks.</li> <li>Merge answers.</li> <li>Route to the right specialist.</li> </ul>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#5-graph-based-agent","title":"5. Graph-Based Agent","text":"<ul> <li>Workflow formalized as a state machine or graph:</li> <li>Nodes = actions/tools/sub-agents.</li> <li>Edges = transitions based on state or LLM decisions.</li> <li>Examples:</li> <li>LangGraph StateGraph.</li> <li>watsonx Orchestrate flows and styles (planner, react).</li> </ul>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#33-framework-tour","title":"3.3 Framework Tour","text":"<p>In the morning we\u2019ll conceptually walk through four frameworks that implement these patterns.</p>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#331-crewai","title":"3.3.1 CrewAI","text":"<ul> <li>What it is: A Python library for multi-agent \u201ccrews\u201d.</li> <li>Mental model:</li> <li><code>Agent</code> \u2013 role, goal, backstory, tools.</li> <li><code>Task</code> \u2013 what needs doing, expected output.</li> <li><code>Crew</code> \u2013 group of agents + tasks + process (<code>Process.sequential</code>, <code>Process.hierarchical</code>, \u2026).</li> <li>Where it shines:</li> <li>Faster prototyping of multi-agent patterns.</li> <li>Narrative workflows (research \u2192 writing \u2192 editing).</li> <li>Workshop angle:</li> <li>A single CrewAI agent using your accelerator RAG API + calculator.</li> </ul>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#332-langflow","title":"3.3.2 Langflow","text":"<ul> <li>What it is: A visual builder for LangChain flows.</li> <li>Mental model:</li> <li>Drag-and-drop components (LLM, retriever, tools, routers, prompts).</li> <li>Connect them as a graph.</li> <li>Export to Python / JSON for production.</li> <li>Where it shines:</li> <li>Teaching, experimentation, non-Python teams.</li> <li>Quickly trying different chains without coding.</li> <li>Workshop angle:</li> <li>Visually design a RAG + tool-calling chain that mirrors your accelerator + agent notebook.</li> </ul>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#333-langgraph","title":"3.3.3 LangGraph","text":"<ul> <li>What it is: A Python framework for building LLM-powered state machines.</li> <li>Mental model:</li> <li>You define a <code>GraphState</code> type.</li> <li>You add nodes (functions) that read &amp; write fields on that state.</li> <li>You define edges (flow of control).</li> <li>Where it shines:</li> <li>Complex, long-running agent workflows.</li> <li>Fine-grained control, retries, routing, timeouts.</li> <li>Tight integration with LangChain and tooling like watsonx.governance Evaluator.</li> <li>Workshop angle:</li> <li>Use LangGraph to orchestrate: RAG retrieval node \u2192 answer generation node \u2192 evaluation.</li> </ul>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#334-watsonx-orchestrate","title":"3.3.4 watsonx Orchestrate","text":"<ul> <li>What it is: An enterprise platform to define, govern, and deploy agents.</li> <li>Key concepts:</li> <li>Agents, tools, connections, knowledge bases, flows.</li> <li>Local Developer Edition and managed IBM Cloud deployments.</li> <li>Where it shines:</li> <li>Production deployment.</li> <li>Governance, monitoring, and integration with corporate IT systems.</li> <li>Workshop angle:</li> <li>Treat your accelerator <code>/ask</code> endpoint as a tool in Orchestrate.</li> <li>Recreate the patterns from your Day 3 notebook as an Orchestrate agent.</li> </ul>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#34-morning-flow-approx-4h","title":"3.4 Morning Flow (Approx. 4h)","text":"<p>Suggested agenda (adapt to your group):</p> <ol> <li>Intro &amp; recap (30\u201345 min) </li> <li>Why agents on top of RAG?  </li> <li> <p>Quick recap of RAG architecture from Day 2.</p> </li> <li> <p>Agent patterns walkthrough (45 min) </p> </li> <li> <p>Tool-calling, ReAct, plan\u2013act, multi-agent, graph-based.</p> </li> <li> <p>Framework lightning demos (90\u2013120 min) </p> </li> <li>CrewAI mini-demo:<ul> <li>One agent with a \u201csupport engineer\u201d role.</li> <li>Tools: calculator + stubbed RAG tool.</li> </ul> </li> <li>Langflow mini-demo:<ul> <li>Visual RAG chain that calls an LLM and shows citations.</li> </ul> </li> <li>LangGraph mini-demo:<ul> <li>Simple graph: retrieve \u2192 generate.</li> </ul> </li> <li> <p>Orchestrate concept demo:</p> <ul> <li>Show YAML / ADK structure for a \u201cHello World Agent\u201d.</li> </ul> </li> <li> <p>Bridge to afternoon labs (15\u201330 min) </p> </li> <li>Show how the accelerator <code>/ask</code> endpoint becomes the key tool.</li> <li>Explain Lab 3.1: notebook-based agent that calls accelerator RAG.</li> <li>Preview Orchestrate labs: turning the same idea into a platform agent.</li> </ol>"},{"location":"tracks/day3-orchestrate/agentic-ai-overview/#35-how-this-connects-to-labs","title":"3.5 How This Connects to Labs","text":"<ul> <li>Day 2 \u2192 Day 3 bridge:</li> <li>Day 2 gave you a production-like RAG service (<code>/ask</code> endpoint).</li> <li> <p>Day 3 gives you agents that call that service as a tool.</p> </li> <li> <p>Lab 3.1 \u2013 Local Agent in simple-watsonx-environment:</p> </li> <li>Agent has two tools: RAG service, calculator.</li> <li> <p>Planner LLM chooses which tool to call, then composes final answer.</p> </li> <li> <p>Agent frameworks (CrewAI / LangGraph / Orchestrate):</p> </li> <li>You can re-express the same logic in different frameworks:<ul> <li>CrewAI for Python multi-agent setups.</li> <li>LangGraph for stateful workflows and evaluation.</li> <li>Orchestrate for production deployment and governance.</li> </ul> </li> </ul> <p>By the end of Day 3, your mental model should be:</p> <p>Docs \u2192 RAG (Day 2) \u2192 Agent on top (Day 3) \u2192 Orchestrated &amp; governed in watsonx (beyond the workshop).</p>"},{"location":"tracks/day3-orchestrate/bridge-orchestrate-governance/","title":"3.2 Bridge to watsonx Orchestrate &amp; Governance","text":""},{"location":"tracks/day3-orchestrate/bridge-orchestrate-governance/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand how notebook patterns translate to watsonx Orchestrate &amp; governance tools.</li> <li>Recognize where policies and monitoring fit.</li> </ul>"},{"location":"tracks/day3-orchestrate/bridge-orchestrate-governance/#mapping-accelerator-agent-to-orchestrate","title":"Mapping Accelerator + Agent to Orchestrate","text":"<p>Think of watsonx Orchestrate as a platform that can host the patterns you built in notebooks.</p> <ul> <li>Conceptual mapping:</li> <li><code>accelerator/service/api.py</code> \u2192 a reusable \u201cRAG answer\u201d action (tool / service).</li> <li>Agent notebook \u2192 blueprint for a multi-step orchestrated workflow.</li> </ul>"},{"location":"tracks/day3-orchestrate/bridge-orchestrate-governance/#example-orchestrate-flow","title":"Example Orchestrate Flow","text":"<ol> <li>Receive a user request (chat, API, UI).</li> <li>Decide whether to:</li> <li>Call the RAG service (accelerator <code>/ask</code> endpoint).</li> <li>Call a calculator or other utility tool.</li> <li>Escalate to a human (handoff / ticket).</li> <li>Log all steps and outcomes for governance.</li> </ol> <p>In Orchestrate terms:</p> <ul> <li>Agent = top-level orchestrator that plans and delegates.</li> <li>Tools = your APIs (RAG, calculator, external systems).</li> <li>Connections = how tools authenticate to external services.</li> <li>Knowledge bases = RAG-ready corpora defined in Orchestrate.</li> </ul>"},{"location":"tracks/day3-orchestrate/bridge-orchestrate-governance/#governance-evaluation","title":"Governance &amp; Evaluation","text":"<p>watsonx.governance adds a layer of control and insight over your agents and models:</p> <ul> <li>Model &amp; asset catalog</li> <li>Track which models, prompts, tools, and agents exist.</li> <li> <p>Versioning and metadata.</p> </li> <li> <p>Policies</p> </li> <li>Allowed models / endpoints.</li> <li>Risk profiles and approval workflows.</li> <li> <p>For example:</p> <ul> <li>\u201cCustomer-facing agents may only use models with a given risk rating.\u201d</li> </ul> </li> <li> <p>Evaluation Studio</p> </li> <li>Run experiments on your agents / RAG flows.</li> <li>Compare LLMs, prompts, retrieval strategies.</li> <li> <p>Track metrics like faithfulness, answer relevance, content safety.</p> </li> <li> <p>Monitoring</p> </li> <li>Track runs and metrics over time.</li> <li>Detect drift or degradation.</li> <li>Investigate failures and edge-cases.</li> </ul>"},{"location":"tracks/day3-orchestrate/bridge-orchestrate-governance/#using-existing-notebooks-as-a-bridge","title":"Using Existing Notebooks as a Bridge","text":"<p>You already have governance-related notebooks in <code>labs-src</code> and <code>accelerator</code>:</p> <ul> <li><code>labs-src/ibm-watsonx-governance-governed-agentic-catalog.ipynb</code>:</li> <li> <p>Shows how to:</p> <ul> <li>Register models &amp; tools in a governed catalog.</li> <li>Create and use governed tools (e.g. PII detectors, jailbreak detectors).</li> </ul> </li> <li> <p><code>labs-src/ibm-watsonx-governance-evaluation-studio-getting-started.ipynb</code>:</p> </li> <li> <p>Shows how to:</p> <ul> <li>Define evaluation datasets.</li> <li>Compute metrics like context relevance, faithfulness, answer relevance.</li> </ul> </li> <li> <p><code>accelerator/assets/notebook/notebook:Analyze_Log_and_Feedback.ipynb</code>:</p> </li> <li>Concrete example of:<ul> <li>Pulling logs from a deployed service.</li> <li>Analyzing quality and user feedback.</li> <li>Bridging logs \u2192 evaluation datasets.</li> </ul> </li> </ul> <p>Your Day 3 agent logs can be fed into these notebooks as a first step towards full governance.</p>"},{"location":"tracks/day3-orchestrate/bridge-orchestrate-governance/#from-logs-to-governance","title":"From Logs to Governance","text":"<p>Where do logs come from?</p> <ul> <li><code>rag/pipeline.py</code>:</li> <li> <p>Can log retrievals and model calls (question, chunks, model, latency).</p> </li> <li> <p><code>service/api.py</code>:</p> </li> <li> <p>Can log user requests, status codes, errors.</p> </li> <li> <p>Agent notebook (Lab 3.1):</p> </li> <li>Can log tool choices, planner outputs, tool responses, final answers.</li> </ul> <p>These logs can be:</p> <ol> <li>Exported periodically as CSV/JSON.</li> <li>Loaded into:</li> <li>Evaluation Studio datasets.</li> <li>Analysis notebooks (e.g. <code>Analyze_Log_and_Feedback.ipynb</code>).</li> </ol> <p>Governance workflows can then:</p> <ul> <li>Detect drift in answer quality.</li> <li>Enforce thresholds:</li> <li>\u201cIf faithfulness &lt; 0.8, flag for review.\u201d</li> <li>Support audit trails:</li> <li>\u201cWhich tools and models were used to answer this question?\u201d</li> </ul>"},{"location":"tracks/day3-orchestrate/bridge-orchestrate-governance/#example-use-cases","title":"Example Use Cases","text":"<p>Some concrete patterns where orchestrate + governance shine:</p> <ul> <li>Governed HR assistant</li> <li>Uses RAG over HR policies.</li> <li>Tools for detecting PII and harmful content.</li> <li> <p>Governed model + tool catalog and metrics.</p> </li> <li> <p>Customer support bot with monitored outputs</p> </li> <li>Agent routes between FAQ RAG, ticket creation, and human handoff.</li> <li> <p>Governance monitors answer relevance and safety.</p> </li> <li> <p>Internal knowledge bot with evaluation cycles</p> </li> <li>Weekly evaluation runs on a curated question set.</li> <li>Results feed into prompt/model/index improvements.</li> </ul>"},{"location":"tracks/day3-orchestrate/bridge-orchestrate-governance/#how-to-go-from-lab-to-production","title":"How to Go from Lab to Production","text":"<p>From Day 3 notebooks to real systems:</p> <ul> <li>Extract the best patterns from:</li> <li><code>agent_watsonx.ipynb</code> (agent loop + tools).</li> <li><code>accelerator</code> service (RAG microservice).</li> <li> <p>Governance notebooks (evaluation + catalog).</p> </li> <li> <p>Embed them into your delivery pipeline:</p> </li> <li>Use <code>Makefile</code> / <code>pyproject.toml</code> / Dockerfiles to package the accelerator.</li> <li>Deploy accelerator API behind a stable endpoint.</li> <li> <p>Register it as a tool / connection in Orchestrate.</p> </li> <li> <p>Connect Orchestrate deployments to governance:</p> </li> <li>Ensure models and agents are registered in the catalog.</li> <li>Route logs to Evaluation Studio or custom analysis.</li> </ul>"},{"location":"tracks/day3-orchestrate/bridge-orchestrate-governance/#discussion-prompts","title":"Discussion Prompts","text":"<p>To close the session, consider:</p> <ul> <li>Where would you use orchestration and governance in your organization?</li> <li>Which components from this workshop are closest to your production needs?</li> <li>What would you need to:</li> <li>Onboard your own data?</li> <li>Apply your internal policies?</li> <li>Connect to your existing IT systems?</li> </ul> <p>Capture these ideas \u2014 they are great seeds for the capstone day and follow-up projects.</p>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/","title":"Lab 3.1 \u2013 Local Agent in <code>simple-watsonx-enviroment</code> + Accelerator API","text":""},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#lab-overview","title":"Lab Overview","text":"<ul> <li>Build a simple agent that decides which tool to call, using Granite as the planner.</li> <li>Wire it into the accelerator RAG API for production-like behaviour.</li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Implement tools as Python functions and HTTP calls.</li> <li>Use an LLM to select and call tools.</li> <li>Log steps in a structured way.</li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#prerequisites","title":"Prerequisites","text":"<ul> <li>Day 2 RAG pipeline implemented and mapped into:</li> <li><code>accelerator/rag/retriever.py</code></li> <li><code>accelerator/rag/pipeline.py</code></li> <li><code>accelerator/rag/prompt.py</code></li> <li>Basic understanding of orchestration patterns.</li> <li>Accelerator FastAPI app running (even with minimal logic).</li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#step-1-finalize-accelerator-back-end","title":"Step 1 \u2013 Finalize Accelerator Back-End","text":""},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#11-implement-ragretrieverpy","title":"1.1 Implement <code>rag/retriever.py</code>","text":"<ul> <li>Replace the placeholder <code>retrieve(q)</code> with real logic:</li> <li>Choose backend (e.g. Elasticsearch or Chroma).</li> <li>Use:<ul> <li>Connection details from <code>config.yaml</code> / environment.</li> <li>Helper functions from <code>assets/data_asset/rag_helper_functions.py</code> if using Elasticsearch.</li> </ul> </li> <li>Return a list of chunks:<ul> <li>Each chunk: <code>{ \"id\": ..., \"text\": ..., \"score\": ..., \"metadata\": {...} }</code>.</li> </ul> </li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#12-implement-ragpipelinepy","title":"1.2 Implement <code>rag/pipeline.py</code>","text":"<ul> <li>Replace the placeholder <code>answer_question(q)</code> with:</li> <li>Call to <code>retrieve(q)</code> to get top-k chunks.</li> <li>Prompt construction using <code>rag/prompt.py</code>:<ul> <li><code>SYSTEM</code></li> <li><code>USER_TEMPLATE.format(question=q, context=concat_chunks)</code>.</li> </ul> </li> <li>Call to watsonx.ai LLM (Granite) configured via <code>service/deps.py</code> settings.</li> <li>Return a dict: <code>{\"answer\": ..., \"chunks\": [...], \"model_id\": ..., \"latency_ms\": ...}</code>.</li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#13-improve-servicedepspy","title":"1.3 Improve <code>service/deps.py</code>","text":"<ul> <li>Implement a proper settings class, e.g. using <code>pydantic.BaseSettings</code>:</li> <li>Fields:<ul> <li>watsonx endpoint, api key, project id.</li> <li>Vector DB connection info.</li> <li>Index name, top_k, etc.</li> </ul> </li> <li>Initialize <code>settings</code> as a reusable instance.</li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#14-harden-serviceapipy","title":"1.4 Harden <code>service/api.py</code>","text":"<ul> <li>Ensure the FastAPI app:</li> <li>Validates request with <code>AskReq</code> (question: str).</li> <li>Calls <code>answer_question</code>.</li> <li>Returns:<ul> <li><code>answer</code></li> <li><code>citations</code> (built from <code>chunks</code>).</li> <li>Optional metadata (model, latency).</li> </ul> </li> <li>Add basic error handling and logging.</li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#step-2-bring-up-the-accelerator-service","title":"Step 2 \u2013 Bring Up the Accelerator Service","text":"<ul> <li>Start the FastAPI app (e.g. via <code>uvicorn</code> or <code>Makefile</code> target).</li> <li>Test <code>POST /ask</code> with:</li> <li>curl</li> <li>HTTP client</li> <li>or a small Python script.</li> </ul> <p>Example test:</p> <pre><code>curl -X POST http://localhost:8000/ask \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"question\": \"What is RAG?\"}'\n</code></pre>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#step-3-build-an-agent-notebook-in-simple-watsonx-enviroment","title":"Step 3 \u2013 Build an Agent Notebook in <code>simple-watsonx-enviroment</code>","text":"<ul> <li>Create <code>agent_watsonx.ipynb</code>.</li> <li> <p>Define tools:</p> </li> <li> <p><code>rag_service_tool(question: str)</code>:</p> <ul> <li>Calls <code>POST /ask</code> on the accelerator API.</li> <li>Returns answer + citations.</li> </ul> </li> <li> <p><code>calculator_tool(expression: str)</code>:</p> <ul> <li>Evaluates arithmetic safely (e.g. small AST-based evaluator).</li> </ul> </li> <li> <p>Wrap each tool so it returns a clean text string and a small metadata dict.</p> </li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#step-4-design-the-planner-prompt","title":"Step 4 \u2013 Design the Planner Prompt","text":"<ul> <li>System prompt:</li> <li>Explain available tools and when to use them.</li> <li>Tool-selection format:</li> <li> <p>e.g. JSON:     <pre><code>{ \"tool\": \"rag_service\", \"arguments\": { \"question\": \"...\" } }\n</code></pre></p> </li> <li> <p>You can embed this in a prompt like:</p> </li> </ul> <pre><code>You are a planner agent. You must choose exactly ONE tool.\n\nTools:\n- \"rag_service\": answer enterprise questions using the /ask RAG API.\n- \"calculator\": evaluate arithmetic expressions like \"2 * (3 + 4)\".\n\nReturn a JSON object with:\n- \"tool\": one of [\"rag_service\", \"calculator\"]\n- \"arguments\": an object with the tool parameters.\n</code></pre>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#step-5-implement-agent-loop","title":"Step 5 \u2013 Implement Agent Loop","text":"<p>High-level steps:</p> <ol> <li>Planner step</li> <li> <p>User input \u2192 Granite model \u2192 JSON with chosen tool + arguments.</p> </li> <li> <p>Tool execution step</p> </li> <li>Python decodes JSON.</li> <li> <p>Calls the corresponding tool (<code>rag_service_tool</code> or <code>calculator_tool</code>).</p> </li> <li> <p>Final answer step</p> </li> <li> <p>Call Granite again with a prompt like:</p> <ul> <li>\u201cUser question: ...; Tool used: ...; Tool output: ...; Please write a helpful final answer.\u201d</li> </ul> </li> <li> <p>Return structured result</p> </li> <li>Dictionary like:</li> </ol> <pre><code>{\n  \"question\": user_input,\n  \"tool\": chosen_tool,\n  \"tool_args\": tool_args,\n  \"tool_output\": tool_output,\n  \"final_answer\": final_answer,\n}\n</code></pre>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#step-6-log-inspect","title":"Step 6 \u2013 Log &amp; Inspect","text":"<p>For each interaction, log:</p> <ul> <li>User question.</li> <li>Tool chosen.</li> <li>Tool arguments.</li> <li>Tool output summary.</li> <li>Final answer.</li> <li>Optional: timestamps, model id, latency.</li> </ul> <p>Log format can be:</p> <ul> <li>List of dicts in memory (for quick exploration).</li> <li>JSONL file (one line per interaction).</li> <li>Pandas DataFrame for quick analysis in a notebook.</li> </ul> <p>These logs can later be analyzed in:</p> <ul> <li><code>accelerator/assets/notebook/notebook:Analyze_Log_and_Feedback.ipynb</code></li> <li>watsonx.governance Evaluation Studio (after exporting as a dataset).</li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#reference-notebooks","title":"Reference Notebooks","text":"<ul> <li><code>labs-src/ibm-watsonx-governance-governed-agentic-catalog.ipynb</code></li> <li><code>accelerator/assets/notebook/notebook:Create_and_Deploy_QnA_AI_Service.ipynb</code></li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#wrap-up","title":"Wrap-Up","text":"<ul> <li>Strengths of the notebook-based agent:</li> <li>Fast iteration.</li> <li>Easy debugging.</li> <li> <p>Great for experimentation.</p> </li> <li> <p>Strengths of the API-based RAG service:</p> </li> <li>Clear contract (<code>/ask</code> endpoint).</li> <li> <p>Can be reused by:</p> <ul> <li>Agents (this lab).</li> <li>UIs (Streamlit app).</li> <li>Other services.</li> </ul> </li> <li> <p>This pattern generalizes to watsonx Orchestrate:</p> </li> <li>Your accelerator API becomes a managed tool / connection.</li> <li>Your planner logic becomes an Orchestrate agent with tools and flows.</li> </ul>"},{"location":"tracks/day3-orchestrate/lab-1-agent-watsonx/#checkpoint","title":"Checkpoint","text":"<ul> <li>\u2705 Accelerator API <code>/ask</code> implemented and tested.</li> <li>\u2705 Agent notebook built and calling the accelerator service as a tool.</li> <li>\u2705 Logging prepared for governance / evaluation.</li> </ul>"},{"location":"tracks/day3-orchestrate/recap-and-next-steps/","title":"3.3 Recap &amp; Next Steps (2h Discussion)","text":""},{"location":"tracks/day3-orchestrate/recap-and-next-steps/#session-overview","title":"Session Overview","text":"<p>This is a 2-hour interactive recap and discussion to consolidate learning and plan next steps.</p>"},{"location":"tracks/day3-orchestrate/recap-and-next-steps/#part-1-timeline-recap","title":"Part 1 \u2013 Timeline Recap","text":"<p>Walk through the workshop story:</p> <ul> <li>Day 0: Environment setup</li> <li>Two env repos (Ollama + watsonx).</li> <li><code>accelerator</code> repo.</li> <li> <p><code>labs-src</code> reference notebooks.</p> </li> <li> <p>Day 1: LLM basics &amp; prompting</p> </li> <li>Prompt patterns and reliability.</li> <li> <p>Compare local vs hosted models (Ollama vs watsonx).</p> </li> <li> <p>Day 2: RAG</p> </li> <li>Local RAG notebooks (Ollama + Chroma).</li> <li>RAG notebooks in <code>labs-src</code> (Elasticsearch, Chroma, Granite).</li> <li> <p>Accelerator ingestion + retriever + pipeline skeleton.</p> </li> <li> <p>Day 3: Orchestration &amp; agents</p> </li> <li>Agent notebook calling the accelerator API.</li> <li>Accelerator API &amp; (optional) Streamlit UI.</li> <li>Governance &amp; Orchestrate notebooks.</li> </ul>"},{"location":"tracks/day3-orchestrate/recap-and-next-steps/#part-2-group-reflection","title":"Part 2 \u2013 Group Reflection","text":"<p>Suggested questions for the group:</p> <ul> <li>What was most surprising across the three days?</li> <li>Biggest \u201caha\u201d moments?</li> <li>Hardest parts of the labs?</li> <li>Where did you hit friction with:</li> <li>Environments?</li> <li>Libraries / SDKs?</li> <li>Conceptual pieces (RAG, agents, governance)?</li> </ul> <p>Capture answers on a shared board or document.</p>"},{"location":"tracks/day3-orchestrate/recap-and-next-steps/#part-3-open-qa-troubleshooting","title":"Part 3 \u2013 Open Q&amp;A &amp; Troubleshooting","text":"<p>Use this time to address remaining technical issues:</p> <ul> <li>Ollama / watsonx environments:</li> <li>Model loading, API keys, network.</li> <li>Accelerator service:</li> <li><code>/ask</code> endpoint behaviour.</li> <li>Vector DB connectivity.</li> <li>Logging &amp; configuration.</li> <li>Agent notebooks:</li> <li>Tool selection.</li> <li>JSON formatting issues.</li> <li>Error handling.</li> </ul> <p>Encourage participants to share their code and walk through solutions live.</p>"},{"location":"tracks/day3-orchestrate/recap-and-next-steps/#part-4-from-workshop-to-real-projects","title":"Part 4 \u2013 From Workshop to Real Projects","text":"<p>Discuss how to turn the accelerator into something \u201creal\u201d:</p> <ul> <li>Production microservice</li> <li>Hardened FastAPI app.</li> <li>Config via <code>service/deps.py</code> and environment variables.</li> <li> <p>Health checks, observability, logging.</p> </li> <li> <p>Scalable ingestion pipeline</p> </li> <li>Batch extraction / chunking / embedding.</li> <li>Scheduling (nightly or event-driven).</li> <li> <p>Monitoring index size and freshness.</p> </li> <li> <p>Integration</p> </li> <li>CI/CD pipelines using <code>Makefile</code>, <code>pyproject.toml</code>, Dockerfiles.</li> <li>Deployment targets: Kubernetes / OpenShift / Cloud Foundry / VM.</li> <li>Governance and evaluation workflows with watsonx.governance.</li> </ul>"},{"location":"tracks/day3-orchestrate/recap-and-next-steps/#part-5-suggested-next-steps","title":"Part 5 \u2013 Suggested Next Steps","text":"<p>Potential directions for deeper dives:</p> <ul> <li>Retrieval &amp; ranking</li> <li>Hybrid search (lexical + semantic).</li> <li> <p>Rerankers and scoring strategies in <code>retriever.py</code>.</p> </li> <li> <p>Prompting &amp; safety</p> </li> <li>Advanced prompt strategies in <code>prompt.py</code>.</li> <li> <p>Safety / refusal patterns and guardrails.</p> </li> <li> <p>Multi-tenancy</p> </li> <li>Tenant-aware configuration in <code>service/deps.py</code>.</li> <li> <p>Per-tenant indices and model settings.</p> </li> <li> <p>UI &amp; UX</p> </li> <li>Richer Streamlit UI (<code>ui/app.py</code>).</li> <li>Conversation history, feedback buttons, source highlighting.</li> </ul> <p>Provide pointers to:</p> <ul> <li>Internal docs and repos.</li> <li>IBM public samples (<code>labs-src</code>, GitHub repos).</li> <li>Governance and Orchestrate documentation.</li> </ul>"},{"location":"tracks/day3-orchestrate/recap-and-next-steps/#optional-feedback-form","title":"Optional: Feedback Form","text":"<p>Share a link or QR code to a short survey covering:</p> <ul> <li>Overall satisfaction.</li> <li>Clarity of explanations.</li> <li>Lab difficulty.</li> <li>Which topics participants want more of.</li> </ul> <p>Use this to plan the capstone day and any follow-up sessions.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/","title":"3.x watsonx Orchestrate \u2013 Labs &amp; Reference","text":"<p>Workshop Focus: Building AI Agents with watsonx Orchestrate Target Audience: Technical professionals, developers, and architects  </p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Environment Setup </li> <li>Introduction to AI Agents </li> <li>Building Your First Agent </li> <li>Creating Python Tools </li> <li>Connections &amp; Integration </li> <li>Knowledge Bases </li> <li>Flow Builder </li> <li>Multi-Agent Systems </li> <li>Agent Protocols (MCP &amp; A2A) </li> <li>Production Deployment </li> </ol>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#1-environment-setup","title":"1. Environment Setup","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher  </li> <li>Docker Desktop installed and running  </li> <li>IBM Cloud account (for production deployment)</li> </ul>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#install-the-adk","title":"Install the ADK","text":"<pre><code>pip install ibm-watsonx-orchestrate\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#configure-environment-file","title":"Configure Environment File","text":"<p>Create a <code>.env</code> file with your credentials:</p> <pre><code># .env file\nWATSONX_API_KEY=your_watsonx_api_key\nWATSONX_PROJECT_ID=your_project_id\nWATSONX_URL=https://us-south.ml.cloud.ibm.com\n\n# Optional: Skip login for Developer Edition\nWO_DEVELOPER_EDITION_SKIP_LOGIN=true\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#start-watsonx-orchestrate-developer-edition","title":"Start watsonx Orchestrate Developer Edition","text":"<pre><code>orchestrate server start --env-file .env --accept-terms-and-conditions\n</code></pre> <p>Available Services:</p> <ul> <li>API Base URL: <code>http://localhost:4321/api/v1</code> </li> <li>OpenAPI Docs: <code>http://localhost:4321/docs</code> </li> <li>UI: <code>http://localhost:3000</code></li> </ul>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#configure-local-environment","title":"Configure Local Environment","text":"<pre><code># Activate local environment\norchestrate env activate local\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#add-remote-environment-optional","title":"Add Remote Environment (Optional)","text":"<pre><code># Add IBM Cloud environment\norchestrate env add -n production -u https://your-instance-url.com\n\n# Activate remote environment\norchestrate env activate production --api-key your-api-key\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#2-introduction-to-ai-agents","title":"2. Introduction to AI Agents","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#what-is-an-ai-agent","title":"What Is an AI Agent?","text":"<p>An AI agent is an autonomous application that can understand, plan, and execute tasks using LLMs to reason and interface with tools, models, and IT systems.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#core-agent-components","title":"Core Agent Components","text":"<ol> <li>LLM \u2013 the language model powering the agent.  </li> <li>Style \u2013 prompting structure (Default, ReAct, Plan\u2013Act).  </li> <li>Instructions \u2013 natural language guidance.  </li> <li>Tools \u2013 functions the agent can execute.  </li> <li>Collaborators \u2013 other agents it can work with.  </li> <li>Knowledge Base \u2013 domain-specific information.</li> </ol>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#3-building-your-first-agent","title":"3. Building Your First Agent","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#lab-1-hello-world-agent","title":"Lab 1: Hello World Agent","text":"<p>Objective: Create a simple agent that responds to greetings.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-1-create-agent-yaml-file","title":"Step 1: Create Agent YAML File","text":"<p>Create <code>hello-world-agent.yaml</code>:</p> <pre><code>spec_version: v1\nkind: native\nname: Hello_World_Agent\ndescription: A simple Hello World agent for testing\ninstructions: |\n  You are a test agent created for a tutorial on how to get started with watsonx Orchestrate ADK.\n  When the user asks 'who are you', respond with: \n  \"I'm the Hello World Agent. Congratulations on completing the Getting Started with watsonx Orchestrate ADK tutorial!\"\nllm: watsonx/meta-llama/llama-3-2-90b-vision-instruct\nstyle: default\ncollaborators: []\ntools: []\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-2-import-the-agent","title":"Step 2: Import the Agent","text":"<pre><code>orchestrate agents import -f hello-world-agent.yaml\n</code></pre> <p>Expected output:</p> <pre><code>\u2713 Agent 'Hello_World_Agent' imported successfully\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-3-test-the-agent","title":"Step 3: Test the Agent","text":"<pre><code>orchestrate chat start\n</code></pre> <p>In the chat interface, type:</p> <pre><code>who are you?\n</code></pre> <p>You should see your \u201cHello World Agent\u201d answer.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#lab-2-weather-agent-with-tools","title":"Lab 2: Weather Agent with Tools","text":"<p>Objective: Create an agent that provides weather information using an external API.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-1-create-weather-tool-python","title":"Step 1: Create Weather Tool (Python)","text":"<p>Create <code>weather_tool.py</code>:</p> <pre><code>from ibm_watsonx_orchestrate.agent_builder.tools import tool\nimport requests\n\n@tool(\n    name=\"get_weather\",\n    description=\"Get current weather for a given city\"\n)\ndef get_weather(city: str) -&gt; str:\n    \"\"\"\n    Retrieves current weather information for a specified city.\n\n    Args:\n        city (str): The name of the city\n\n    Returns:\n        str: Weather information as a formatted string\n    \"\"\"\n    # Using a free weather API (replace with your API key)\n    api_key = \"your_openweather_api_key\"\n    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n\n    params = {\n        \"q\": city,\n        \"appid\": api_key,\n        \"units\": \"metric\"\n    }\n\n    try:\n        response = requests.get(base_url, params=params)\n        response.raise_for_status()\n        data = response.json()\n\n        temp = data['main']['temp']\n        description = data['weather'][0]['description']\n\n        return f\"The current weather in {city} is {description} with a temperature of {temp}\u00b0C.\"\n    except Exception as e:\n        return f\"Sorry, I couldn't fetch the weather for {city}. Error: {str(e)}\"\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-2-import-the-tool","title":"Step 2: Import the Tool","text":"<pre><code>orchestrate tools import -k python -f weather_tool.py\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-3-create-weather-agent","title":"Step 3: Create Weather Agent","text":"<p>Create <code>weather-agent.yaml</code>:</p> <pre><code>spec_version: v1\nkind: native\nname: Weather_Agent\ndescription: An agent that provides current weather information for cities\ninstructions: |\n  You are a helpful weather assistant. When users ask about weather in a city:\n  1. Use the get_weather tool to fetch current conditions\n  2. Present the information in a friendly, conversational manner\n  3. If the city is not found, politely ask for clarification\nllm: watsonx/meta-llama/llama-3-2-90b-vision-instruct\nstyle: react\ncollaborators: []\ntools:\n  - get_weather\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-4-import-and-test","title":"Step 4: Import and Test","text":"<pre><code>orchestrate agents import -f weather-agent.yaml\norchestrate chat start\n</code></pre> <p>Test with:</p> <pre><code>What's the weather in Paris?\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#4-creating-python-tools","title":"4. Creating Python Tools","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#lab-3-advanced-python-tool-with-connections","title":"Lab 3: Advanced Python Tool with Connections","text":"<p>Objective: Create a tool that uses secure connections to external APIs.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-1-create-connection-configuration","title":"Step 1: Create Connection Configuration","text":"<p>Create <code>github-connection.yaml</code>:</p> <pre><code>spec_version: v1\nkind: connection\napp_id: github_api\nenvironments:\n  draft:\n    kind: bearer\n    type: bearer_token\n  live:\n    kind: bearer\n    type: bearer_token\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-2-import-connection","title":"Step 2: Import Connection","text":"<pre><code>orchestrate connections import -f github-connection.yaml\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-3-set-credentials","title":"Step 3: Set Credentials","text":"<pre><code>orchestrate connections set-credentials   --app-id github_api   --env draft   --token your_github_personal_access_token\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-4-create-github-tool","title":"Step 4: Create GitHub Tool","text":"<p>Create <code>github_tool.py</code>:</p> <pre><code>from ibm_watsonx_orchestrate.agent_builder.tools import tool\nfrom ibm_watsonx_orchestrate.agent_builder.connections import ConnectionType\nfrom ibm_watsonx_orchestrate.run import connections\nimport requests\n\n@tool(\n    name=\"search_github_repos\",\n    description=\"Search for GitHub repositories by keyword\",\n    expected_credentials=[\n        {\"app_id\": \"github_api\", \"type\": ConnectionType.BEARER_TOKEN}\n    ]\n)\ndef search_github_repos(query: str, max_results: int = 5) -&gt; str:\n    \"\"\"\n    Search GitHub repositories.\n\n    Args:\n        query (str): Search query\n        max_results (int): Maximum number of results to return\n\n    Returns:\n        str: Formatted list of repositories\n    \"\"\"\n    creds = connections.bearer_token(\"github_api\")\n\n    headers = {\n        \"Authorization\": f\"Bearer {creds.token}\",\n        \"Accept\": \"application/vnd.github.v3+json\"\n    }\n\n    url = \"https://api.github.com/search/repositories\"\n    params = {\n        \"q\": query,\n        \"sort\": \"stars\",\n        \"order\": \"desc\",\n        \"per_page\": max_results\n    }\n\n    try:\n        response = requests.get(url, headers=headers, params=params)\n        response.raise_for_status()\n        data = response.json()\n\n        results = []\n        for repo in data['items'][:max_results]:\n            results.append(\n                f\"- {repo['full_name']}: {repo['description']} \"\n                f\"(\u2b50 {repo['stargazers_count']})\"\n            )\n\n        return \"\\n\".join(results)\n    except Exception as e:\n        return f\"Error searching repositories: {str(e)}\"\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-5-import-tool-with-connection","title":"Step 5: Import Tool with Connection","text":"<pre><code>orchestrate tools import -k python -f github_tool.py -a github_api\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#5-connections-integration","title":"5. Connections &amp; Integration","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#lab-4-multiple-connection-types","title":"Lab 4: Multiple Connection Types","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#api-key-connection","title":"API Key Connection","text":"<p>Create <code>api-key-connection.yaml</code>:</p> <pre><code>spec_version: v1\nkind: connection\napp_id: my_api_service\nenvironments:\n  draft:\n    kind: api_key\n    type: api_key_auth\n</code></pre> <p>Set credentials:</p> <pre><code>orchestrate connections set-credentials   --app-id my_api_service   --env draft   --api-key your_api_key   --url https://api.example.com\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#oauth-connection","title":"OAuth Connection","text":"<p>Create <code>oauth-connection.yaml</code>:</p> <pre><code>spec_version: v1\nkind: connection\napp_id: salesforce\nenvironments:\n  draft:\n    kind: oauth_auth_code_flow\n    type: oauth_auth_code_flow\n    oauth_config:\n      client_id: your_client_id\n      client_secret: your_client_secret\n      authorization_url: https://login.salesforce.com/services/oauth2/authorize\n      token_url: https://login.salesforce.com/services/oauth2/token\n      scope: api refresh_token\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#key-value-connection","title":"Key-Value Connection","text":"<p>Create <code>kv-connection.yaml</code>:</p> <pre><code>spec_version: v1\nkind: connection\napp_id: custom_service\nenvironments:\n  draft:\n    kind: key_value\n    type: key_value\n</code></pre> <p>Set credentials:</p> <pre><code>orchestrate connections set-credentials   --app-id custom_service   --env draft   --key api_key --value your_key   --key api_secret --value your_secret   --key base_url --value https://api.example.com\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#6-knowledge-bases","title":"6. Knowledge Bases","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#lab-5-create-knowledge-base-with-rag","title":"Lab 5: Create Knowledge Base with RAG","text":"<p>Objective: Build an agent that answers questions from uploaded documents.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-1-create-knowledge-base-yaml","title":"Step 1: Create Knowledge Base (YAML)","text":"<p>Create <code>company_kb.yaml</code>:</p> <pre><code>spec_version: v1\nkind: knowledge_base\nname: company_policies\ndescription: Company policies and procedures documentation\ndocuments:\n  - ./docs/employee_handbook.pdf\n  - ./docs/it_policies.pdf\n  - ./docs/hr_guidelines.pdf\nconversational_search_tool:\n  generation:\n    prompt_instruction: \"Provide accurate answers based on company policies. If unsure, say so.\"\n    max_docs_passed_to_llm: 5\n    generated_response_length: Moderate\n    idk_message: \"I don't have information about that in the company policies.\"\n  confidence_thresholds:\n    retrieval_confidence_threshold: Low\n    response_confidence_threshold: Low\n  query_rewrite:\n    enabled: true\n  citations:\n    citations_shown: 3\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-2-import-knowledge-base","title":"Step 2: Import Knowledge Base","text":"<pre><code>orchestrate knowledge-bases import -f company_kb.yaml\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-3-create-agent-with-knowledge-base","title":"Step 3: Create Agent with Knowledge Base","text":"<p>Create <code>hr-agent.yaml</code>:</p> <pre><code>spec_version: v1\nkind: native\nname: HR_Assistant\ndescription: HR assistant that answers questions about company policies\ninstructions: |\n  You are an HR assistant. Use the company_policies knowledge base to answer questions about:\n  - Employee benefits\n  - Leave policies\n  - IT guidelines\n  - Company procedures\n\n  Always cite your sources and be accurate. If you're unsure, say so.\nllm: watsonx/meta-llama/llama-3-2-90b-vision-instruct\nstyle: default\nknowledge_base: company_policies\ncollaborators: []\ntools: []\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-4-import-and-test_1","title":"Step 4: Import and Test","text":"<pre><code>orchestrate agents import -f hr-agent.yaml\norchestrate chat start\n</code></pre> <p>Test with:</p> <pre><code>What is the company's vacation policy?\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#7-flow-builder-planner-style","title":"7. Flow Builder / Planner Style","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#lab-6-company-information-flow","title":"Lab 6: Company Information Flow","text":"<p>Objective: Create a flow that validates company names and returns information.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-1-create-company-validation-tool","title":"Step 1: Create Company Validation Tool","text":"<p>Create <code>company_validator.py</code>:</p> <pre><code>from ibm_watsonx_orchestrate.agent_builder.tools import tool\n\n@tool(\n    name=\"validate_company\",\n    description=\"Check if a company exists in our database\"\n)\ndef validate_company(company_name: str) -&gt; dict:\n    \"\"\"\n    Validate if a company exists.\n\n    Args:\n        company_name (str): Name of the company\n\n    Returns:\n        dict: Validation result with company info\n    \"\"\"\n    # Simulated company database\n    companies = {\n        \"IBM\": {\n            \"exists\": True,\n            \"founded\": 1911,\n            \"industry\": \"Technology\",\n            \"headquarters\": \"Armonk, NY\"\n        },\n        \"Microsoft\": {\n            \"exists\": True,\n            \"founded\": 1975,\n            \"industry\": \"Technology\",\n            \"headquarters\": \"Redmond, WA\"\n        }\n    }\n\n    company_info = companies.get(company_name, {\"exists\": False})\n\n    return {\n        \"company_name\": company_name,\n        **company_info\n    }\n\n@tool(\n    name=\"get_company_fact\",\n    description=\"Get an interesting fact about a company\"\n)\ndef get_company_fact(company_name: str) -&gt; str:\n    \"\"\"\n    Get an interesting fact about a company.\n\n    Args:\n        company_name (str): Name of the company\n\n    Returns:\n        str: Interesting fact\n    \"\"\"\n    facts = {\n        \"IBM\": \"IBM holds more patents than any other U.S. technology company.\",\n        \"Microsoft\": \"Microsoft's original name was 'Micro-Soft' (with a hyphen).\"\n    }\n\n    return facts.get(company_name, f\"No facts available for {company_name}\")\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-2-import-tools","title":"Step 2: Import Tools","text":"<pre><code>orchestrate tools import -k python -f company_validator.py\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-3-create-flow-based-agent","title":"Step 3: Create Flow-Based Agent","text":"<p>Create <code>company-info-agent.yaml</code>:</p> <pre><code>spec_version: v1\nkind: native\nname: Company_Info_Agent\ndescription: Provides information about companies\ninstructions: |\n  When a user asks about a company:\n  1. First, validate the company exists using validate_company tool\n  2. If the company exists, provide the basic information\n  3. Then use get_company_fact to share an interesting fact\n  4. If the company doesn't exist, politely inform the user\n\n  Always be helpful and informative.\nllm: watsonx/meta-llama/llama-3-2-90b-vision-instruct\nstyle: planner\ncollaborators: []\ntools:\n  - validate_company\n  - get_company_fact\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-4-import-and-test_2","title":"Step 4: Import and Test","text":"<pre><code>orchestrate agents import -f company-info-agent.yaml\norchestrate chat start\n</code></pre> <p>Test with:</p> <pre><code>Tell me about IBM\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#8-multi-agent-systems","title":"8. Multi-Agent Systems","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#lab-7-customer-service-multi-agent-system","title":"Lab 7: Customer Service Multi-Agent System","text":"<p>Objective: Create a supervisor agent that delegates to specialized agents.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-1-create-specialized-agents","title":"Step 1: Create Specialized Agents","text":"<p>Billing Agent \u2013 <code>billing-agent.yaml</code>:</p> <pre><code>spec_version: v1\nkind: native\nname: Billing_Agent\ndescription: Handles billing inquiries, invoices, and payment issues\ninstructions: |\n  You are a billing specialist. Help customers with:\n  - Invoice questions\n  - Payment issues\n  - Billing disputes\n  - Account balance inquiries\n\n  Be professional and empathetic.\nllm: watsonx/meta-llama/llama-3-2-90b-vision-instruct\nstyle: default\ncollaborators: []\ntools: []\n</code></pre> <p>Technical Support Agent \u2013 <code>tech-support-agent.yaml</code>:</p> <pre><code>spec_version: v1\nkind: native\nname: Tech_Support_Agent\ndescription: Provides technical support and troubleshooting assistance\ninstructions: |\n  You are a technical support specialist. Help customers with:\n  - Product setup and configuration\n  - Troubleshooting technical issues\n  - Software updates\n  - Technical documentation\n\n  Provide clear, step-by-step guidance.\nllm: watsonx/meta-llama/llama-3-2-90b-vision-instruct\nstyle: default\ncollaborators: []\ntools: []\n</code></pre> <p>Account Management Agent \u2013 <code>account-agent.yaml</code>:</p> <pre><code>spec_version: v1\nkind: native\nname: Account_Agent\ndescription: Manages account settings, profile updates, and subscriptions\ninstructions: |\n  You are an account management specialist. Help customers with:\n  - Profile updates\n  - Subscription changes\n  - Account settings\n  - Security settings\n\n  Ensure customer data privacy and security.\nllm: watsonx/meta-llama/llama-3-2-90b-vision-instruct\nstyle: default\ncollaborators: []\ntools: []\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-2-import-specialized-agents","title":"Step 2: Import Specialized Agents","text":"<pre><code>orchestrate agents import -f billing-agent.yaml\norchestrate agents import -f tech-support-agent.yaml\norchestrate agents import -f account-agent.yaml\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-3-create-supervisor-agent","title":"Step 3: Create Supervisor Agent","text":"<p>Create <code>customer-service-supervisor.yaml</code>:</p> <pre><code>spec_version: v1\nkind: native\nname: Customer_Service_Supervisor\ndescription: Main customer service agent that routes requests to specialized agents\ninstructions: |\n  You are a customer service supervisor. Analyze customer requests and route them to the appropriate specialist:\n\n  - Billing_Agent: For billing, invoices, payments, charges\n  - Tech_Support_Agent: For technical issues, setup, troubleshooting\n  - Account_Agent: For account settings, profile, subscriptions\n\n  If a request involves multiple areas, handle them sequentially.\n  Always greet customers warmly and summarize the resolution.\nllm: watsonx/meta-llama/llama-3-2-90b-vision-instruct\nstyle: planner\ncollaborators:\n  - Billing_Agent\n  - Tech_Support_Agent\n  - Account_Agent\ntools: []\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-4-import-and-test_3","title":"Step 4: Import and Test","text":"<pre><code>orchestrate agents import -f customer-service-supervisor.yaml\norchestrate chat start\n</code></pre> <p>Test with:</p> <pre><code>I have a question about my last invoice and need help setting up my account\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#9-agent-protocols-mcp-a2a","title":"9. Agent Protocols (MCP &amp; A2A)","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#lab-8-mcp-server-integration","title":"Lab 8: MCP Server Integration","text":"<p>Objective: Connect to an MCP server for additional tools.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-1-install-mcp-toolkit","title":"Step 1: Install MCP Toolkit","text":"<pre><code># Import MCP toolkit from npm\norchestrate toolkits import   --kind mcp   --name filesystem   --source npm   --package @modelcontextprotocol/server-filesystem   --args '{\"rootPath\": \"/tmp/workspace\"}'\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-2-create-agent-using-mcp-tools","title":"Step 2: Create Agent Using MCP Tools","text":"<p>Create <code>file-manager-agent.yaml</code>:</p> <pre><code>spec_version: v1\nkind: native\nname: File_Manager_Agent\ndescription: Agent that can read and manage files using MCP\ninstructions: |\n  You are a file management assistant. You can:\n  - List files in directories\n  - Read file contents\n  - Search for files\n\n  Use the MCP filesystem tools to help users manage their files.\nllm: watsonx/meta-llama/llama-3-2-90b-vision-instruct\nstyle: react\ncollaborators: []\ntools:\n  - filesystem:list_directory\n  - filesystem:read_file\n  - filesystem:search_files\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-3-import-and-test","title":"Step 3: Import and Test","text":"<pre><code>orchestrate agents import -f file-manager-agent.yaml\norchestrate chat start\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#lab-9-external-agent-integration-a2a","title":"Lab 9: External Agent Integration (A2A)","text":"<p>Objective: Connect an external agent from another platform.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-1-create-external-agent-configuration","title":"Step 1: Create External Agent Configuration","text":"<p>Create <code>external-news-agent.yaml</code>:</p> <pre><code>spec_version: v1\nkind: external\nname: News_Agent\ntitle: News Agent\ndescription: External agent that searches and summarizes news articles\napi_url: https://your-external-agent-url.com/api/chat\nauth_scheme: API_KEY\nauth_config:\n  token: your_external_agent_api_key\nchat_params:\n  stream: true\nconfig:\n  hidden: false\n  enable_cot: true\ntags:\n  - news\n  - external\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-2-import-external-agent","title":"Step 2: Import External Agent","text":"<pre><code>orchestrate agents import -f external-news-agent.yaml\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-3-create-supervisor-with-external-collaborator","title":"Step 3: Create Supervisor with External Collaborator","text":"<p>Create <code>news-supervisor.yaml</code>:</p> <pre><code>spec_version: v1\nkind: native\nname: News_Supervisor\ndescription: Supervisor that can delegate to external news agent\ninstructions: |\n  You coordinate news-related requests. When users ask about current events or news:\n  1. Delegate to the News_Agent collaborator\n  2. Summarize the results for the user\n  3. Offer to search for related topics\nllm: watsonx/meta-llama/llama-3-2-90b-vision-instruct\nstyle: default\ncollaborators:\n  - News_Agent\ntools: []\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-4-import-and-test_4","title":"Step 4: Import and Test","text":"<pre><code>orchestrate agents import -f news-supervisor.yaml\norchestrate chat start\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#10-production-deployment","title":"10. Production Deployment","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#lab-10-deploy-to-production","title":"Lab 10: Deploy to Production","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-1-add-production-environment","title":"Step 1: Add Production Environment","text":"<pre><code>orchestrate env add   -n production   -u https://your-production-instance.cloud.ibm.com\n\norchestrate env activate production --api-key your-production-api-key\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-2-export-agent-from-local","title":"Step 2: Export Agent from Local","text":"<pre><code># Switch to local environment\norchestrate env activate local\n\n# Export agent\norchestrate agents export -n Weather_Agent -o weather-agent-export.yaml\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-3-import-to-production","title":"Step 3: Import to Production","text":"<pre><code># Switch to production\norchestrate env activate production\n\n# Import agent\norchestrate agents import -f weather-agent-export.yaml\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-4-deploy-agent","title":"Step 4: Deploy Agent","text":"<pre><code>orchestrate agents deploy --name Weather_Agent\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#step-5-verify-deployment","title":"Step 5: Verify Deployment","text":"<pre><code>orchestrate agents list\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#workshop-exercises","title":"Workshop Exercises","text":"<p>You can use the following as optional or advanced exercises.</p>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#exercise-1-build-a-restaurant-recommendation-agent","title":"Exercise 1: Build a Restaurant Recommendation Agent","text":"<p>Requirements:</p> <ul> <li>Create a tool that searches restaurants by cuisine type.</li> <li>Create a tool that gets restaurant reviews.</li> <li>Build an agent that recommends restaurants based on user preferences.</li> <li>Use the ReAct style for iterative reasoning.</li> </ul>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#exercise-2-document-processing-pipeline","title":"Exercise 2: Document Processing Pipeline","text":"<p>Requirements:</p> <ul> <li>Create a knowledge base from company documents.</li> <li>Build an agent that can answer questions from the documents.</li> <li>Implement confidence thresholds.</li> <li>Add citation support.</li> </ul>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#exercise-3-multi-agent-workflow","title":"Exercise 3: Multi-Agent Workflow","text":"<p>Requirements:</p> <ul> <li>Create 3 specialized agents (your choice of domains).</li> <li>Build a supervisor agent that routes requests.</li> <li>Test with complex multi-step queries.</li> <li>Implement error handling.</li> </ul>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#best-practices","title":"Best Practices","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#agent-design","title":"Agent Design","text":"<ul> <li>Single responsibility \u2013 each agent should have a clear, focused purpose.  </li> <li>Clear instructions \u2013 write detailed, unambiguous instructions.  </li> <li>Tool selection \u2013 only include tools the agent actually needs.  </li> <li>Testing \u2013 test agents thoroughly before deployment.</li> </ul>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#tool-development","title":"Tool Development","text":"<ul> <li>Error handling \u2013 always include try\u2013except blocks.  </li> <li>Type hints \u2013 use Python type hints for better LLM understanding.  </li> <li>Documentation \u2013 write clear docstrings.  </li> <li>Validation \u2013 validate inputs before processing.</li> </ul>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#security","title":"Security","text":"<ul> <li>Credentials \u2013 never hardcode credentials.  </li> <li>Connections \u2013 use the connection framework for all external APIs.  </li> <li>Validation \u2013 validate all user inputs.  </li> <li>Least privilege \u2013 grant minimum necessary permissions.</li> </ul>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#performance","title":"Performance","text":"<ul> <li>Caching \u2013 cache frequently accessed data.  </li> <li>Timeouts \u2013 set appropriate timeouts for external calls.  </li> <li>Batch processing \u2013 process multiple items together when possible.  </li> <li>Monitoring \u2013 use observability tools to track performance.</li> </ul>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#common-issues","title":"Common Issues","text":"<p>Agent not responding:</p> <pre><code># Check agent status\norchestrate agents list\n\n# View agent details\norchestrate agents get -n Your_Agent_Name\n\n# Check logs\ndocker logs docker-api-1\n</code></pre> <p>Tool import fails:</p> <pre><code># Verify tool syntax\npython your_tool.py\n\n# Check dependencies\npip install -r requirements.txt\n\n# Import with verbose output\norchestrate tools import -k python -f your_tool.py --verbose\n</code></pre> <p>Connection issues:</p> <pre><code># List connections\norchestrate connections list\n\n# Verify credentials\norchestrate connections get --app-id your_app_id\n\n# Reset credentials\norchestrate connections set-credentials --app-id your_app_id --env draft\n</code></pre>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#additional-resources","title":"Additional Resources","text":""},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#documentation","title":"Documentation","text":"<ul> <li>watsonx Orchestrate ADK documentation  </li> <li>Agent builder guide  </li> <li>Tool development guide</li> </ul>"},{"location":"tracks/day3-orchestrate/watsonx-orchestrate-labs/#community","title":"Community","text":"<ul> <li>GitHub: <code>ibm-watsonx-orchestrate-adk</code> </li> <li>Examples: ADK examples repo</li> </ul> <p>Use these resources to continue experimenting beyond the workshop.</p>"}]}