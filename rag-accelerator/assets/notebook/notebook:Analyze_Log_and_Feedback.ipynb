{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "f91dc51f-691f-466d-bdcd-84f46d47709d",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# RAG Function Log Analysis and Feedback Evaluation with Topic Modeling\n",
        "\n",
        "Use this notebook to gain insights into user's feedback.<br><br>\n",
        "This notebook implements log data retrieval, data preparation, topic modeling and correlation analysis.<br>\n",
        "It forms a basis for feedback analysis and is intended to be enhanced by more functions.\n",
        "\n",
        "\n",
        "## Contents\n",
        "\n",
        "This notebook contains the following parts:\n",
        "- [Pre-Requisite Libraries and Dependencies](#Pre-Requisite-Libraries-and-Dependencies)\n",
        "- [Connecting to Vector Database](#connect)\n",
        "- [Import Log Records](#Import-Log-Records)\n",
        "- [Map Feedback to Percentage](#Map-Feedback-to-Percentage)\n",
        "- [Rating Distribution](#Rating-Distribution)\n",
        "- [Topic Modeling](#Topic-Modeling)\n",
        "- [Topic Labels](#Topic-Labels)\n",
        "- [Score by Topic](#Score-by-Topic)\n",
        "- [Score by Response Length](#Score-by-Response-Length)\n",
        "- [Document Search Score by Topic](#Document-Search-Score-by-Topic)\n",
        "- [Feedback on Answer by Topic](#Feedback-on-Answer-by-Topic)\n",
        "\n",
        "## Topic Modeling\n",
        "\n",
        "Topic modeling is a key feature of this analysis notebook. It is an unsupervised learning technique that clusters a set of text documents into groups by detecting common word and phrase pattern. Each group is represented by a topic, which is a set of keywords that appear to be relevant in the belonging documents. Depending on its keywords affinity, a topic might appear explicit or somewhat abstract.\n",
        "Two models for topic modelling are supported:\n",
        "* **Watson NLP**: A hierarchical topic model that supports a lot of tuning options, in particular stop word optimization.\n",
        "* **Top2Vec**: An algorithm that leverages document and word embeddings to find topic vectors. It has few tuning options and does not require stop word removal. Its recommended to use this notebook with an appropriate environment (\"custom-top2vec-template\") template and update runtime accordingly.\n",
        "* **BERTopic**: A model based on transformers and c-TF-IDF with few tuning options but very good out-of-the-box performance.\n",
        "\n",
        "Watson NLP is used if parameter `topic_modeling_method` is set to `watson_nlp` or not set at all AND Python module `watson_nlp` is available. On IBM Cloud, this notebook must run in an appropriate environment (\"NLP + DO runtime...\").<br>\n",
        "Top2Vec is used if parameter `topic_modeling_method` is set to `top2vec`<br>\n",
        "BERTopic is used if parameter `topic_modeling_method` is set to `bertopic`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cf88c51-0017-45ea-a15f-38440d94f051"
      },
      "source": [
        "### Pre-Requisite Libraries and Dependencies\n",
        "Download and import mandatory libraries and dependencies. \n",
        "\n",
        "Note : Some of the versions of the libraries may throw warnings after installation. These library versions are crucial for successful execution of the accelerator. Please ignore the warning/error and proceed with your execution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "148dca6e-ca25-46d6-a22c-30c7852726d7"
      },
      "outputs": [],
      "source": [
        "!pip install elasticsearch==8.17.2 | tail -n 1\n",
        "!pip install wordcloud | tail -n 1\n",
        "!pip install pymilvus==2.5.11 | tail -n 1\n",
        "!pip install 'torch>=2.3.0' | tail -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdaa0b8e-053a-4014-bf53-2ed6f0e27d68"
      },
      "source": [
        "Restart the kernel after performing the pip install if the below cell fails to import all the libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eb14732-5cf0-47fe-9af3-9266a9b99243"
      },
      "outputs": [],
      "source": [
        "from elasticsearch import Elasticsearch, helpers\n",
        "from wordcloud import WordCloud\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from datetime import datetime, timedelta\n",
        "from matplotlib.colors import hsv_to_rgb, TABLEAU_COLORS\n",
        "\n",
        "from pymilvus import(connections,Collection,utility)\n",
        "from ibm_watsonx_ai import APIClient\n",
        "from ibm_watsonx_ai import Credentials\n",
        "from ibm_watsonx_ai.foundation_models.prompts import PromptTemplate, PromptTemplateManager\n",
        "from ibm_watsonx_ai.foundation_models.utils.enums import PromptTemplateFormats\n",
        "from ibm_watsonx_ai.foundation_models import ModelInference, get_supported_tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5b7c84d-b225-4cf1-b753-b1feeb5cb02b"
      },
      "outputs": [],
      "source": [
        "project_id=os.environ['PROJECT_ID']\n",
        "# Environment and host url\n",
        "hostname = os.environ['RUNTIME_ENV_APSX_URL']\n",
        "\n",
        "if hostname.endswith(\"cloud.ibm.com\") == True:\n",
        "    environment = \"cloud\"\n",
        "    runtime_region = os.environ[\"RUNTIME_ENV_REGION\"]\n",
        "else:\n",
        "    environment = \"on-prem\"\n",
        "    from ibm_watson_studio_lib import access_project_or_space\n",
        "    wslib = access_project_or_space()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510ec21f-e377-41ca-8f8b-be8f4067639f"
      },
      "source": [
        "### Import Parameter Set, Credentials and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f523549d-2cc1-4756-933c-23c0da7120f3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    filename = 'rag_helper_functions.py'\n",
        "    wslib.download_file(filename)\n",
        "    import rag_helper_functions\n",
        "    print(\"rag_helper_functions imported from the project assets\")\n",
        "except NameError as e:\n",
        "    print(str(e))\n",
        "    print(\"If running watsonx.ai aaS on IBM Cloud, check that the first cell in the notebook contains a project token. If not, select the vertical ellipsis button from the notebook toolbar and `insert project token`. Also check that you have specified your ibm_api_key in the second code cell of the notebook\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ece41b3-033a-48a4-8cfc-adfc4817a705"
      },
      "outputs": [],
      "source": [
        "parameter_sets = [\"RAG_parameter_set\",\"RAG_advanced_parameter_set\"]\n",
        "\n",
        "parameters=rag_helper_functions.get_parameter_sets(wslib, parameter_sets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dac92b34-c7ab-433b-a449-8e69387dd459"
      },
      "outputs": [],
      "source": [
        "ibm_api_key=parameters['watsonx_ai_api_key']\n",
        "space_uid = parameters['watsonx_ai_space_id']\n",
        "\n",
        "if environment == \"cloud\":\n",
        "    WML_SERVICE_URL=f\"https://{runtime_region}.ml.cloud.ibm.com\" \n",
        "    wml_credentials = Credentials(api_key=parameters['watsonx_ai_api_key'], url=WML_SERVICE_URL)\n",
        "else:\n",
        "    token = os.environ['USER_ACCESS_TOKEN']\n",
        "    wml_credentials=Credentials(token=os.environ['USER_ACCESS_TOKEN'],url=hostname,instance_id='openshift')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "822f530a-f6b7-46bc-a351-35ac151c04b8"
      },
      "source": [
        "### Set Watsonx.ai client\n",
        "Below cell uses the watson machine learning credentials to create an API client to interact with the project and deployment space. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebafb690-510c-4fd9-a204-14deaad78799"
      },
      "outputs": [],
      "source": [
        "client = APIClient(wml_credentials)\n",
        "client.set.default_project(project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connecting to Vector Database\n",
        "#### Connecting using Project Connection Asset (default)\n",
        "The notebook, by default, will look for a connection asset in the project named `milvus_connect` or `elasticsearch_connect` or `datastax_connect`.  You can set this up by following the instructions in the project readme. \n",
        "This code checks if a specified connection exists in the project. If found, it retrieves the connection details and identifies the connection type. Depending on the connection type, it establishes a connection to the appropriate database. If the connection is not found, it raises an error indicating the absence of the specified connection in the project.\n",
        "\n",
        "**Note** Datastax is not supported in this cloud version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46d4404f-e1ad-4c4c-92e6-453fa48cb5fb"
      },
      "outputs": [],
      "source": [
        "if not 'log_index_name' in parameters or parameters['log_index_name'] == '':\n",
        "    print(f\"Log index is not specified.\")\n",
        "    raise\n",
        "else:\n",
        "    log_index_name = parameters['log_index_name']\n",
        "    log_connection_name=parameters[\"log_connection_asset\"]\n",
        "    if(next((conn for conn in wslib.list_connections() if conn['name'] == log_connection_name), None)):\n",
        "        print(log_connection_name, \"Log connection found in the project\")\n",
        "        log_db_connection = wslib.get_connection(log_connection_name)\n",
        "        print(\"Successfully retrieved the log connection details\")\n",
        "        log_connection_datatypesource_id=client.connections.get_details(log_db_connection['.']['asset_id'])['entity']['datasource_type']\n",
        "        log_connection_type = client.connections.get_datasource_type_details_by_id(log_connection_datatypesource_id)['entity']['name']\n",
        "        log_client = None\n",
        "        print(\"Log Connection type is identified as:\",log_connection_name)\n",
        "    try:\n",
        "        if log_connection_type == 'elasticsearch':\n",
        "            log_client = rag_helper_functions.create_and_check_elastic_client(log_db_connection, parameters['elastic_search_model_id'])\n",
        "        elif log_connection_type == \"milvus\" or log_connection_type == \"milvuswxd\":\n",
        "            milvus_credentials=rag_helper_functions.connect_to_milvus_database(log_db_connection, parameters)  \n",
        "        elif log_connection_type == \"datastax\":\n",
        "            if environment == \"cloud\":\n",
        "                raise ValueError(f\"ERROR! we don't support datastax connection for Cloud as of now\")\n",
        "            datastax_session,datastax_cluster = rag_helper_functions.connect_to_datastax(log_db_connection, parameters)\n",
        "    except:\n",
        "        print(f\"Cannot connect to databased for index {log_index_name}.\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "064c2548-6079-40f1-afcb-8b5bf4935640"
      },
      "source": [
        "## Import Log Records\n",
        "\n",
        "Read log records from Elasticsearch index or Milvus Collection or Datastax Collection based on the log connection type. Use query to filter retrieved records.\\\n",
        "By default, data is retrieved for last 30 days. If custom date range is provided, then it will be used for filtered data retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88758eeb-a28d-4710-835c-39f66993ea96"
      },
      "source": [
        "Date should be in **`mm/dd/yyyy`** format and valid date range should be provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f244e52-018f-4cbd-98c5-81b5b7937702"
      },
      "outputs": [],
      "source": [
        "# For custom date range filtering, specify both dates in mm/dd/yyyy format.\n",
        "start_dt = ''\n",
        "end_dt = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8829a50-5f6d-469d-8e1e-2fb5d9676f6d"
      },
      "outputs": [],
      "source": [
        "query_es = {}\n",
        "query_mil = ''\n",
        "try:\n",
        "    date_curr = datetime.now()\n",
        "\n",
        "    if end_dt == '':\n",
        "        date_en = datetime.now()\n",
        "    else:\n",
        "        date_en = datetime.strptime(end_dt, '%m/%d/%Y')\n",
        "        if date_en > date_curr:\n",
        "            date_en = date_curr\n",
        "        \n",
        "    if start_dt == '':\n",
        "        date_st = date_en - timedelta(30)\n",
        "    else:\n",
        "        date_st = datetime.strptime(start_dt, '%m/%d/%Y')\n",
        "\n",
        "    if date_st > date_curr:\n",
        "        raise Exception('Invalid start date provided! It cannot be ahead of time.') \n",
        "\n",
        "    if date_st > date_en:\n",
        "        raise Exception('Invalid date range provided!')\n",
        "\n",
        "    print(f\"Will retrieve log records from {date_st} to {date_en}\")\n",
        "\n",
        "\n",
        "    # Elasticsearch query\n",
        "    query_es = {\"bool\": {\"must\": [ {\"range\": {\"log_timestamp\": {\"gte\": date_st.isoformat(), \"lte\": date_en.isoformat()}}} ] } }\n",
        "    \n",
        "    # Milvus query\n",
        "    query_mil = ''\n",
        "    delta = timedelta(days=1)\n",
        "    while date_st <= date_en:\n",
        "        dt = date_st.isoformat()\n",
        "        if len(query_mil) > 0:\n",
        "            query_mil += ' or '\n",
        "        query_mil += f\"log_timestamp like '{dt[:11]}%'\"\n",
        "        date_st += delta\n",
        "        \n",
        "except ValueError as e:\n",
        "    print(f\"Error: Invalid date provided: {e}. Please provide valid dates to proceed!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e} Please provide a valid date range to proceed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "066ff4b7-cf5f-4dc9-bdd7-32813f48a6ac"
      },
      "outputs": [],
      "source": [
        "if query_es == {} and query_mil == '':\n",
        "    raise Exception(f\"Query not generated!. Please provide a valid date range to proceed!!\")\n",
        "else:       \n",
        "    try:\n",
        "        response_data = []\n",
        "        if log_connection_type == 'elasticsearch':\n",
        "            log_query = {\"query\": query_es, \"_source\": [\"question\", \"response\", \"feedback\", \"source_documents.score\"]}\n",
        "            log_response = helpers.scan(\n",
        "                log_client, \n",
        "                index = log_index_name,\n",
        "                query = log_query\n",
        "                )\n",
        "            response_data = [dict(item)['_source'] for item in log_response]\n",
        "            if len(response_data) == 0:\n",
        "                # workaround in case helpers.scan function is not working: search\n",
        "                print(f\"Could not 'scan' data, trying 'search'.\")\n",
        "                log_response = log_client.search(\n",
        "                    size = 10000,\n",
        "                    index = log_index_name, \n",
        "                    body = log_query\n",
        "                )\n",
        "                response_data = [dict(item)['_source'] for item in log_response['hits']['hits']]\n",
        "        elif log_connection_type == \"milvus\" or log_connection_type == \"milvuswxd\":\n",
        "            collection = Collection(name=log_index_name)\n",
        "            log_response = collection.query(expr=query_mil, output_fields=[\"question\", \"response\", \"feedback\", \"source_documents\"])\n",
        "            if len(log_response)>=1:\n",
        "                for item in log_response:\n",
        "                    source_docs = item.pop('source_documents')\n",
        "                    source_doc_score = []\n",
        "                    for doc in source_docs:\n",
        "                        source_doc_score.append({'score': doc['score']})\n",
        "                    item.update({'source_documents': source_doc_score})\n",
        "                    response_data.append(copy.deepcopy(item))\n",
        "        elif log_connection_type == \"datastax\":\n",
        "            keyspace=log_db_connection.get('keyspace')\n",
        "            #select_log_query = datastax_session.prepare(f\"SELECT * FROM {keyspace}.{log_index_name} WHERE log_timestamp >= ? and log_timestamp <= ? ALLOW FILTERING\")\n",
        "            select_query = datastax_session.prepare(f\"SELECT * FROM {keyspace}.{log_index_name} WHERE feedback !='' ALLOW FILTERING\")\n",
        "            results=datastax_session.execute(select_query)\n",
        "            \n",
        "            for row in results:\n",
        "                question=row.question\n",
        "                expert_details=row.expert_details\n",
        "                import ast\n",
        "                source_docs = ast.literal_eval(row.source_documents)\n",
        "                source_doc_score = []\n",
        "                for doc in source_docs:\n",
        "                    source_doc_score.append({'score': doc['score']})\n",
        "                    row_item={  \n",
        "                            \"question\": row.question, \n",
        "                            \"response\": row.response, \n",
        "                            \"feedback\": ast.literal_eval(row.feedback), \n",
        "                            \"source_documents\": source_doc_score\n",
        "                    }\n",
        "                response_data.append(row_item)\n",
        "            \n",
        "    except:\n",
        "        print(f\"Cannot read data from index {log_index_name}.\")\n",
        "        raise\n",
        "        \n",
        "    data = pd.DataFrame().from_dict(response_data)\n",
        "    \n",
        "    if len(data)==0:\n",
        "        raise Exception('No Feedback records found in the log index! Please index feedback records & re run this notebook')\n",
        "    else:\n",
        "        print(f\"{str(len(data))} log records read.\")\n",
        "\n",
        "    data['source_documents'] = data['source_documents'].apply(lambda x: x if type(x)==list else [])\n",
        "    data['feedback'] = data['feedback'].fillna('') if 'feedback' in data.columns else ''\n",
        "    \n",
        "    if len(data)<25:\n",
        "        raise Exception('Volume of log entries is too low to run the analysis in this notebook! Please index more feedback records and re-run this notebook')\n",
        "        \n",
        "    if len(data)<50:\n",
        "        print(\"Warning: Volume of log entries is low! Some parts of the notebook may fail or provide inconsistent results due to low volume of data.\")\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7c2a41-d021-4256-9a7a-2736d43028d2"
      },
      "source": [
        "## Map Feedback to Percentage\n",
        "Map the user rating to a percentage by linear interpolation. **Best rating maps to 100%, worst rating maps to 0%. If no feedback is given, rating percentage is -1.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccd9ab88-32c2-42ad-b76a-8c20eb2fdfd0"
      },
      "outputs": [],
      "source": [
        "# Thumb up, thumb down rating\n",
        "#supported_ratings = ['positive', 'negative']\n",
        "\n",
        "# 5 rating options\n",
        "supported_ratings = ['perfect', 'good', 'ok', 'bad', 'very bad']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfe57a8a-e984-4b3d-ad73-47270ae4a693"
      },
      "outputs": [],
      "source": [
        "rating_count = len(supported_ratings)\n",
        "rating_percentage = {supported_ratings[i]: int(100.5 - i * (100 / (rating_count-1))) for i in range(rating_count)} | {'n/a': -1}\n",
        "rating_options = list(rating_percentage.keys())\n",
        "rating_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ace306b3-e855-44c9-b232-857ce955be6c"
      },
      "outputs": [],
      "source": [
        "data['eval_rating_percent'] = data['feedback'].map(lambda val: -1 if not isinstance(val, dict) or len(val)==0 or not 'value' in val else \n",
        "    int(val['value']) if val['value'].isnumeric() else \n",
        "    rating_percentage[val['value']] if val['value'] in rating_options else\n",
        "    -1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef49f0af-05c5-45ad-bd96-1d3868f7c7cf"
      },
      "outputs": [],
      "source": [
        "data['eval_rating'] = data['eval_rating_percent'].map(lambda val: \\\n",
        "    rating_options[len([v for v in [rating_percentage[rating] for rating in rating_percentage.keys()] if v > val])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc41fad5-7c16-4551-ba0e-31a3f42444db"
      },
      "source": [
        "## Colors\n",
        "Calculate colors for ratings using HSV (hue, saturation, value) format. Loop on hue from 33% (green) to 0% (red) and on value from 0.6 to 1 to 0.6 (sinus curve).<br>\n",
        "A color is calculated for each rating option, including lightgray for \"no feedback\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42df3f9f-3973-4461-ae85-ee87193242ae"
      },
      "outputs": [],
      "source": [
        "rating_color = [hsv_to_rgb((0.33 * (x/(rating_count-1)), 1, 0.6 + 0.4 * math.sin((x/(rating_count-1)) * math.pi))) for x in range(rating_count-1, -1, -1)] + [\"lightgray\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9d6423-e2a3-4cb3-bbaa-25bd3dea348a"
      },
      "source": [
        "\n",
        "## Rating Distribution\n",
        "Number of user rates by rating option.<br>\n",
        "Shows the rating distribution, \"n/a\" for records without feedback (often the largest number). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffed8012-682f-4442-9f4a-5912feb3e78b"
      },
      "outputs": [],
      "source": [
        "distribution = data['eval_rating'].value_counts()\n",
        "distribution = {key: distribution[key] if key in distribution.keys() else 0 for key in rating_options}\n",
        "plt.bar(distribution.keys(), distribution.values(), color = rating_color)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d13a0777-fdcf-4958-9ba6-8756b93a1af6"
      },
      "source": [
        "\n",
        "## Topic Modeling\n",
        "Topic modeling can be done on user questions and/or on RAG function responses.<br>\n",
        "Topic modeling on user questions might be less valuable due to shorter texts and deviate questions.<br>\n",
        "Topic modeling on RAG function responses only cannot find topics that are asked for by the user but not part of the content database.<br>\n",
        "**Depending on the number of documents, the topic modeling procedure may take some time.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5525268f-7f3e-46ec-b2e7-fc6b54b33790"
      },
      "outputs": [],
      "source": [
        "# Topic modeling on user questions\n",
        "evaluation_field = 'question'\n",
        "\n",
        "# Topic modeling on RAG function responses\n",
        "#evaluation_field = 'response'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c3f7ff1-9e8b-4af8-8ec1-1feb3aedfa87"
      },
      "outputs": [],
      "source": [
        "# Determine topic modeling method. Possible values are 'watson_nlp', 'top2vec', 'bertopic'\n",
        "topic_modeling_methods_supported = ['watson_nlp', 'top2vec', 'bertopic']\n",
        "topic_modeling_method = topic_modeling_methods_supported[0] if 'topic_modeling_method' not in parameters else parameters['topic_modeling_method'].lower()\n",
        "if not topic_modeling_method in topic_modeling_methods_supported:\n",
        "    raise Exception(f\"Topic modeling method '{topic_modeling_method}' not supported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59b7e4b5-ca38-41bb-b762-6ec796d3b3cc"
      },
      "source": [
        "### Watson NLP\n",
        "Topic detection with Watson NLP. To optimize detected topic, add words to `stopwords` that appear in your log records on various subjects and are therefore not beneficial for topic separation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53c1bb48-d9fa-469d-9e5f-e0046943cc25"
      },
      "outputs": [],
      "source": [
        "if not topic_modeling_method == topic_modeling_methods_supported[0]:\n",
        "    print(f\"Skipped ... Topic modeling method is '{topic_modeling_method}'.\")\n",
        "else:\n",
        "    try:\n",
        "        import watson_nlp\n",
        "        from watson_nlp.toolkit.summary_utils import NGramSummary\n",
        "        from watson_nlp.blocks.topics import HierarchicalClustering\n",
        "        from watson_nlp import data_model as dm\n",
        "    except:\n",
        "        topic_modeling_method = topic_modeling_methods_supported[1]\n",
        "        print(f\"Watson NLP not used: Watson NLP not available in runtime.\\nSwitching to topic detection method {topic_modeling_method}.\")\n",
        "\n",
        "if topic_modeling_method == topic_modeling_methods_supported[0]:\n",
        "\n",
        "    eval_df = pd.DataFrame(data[evaluation_field])\n",
        "    train_file = './TMP_train_data.csv'\n",
        "    eval_df.to_csv(train_file)\n",
        "\n",
        "    # load the syntax model\n",
        "    syntax_model = watson_nlp.load('syntax_izumo_en_stock')\n",
        "\n",
        "    csv_stream = dm.DataStream.from_csv(train_file, skip=1)\n",
        "    syntax_data = syntax_model.stream(csv_stream[1])\n",
        "\n",
        "    wnlp_stopwords = watson_nlp.load('text_stopwords_classification_ensemble_en_stock').stopwords\n",
        "    stopwords = list(wnlp_stopwords)\n",
        "    stopwords.extend(['create','add','delete','select','find','click','go','app','allow','type','based','base','provided','answer','question','following','information','difference'])\n",
        "    # words that do not separate topics in actual business context, adjust to meet your needs\n",
        "    #stopwords.extend(['transaction','report','program','code','reference','business','document','data','date'])\n",
        "\n",
        "    summary_model = NGramSummary.train(train_data=syntax_data, train_params={\n",
        "        'min_words_per_utterance': 5,\n",
        "        'num_turns_to_remove': 0,\n",
        "        'beginning_ratio': 1,\n",
        "        'beginning_weighting_factor': 5,\n",
        "        'min_ngram_size': 2,\n",
        "        'max_ngram_size': 3,\n",
        "        'max_ngrams': 50,\n",
        "        'stopwords': list(stopwords)\n",
        "    })\n",
        "\n",
        "    topic_model = HierarchicalClustering.train(train_data=syntax_data, summary_model=summary_model, train_params = {\n",
        "        'king_cluster_min_ratio': 1.5, \n",
        "        'min_records_per_king_cluster': 10 + int(len(eval_df) / 50),\n",
        "        'num_topics_per_iteration': 10, \n",
        "        'max_num_iters_per_model': 4, \n",
        "        'max_ngrams_per_topic': 10\n",
        "    })\n",
        "\n",
        "    topics_list = sorted(topic_model.model.to_json_summary()['clusters'], key=lambda t: t['numDocuments'], reverse=True)\n",
        "\n",
        "    keywords_list = {}\n",
        "    for _t in topics_list:\n",
        "        keywords = {s.split(',')[0].split('(')[0].strip(): float(s.split(',')[-1]) for s in _t['modelWords']}\n",
        "        if _t['topicName'] in keywords_list:\n",
        "            keywords_list[_t['topicName']] = keywords_list[_t['topicName']] | keywords\n",
        "        else:\n",
        "            keywords_list[_t['topicName']] = keywords\n",
        "        \n",
        "    topicnames = {i: name for i, name in enumerate(keywords_list.keys())} | {-1: 'unknown'}\n",
        "    topicnames_reverse = {topicnames[key]: key for key in topicnames.keys()}\n",
        "\n",
        "    def extract(item):\n",
        "        return topicnames_reverse[item.topics[0].name] if len(item.topics) > 0 and item.topics[0].name in topicnames_reverse else -1\n",
        "    data['eval_topic'] = data[evaluation_field].map(lambda r: extract(topic_model.run(syntax_model.run(r.strip()))))\n",
        "    \n",
        "    print(f\"{str(len(topicnames)-1)} topics detected.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65154f39-86ec-4194-9369-f7789f25946f"
      },
      "source": [
        "### Top2Vec\n",
        "Top2Vec finds topic vectors that are jointly embedded with the document and word vectors with distance between them representing semantic similarity. It does not require stopword lists, stemming or lemmatization. Also, it automatically finds the number of topics. See also https://github.com/ddangelov/Top2Vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b73f40b5-7737-49cb-80e1-d3c2b2d52a08"
      },
      "outputs": [],
      "source": [
        "if not topic_modeling_method == topic_modeling_methods_supported[1]:\n",
        "    print(f\"Skipped ... Topic modeling method is '{topic_modeling_method}'.\")\n",
        "else:\n",
        "    keywords_list, topicnames = dict(), dict()\n",
        "    try:\n",
        "        !pip install scikit-learn==1.6.1 | tail -n 1\n",
        "        !pip install -U top2vec==1.0.36 | tail -n 1\n",
        "        try:\n",
        "            from top2vec import top2vec\n",
        "        except ImportError as e:\n",
        "            raise ImportError(f\"{str(e)}\\n Please Restart kernel and re-run this notebook to continue. Or use `custom_top2vec_template` runtime for this notebook when this topic modelling method is selected!\")\n",
        "\n",
        "        # perform model detection\n",
        "        documents = data[evaluation_field].tolist()\n",
        "        top2vec_config = {\n",
        "            'min_count': 1,\n",
        "            'embedding_model': 'all-MiniLM-L6-v2',\n",
        "            'umap_args': {\n",
        "                'n_neighbors': 10, \n",
        "                'n_components': 2, \n",
        "                'min_dist': 0.1, \n",
        "                'random_state': 42\n",
        "            },\n",
        "            'hdbscan_args': {\n",
        "                'min_cluster_size': 25,\n",
        "                'min_samples': 5\n",
        "            },\n",
        "            'contextual_top2vec': True\n",
        "        }\n",
        "        topic_model = top2vec.Top2Vec(documents=documents, **top2vec_config)\n",
        "\n",
        "        # get topic detection results\n",
        "        topic_words, word_scores, topic_nums = topic_model.get_topics(num_topics=None)\n",
        "\n",
        "        # compile topic names from 4 most relevant keywords\n",
        "        topicnames = {_i: '_'.join([x for i,x in enumerate(_keywords) if i < 4]) for _i, _keywords in enumerate(topic_words)} | {-1: 'unknown'}\n",
        "        topicnames_reverse = {topicnames[key]: key for key in topicnames.keys()}\n",
        "\n",
        "        # compile keywords lists (including scores)\n",
        "        keywords_list = {topicnames[_i]: {_word_score[0]: _word_score[1] for _word_score in zip(topic_words[_i], word_scores[_i])} for _i in topicnames.keys() if _i>= 0}\n",
        "\n",
        "        # assign topic to log records\n",
        "        # if topic score is lower than 0.18 topic is considered 'unkown'\n",
        "        topic_relevance = topic_model.get_document_topic_relevance()\n",
        "        doc_topic_id = topic_relevance.argmax(axis=1)\n",
        "        data['eval_topic'] = [doc_topic_id[_i] if topic_relevance[_i,doc_topic_id[_i]] >= 0.18 else -1 for _i in range(len(doc_topic_id))]\n",
        "\n",
        "        # final message\n",
        "        print(f\"{str(len(topicnames)-1)} topics detected.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'at least one array to concatenate' in str(e):\n",
        "            raise Exception('Not enough documents to find more than 1 topic.')\n",
        "        else:\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dcd6238-a47c-4ed3-9657-63fa3d9152c4"
      },
      "source": [
        "### BERTopic\n",
        "BERTopic creates clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Find information on BERTopic at https://maartengr.github.io/BERTopic/index.html. The below cell detects stopwords, generates keywords (without stopwords) & topic names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01d1d252-fd14-4076-a154-b37e0a4d9106"
      },
      "outputs": [],
      "source": [
        "if not topic_modeling_method == topic_modeling_methods_supported[2]:\n",
        "    print(f\"Skipped ... Topic modeling method is '{topic_modeling_method}'.\")\n",
        "else:\n",
        "    keywords_list, topicnames = dict(), dict()\n",
        "    try:\n",
        "        !pip install --upgrade bertopic | tail -n 1\n",
        "\n",
        "        from bertopic import BERTopic\n",
        "\n",
        "        docs = data[evaluation_field].astype(str).values.tolist()\n",
        "        if len(docs) == 0:\n",
        "            raise Exception('Data is very low!')\n",
        "        elif len(docs) < 1000:\n",
        "            topic_model = BERTopic()\n",
        "        else:\n",
        "            topic_model = BERTopic(nr_topics='auto')\n",
        "\n",
        "        topic_model.fit(docs)\n",
        "        topics, probs = topic_model.fit_transform(docs)\n",
        "\n",
        "        topic_count = len(topic_model.get_topic_info())\n",
        "\n",
        "        if topic_count < 1 or (topic_count == 1 and topic_model.get_topic(-1)):\n",
        "            raise Exception(\"Topic modeling failed: No topic found.\")\n",
        "        if not topic_model.get_topic(-1):\n",
        "            print(f\"Warning: Detected topics might be poor.\")\n",
        "        else:\n",
        "            print(f\"{str(topic_count-1)} topics detected.\")\n",
        "\n",
        "        # Detected worlds that are not relevant for topic separation (\"stopwords\")\n",
        "        stopwords = [w[0] for w in topic_model.get_topic(-1)] if topic_model.get_topic(-1) else []\n",
        "\n",
        "        # Generate list of keywords (without stopwords) and frequencies\n",
        "        keywords_list = {\"_\".join([y for i,y in enumerate([x for x in r[1] if x not in stopwords]) if i < 4]): {w[0]: w[1] for w in topic_model.get_topic(r[0]) if w[0] not in stopwords} \\\n",
        "                         for r in topic_model.get_topic_info().loc[:, ['Topic','Representation']].values if r[0] >= 0}\n",
        "\n",
        "        # Find duplicate topics\n",
        "        topic_tuples = [(num, k) for num, k in enumerate(keywords_list.keys())]\n",
        "        topic_tuples.sort(key = lambda t: t[1])\n",
        "        prev_keyword = '#'\n",
        "        prev_keyword_num = -1\n",
        "        duplicate_map = {}\n",
        "        for _t in topic_tuples:\n",
        "            if not _t[1] == prev_keyword:\n",
        "                prev_keyword = _t[1]\n",
        "                prev_keyword_num = _t[0]\n",
        "            else:\n",
        "                duplicate_map[_t[0]] = prev_keyword_num\n",
        "                print(f\"Duplicate topic pruned: {prev_keyword}\") \n",
        "\n",
        "        # Remove duplicate topics\n",
        "        keywords_list = {k: keywords_list[k] for num, k in enumerate(keywords_list.keys()) if not num in duplicate_map.keys()}\n",
        "        topics = [t if not t in duplicate_map else duplicate_map[t] for t in topics]\n",
        "\n",
        "        # Generate topic name from top 4 keywords\n",
        "        topicnames = {num: k for num, k in enumerate(keywords_list.keys())} | {-1: 'unknown'}\n",
        "\n",
        "        # Append topic assignment to data frame\n",
        "        data['eval_topic'] = topics\n",
        "\n",
        "    except TypeError as e:\n",
        "        print('BERTopic Error: Data is low')\n",
        "        \n",
        "    except Exception as e:\n",
        "        print('Error:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d85348b-644f-4c97-8eb8-bd679346c17d"
      },
      "source": [
        "## Topic Labels\n",
        "Use GenAI to generate topic labels from keywords and questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65fef74e-144a-451f-ba29-7c74df8391e6"
      },
      "outputs": [],
      "source": [
        "# for each topic, collect all questions assigned to it\n",
        "df1 = data[['eval_topic', 'question']]\n",
        "df_grouped = df1.groupby('eval_topic', as_index=False).agg({'question': list})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbddb5da-9d72-49d3-81ff-d6c220265f42"
      },
      "outputs": [],
      "source": [
        "# get prompt template manager instance\n",
        "prompt_template_manager = PromptTemplateManager(credentials=wml_credentials, project_id=project_id)\n",
        "\n",
        "# get prompt asset for topic labelling\n",
        "df_p = prompt_template_manager.list()\n",
        "prompt_id = prompt_id = df_p[df_p['NAME'].str.contains(\"topic_labeling\", na=False, case=False)]['ID'].iloc[0]\n",
        "prompt = prompt_template_manager.load_prompt(prompt_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77b531c2-fbbc-4e13-ba9e-e37137bc8ecf"
      },
      "outputs": [],
      "source": [
        "topics = df_grouped.to_dict(orient='records')\n",
        "model = ModelInference(\n",
        "    model_id=prompt.model_id,\n",
        "    params=prompt.model_params,\n",
        "    credentials=wml_credentials,\n",
        "    project_id=project_id\n",
        ")\n",
        "topiclabels = {}\n",
        "for _topic in topics:\n",
        "    if _topic['eval_topic'] == -1:\n",
        "        label = 'Others'\n",
        "    else:\n",
        "        variables = {\n",
        "            'questions': '\\n'.join(_topic['question']),\n",
        "            'keywords': '\"' + '\"\\n\"'.join(keywords_list[topicnames[_topic['eval_topic']]].keys()) + '\"'\n",
        "        }\n",
        "        input_text = prompt_template_manager.load_prompt(prompt_id, PromptTemplateFormats.STRING, prompt_variables=variables)\n",
        "        generated_text = model.generate(prompt=input_text)['results'][0]['generated_text']\n",
        "        matches = re.search('Label\\s*:\\s*([^\\\\n]*)', generated_text, flags=re.IGNORECASE)\n",
        "        if matches == None or not matches.group(0):\n",
        "            matches = re.search('\\s*([^\\\\n]*)', generated_text, flags=re.IGNORECASE)\n",
        "        try:\n",
        "            label = (matches.group(1) if matches.group(1) else matches.group(0)).strip('[](){}\" \\'')\n",
        "            if label == '' or label.startswith('\\n'):\n",
        "                label = topicnames[_topic['eval_topic']]\n",
        "        except:\n",
        "            print(f\"Warning: Label not found in '{generated_text}'\")\n",
        "            label = topicnames[_topic['eval_topic']]\n",
        "    topiclabels[_topic['eval_topic']] = label.strip()\n",
        "    print(f\"Topic {_topic['eval_topic']:d}: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1bbde8f-7b92-44d6-9245-e438abe11038"
      },
      "source": [
        "## Topic Keywords\n",
        "For top 10 topics, plot keywords as wordcloud to visualize topic subject. Use this visualization to perceive the essence of the detected topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5f07bfb-5f60-46fb-97ae-4a04ec38de7d"
      },
      "outputs": [],
      "source": [
        "def plot_wordcloud_top10_topics(labels, keywords_list):\n",
        "    colors = [color for name, color in TABLEAU_COLORS.items()]\n",
        "    cloud = WordCloud(background_color='white', width=400, height=400, max_words=8, color_func=lambda *args, **kwargs: colors[i], prefer_horizontal=1.0)\n",
        "    count = min(10,len(keywords_list))\n",
        "    if count < 1:\n",
        "        return\n",
        "    \n",
        "    # create and fill 2 x 5 grid of subplots with 10 word clouds\n",
        "    figure, axes = plt.subplots(nrows=2, ncols=5, figsize=(25,10), dpi=100, sharex=True, sharey=True)\n",
        "    for i, topic in enumerate(zip(labels, keywords_list.keys())):\n",
        "        if i >= count:\n",
        "            break\n",
        "        figure.add_subplot(axes[divmod(i,5)])\n",
        "        topic_words = keywords_list[topic[1]]\n",
        "        cloud.generate_from_frequencies(topic_words, max_font_size=72)\n",
        "        plt.gca().imshow(cloud)\n",
        "        plt.gca().set_title(f\"{str(topic[0])}: {labels[topic[0]]}\", fontdict=dict(size=16))\n",
        "        \n",
        "    # in case there are less than 10 topics, remove redundant subplots\n",
        "    for i in range(count,10):\n",
        "        axes[divmod(i,5)].remove()\n",
        "\n",
        "    # adjust margins, remove ticks and show plot\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    plt.margins(x=0, y=0)\n",
        "    plt.tight_layout()\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "836166d2-9193-4ea4-a36c-b175bb7d1475"
      },
      "outputs": [],
      "source": [
        "plot_wordcloud_top10_topics({_id: _label if len(_label) < 33 else _label[:30]+'...' for _id, _label in topiclabels.items() if _id>=0}, keywords_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6e88ecf-651f-4315-9751-1080bb0a286f"
      },
      "source": [
        "## Score by Topic\n",
        "Find correlation between rating and topic.<br>\n",
        "Poor rating on a topic indicates that content on that topic is bad or missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc861ee1-e9aa-4263-bc23-2f666f0e5256"
      },
      "outputs": [],
      "source": [
        "def displayName(title):\n",
        "    return ' '.join([_t[0].upper() + (_t[1:] if len(_t) > 1 else '') for _t in title.replace('eval_','').replace('_', ' ').strip().split()]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac0ae104-29be-4048-805d-d092960ae77d"
      },
      "outputs": [],
      "source": [
        "def correlation_matrix(eval_column, row_order=None, index_mapper=None, totals=True):\n",
        "    name = eval_column.replace('eval_','').replace('_', ' ')\n",
        "    if len(data)>0:\n",
        "        correlation_df = data.groupby([eval_column, 'eval_rating']).size().unstack(fill_value=0).reindex(index=row_order, columns=supported_ratings)\n",
        "        if index_mapper:\n",
        "             correlation_df.index = correlation_df.index.map(index_mapper)\n",
        "        if totals:\n",
        "            correlation_df.loc['Total'] = correlation_df.sum(numeric_only=True)\n",
        "\n",
        "        title = f\"Rating by {displayName(name)}\"\n",
        "        display(correlation_df)\n",
        "        correlation_df.plot.bar(rot=90, stacked=True, color=rating_color, title=title, legend=True, xlabel='')\n",
        "\n",
        "        title = f\"\\nRating by {displayName(name)} [Percentages]\"\n",
        "        correlation_df_percentage = correlation_df.div(correlation_df.sum(axis=1), axis=0).mul(100).round(1) \n",
        "        print(title)\n",
        "        display(correlation_df_percentage)\n",
        "        correlation_df_percentage.plot.bar(rot=90, stacked=True, color=rating_color, title=title, legend=False, xlabel='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4583170-a08e-437d-bb23-133be270fcf3"
      },
      "outputs": [],
      "source": [
        "def score(eval_column, value_column='eval_rating_percent', rename=None, reindex=None):\n",
        "    if len(data)>0:\n",
        "        score_df = data[[eval_column, value_column]].groupby([eval_column]).mean()\n",
        "        if rename:\n",
        "            score_df = score_df.rename(index=rename)\n",
        "        if reindex:\n",
        "            score_df = score_df.reindex(reindex)\n",
        "        score_df.plot.barh(title=f\"{displayName(value_column)} by {displayName(eval_column)}\", legend=False, xlabel='', ylabel='') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19dd01a7-a711-4fbf-9ddc-9481e1446f01"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    correlation_matrix('eval_topic', None, topiclabels, False)\n",
        "except Exception as e:\n",
        "    raise Exception('Error: Topics not generated, may be due to insufficient logs. Check topic modelling', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad347a28-7ecf-41ca-b414-1c56656a1b36"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    score('eval_topic', rename=topiclabels)\n",
        "except Exception as e:\n",
        "    raise Exception('Error: Topics not generated, may be due to insufficient logs. Check topic modelling', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8f5fd6c-a83a-491b-9075-509190a623e4"
      },
      "source": [
        "## Score by Response Length\n",
        "Find correlation between rating and response length.<br>\n",
        "Based on the result, model parameters can be adjusted to generate more or less tokens, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dba9d58-caa2-4427-84ad-c3b31b4e00fc"
      },
      "outputs": [],
      "source": [
        "# Number of bins for histogram\n",
        "number_of_bins = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a66b4669-f595-462e-9eeb-65909a67c5ad"
      },
      "outputs": [],
      "source": [
        "# Count words in response\n",
        "data['eval_words'] = data['response'].map(lambda txt: len(txt.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d517f81f-bc8e-45b9-8fc2-1804f6e7e6fc"
      },
      "outputs": [],
      "source": [
        "# Calculate bin ranges and labels\n",
        "hist, bins_raw = np.histogram(data['eval_words'], number_of_bins)\n",
        "bins = [int(b+0.5) for b in bins_raw]\n",
        "bins_labels = ['<'+str(bins[1])] + [str(bins[i])+'-'+str(bins[i+1]-1) for i in range(1,len(bins)-2)] + ['>='+str(bins[len(bins)-2])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c824740e-fae7-4309-821a-f84ee3fbaeeb"
      },
      "outputs": [],
      "source": [
        "data['eval_words_bins'] = data['eval_words'].map(lambda x: bins_labels[len([v for v in bins[:-2] if v < x])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "171ff527-56c8-4243-8766-b6ccdb4fd8ec"
      },
      "outputs": [],
      "source": [
        "_ = plt.hist(data['eval_words'], bins=bins)\n",
        "plt.title(\"Response Length [Words]\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b29ee656-3dee-441d-92c2-70173a4c4524"
      },
      "outputs": [],
      "source": [
        "df = correlation_matrix('eval_words_bins', bins_labels, None, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "246c6366-47f6-4966-94b3-c14c1ec92b9b"
      },
      "outputs": [],
      "source": [
        "score('eval_words_bins', reindex=bins_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb2ac87e-fd40-4a8d-80a5-9bf98fb8fb54"
      },
      "source": [
        "## Document Search Score by Topic\n",
        "List document search score by topic. The document search score is the maximum search score over all source documents in a log record. Search score by topic is the average document search score over all queries or responses on that topic.<br>\n",
        "A poor score indicates insufficient coverage of the corresponding topic within the content database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63be4ea2-64cd-4319-a370-dd64b54019da"
      },
      "outputs": [],
      "source": [
        "data['source_documents'] = data['source_documents'].apply(lambda x: x if len(x)>=1 else np.nan)\n",
        "\n",
        "if 'source_documents' in data:\n",
        "    data['eval_search_score'] = data['source_documents'].map(lambda _scores: max([_s['score'] for _s in _scores]), na_action='ignore')\n",
        "else:\n",
        "    print(f\"Document scores not available!\")\n",
        "    data['eval_search_score'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ae57843-1045-4e78-ae23-6faaa073ee42"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    score('eval_topic', value_column='eval_search_score', rename=topiclabels)\n",
        "except Exception as e:\n",
        "    raise Exception('Error: Topics not generated, may be due to insufficient logs. Check topic modelling', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a48ad4e-fc41-4cb7-ad6e-23f6666409e2"
      },
      "source": [
        "## Feedback on Answer by Topic\n",
        "Comments that users have provided for the worst rated answers. These might give insights on document quality and completeness as well as on the user's expectations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5a8cff7-33e5-48d4-9d7e-d5e02f6c8720"
      },
      "outputs": [],
      "source": [
        "# number of comments per topic to be displayed\n",
        "number_of_comments_per_topic = 5\n",
        "\n",
        "# only comments with rating value below this threshold are displayed\n",
        "rating_value_threshold = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "886b0814-aa8d-4b0e-aa55-513e3cfd2e95"
      },
      "outputs": [],
      "source": [
        "html = '<table><tr><th style=\\\"vertical-align: top;text-align: left;\\\">Topic</th><th style=\\\"vertical-align: top;text-align: left;\\\">Comment</th><th style=\\\"vertical-align: top;text-align: left;\\\">Question</th></tr>'\n",
        "for _topic_no in topiclabels.keys():\n",
        "    feedback_raw = data[data['eval_topic'] == _topic_no][['question', 'feedback', 'eval_rating_percent']]\n",
        "    feedback_dat = pd.DataFrame(data={'question': feedback_raw['question'], 'comment': feedback_raw['feedback'].str['comment'], 'value': feedback_raw['eval_rating_percent']})\n",
        "    feedback_df  = feedback_dat[((feedback_dat['value'] < rating_value_threshold) & (feedback_dat['value'] >= 0)) & (feedback_dat['comment'].values != None) & (feedback_dat['comment'].values != '')].sort_values(by=['value'])\n",
        "    comment_count = min(feedback_df.shape[0], number_of_comments_per_topic)\n",
        "    html = html + f\"<tr style=\\\"border-top: 1px solid;\\\"><td style=\\\"vertical-align: top;text-align: left;\\\" rowspan=\\\"{comment_count:1d}\\\">{topiclabels[_topic_no]}</td>\" + \\\n",
        "                \"<tr>\".join(['<td style=\\\"vertical-align: top;text-align: left;\\\">'+_row[0]+'</td><td style=\\\"vertical-align: top;text-align: left;\\\">'+_row[1]+'</td></tr>' for _row in feedback_df[:comment_count][['comment', 'question']].values])\n",
        "html = html + '</table>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3262ea21-2d94-458b-a427-a29bb3922539"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note** It's recommended to close the datastax session once your steps are ran for this notebook for optimal performance. once you execute this cell existing datastax connections are closed. if have to re run above code cells you have to create new connection for datastax by re running cells from `Connect to Vector Database`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43f96372-1864-4203-baf0-788cf95f008d"
      },
      "outputs": [],
      "source": [
        "if log_connection_type==\"datastax\" and environment != \"cloud\":\n",
        "    if not datastax_session.is_shutdown:\n",
        "        datastax_session.shutdown()\n",
        "        print(f\"datastax_session got shutdown : {datastax_session.is_shutdown}\")\n",
        "    if not datastax_cluster.is_shutdown:\n",
        "        datastax_cluster.shutdown()\n",
        "        print(f\"datastax_cluster got shutdown : {datastax_cluster.is_shutdown}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e8ab3d-38fa-41df-8e33-060cf8288eaa"
      },
      "source": [
        "**Sample Materials, provided under license.</a> <br>\n",
        "Licensed Materials - Property of IBM. <br>\n",
        " Copyright IBM Corp. 2024, 2025. All Rights Reserved. <br>\n",
        "US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. <br>**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
