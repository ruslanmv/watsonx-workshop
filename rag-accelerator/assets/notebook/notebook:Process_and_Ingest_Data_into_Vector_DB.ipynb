{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2c942354ffc4b7b97ba3c5039dcfce1"
      },
      "source": [
        "# Data Processing and Ingestion\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook handles various input data types, transforming them into a document format, and then incorporates them into a vector database for subsequent processing. The indexed documents are further used in the next notebook to create an AI service function and deploy it on [watsonx.ai](https://www.ibm.com/products/watsonx-ai) . \n",
        "\n",
        "The initial section of the notebook focuses onfocuses on handling any type of data contained within a file, directory or a zip archive. The process involves extracting content from different file types, segmenting the data, converting it into a document format, and finally indexing the content into a vector database. \n",
        "\n",
        "The accelerator currently supports **Elasticsearch, Milvus and Datastax** vector databases.\n",
        "\n",
        "The ingestion process uses vector embeddings to enhance data storage and retrieval within either Elasticsearch or Milvus or Datastax vector databases, ensuring both efficiency and effectiveness. \n",
        "\n",
        "- Establishing a connection to the chosen vector database (Elasticsearch or Milvus or Datastax) and loading input data from processed documents.\n",
        "- Generating unique IDs for documents and splitting them into manageable chunks for indexing in the vector database.\n",
        "- Inserting the documents with embeddings in batches into Elasticsearch or Milvus or Datastax using these generated IDs, with progress monitoring provided by a progress bar.\n",
        "- When using a Milvus connection, the code sets up a Milvus vector store with dense or hybrid (dense + sparse) embeddings based on the search type. \n",
        "- When using a Datastax connection, the code sets up a Datastax vector store with dense embeddings. \n",
        "\n",
        "**Note**: \n",
        "- It is recommended to run this notebook in a Python environment on watsonx.ai software with a GPU-enabled or high vCPU and RAM hardware configuration, as generating embeddings may require significant memory.\n",
        "- Datastax is not supported in this cloud version.\n",
        "\n",
        "## Contents\n",
        "\n",
        "This notebook contains the following parts:\n",
        "- [Setup](#setup)\n",
        "- [Import Dependencies](#import)\n",
        "- [Extract Data from Input files](#input)\n",
        "- [Process and Split Extracted Data](#split)\n",
        "- [Connect to Vector Database](#connect)\n",
        "- [Index Documents using langchains vectorstore](#insert_documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e09609f5baf4507a57aca85bd321ff9"
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "### Pre-Requisite Libraries and Dependencies\n",
        "Below cell downloads and installs specific mandatory libraries and dependencies required to run this notebook.\n",
        "\n",
        "**Note** : Some of the versions of the libraries may throw warnings after installation. These library versions are crucial for execution of the accelerator. Please ignore the warning/error and proceed with your execution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc36dbb7f1a446988816cff0db8aa113"
      },
      "outputs": [],
      "source": [
        "!pip install elasticsearch==8.18.1 | tail -n 1\n",
        "!pip install langchain_community | tail -n 1\n",
        "!pip install unstructured==0.17.2 | tail -n 1\n",
        "!pip install langchain | tail -n 1\n",
        "!pip install ibm_watsonx_ai==1.3.26| tail -n 1\n",
        "!pip install langchain_elasticsearch==0.3.2 | tail -n 1\n",
        "!pip install langchain_milvus==0.2.0 | tail -n 1\n",
        "!pip install pymilvus==2.5.11 | tail -n 1\n",
        "!pip install tiktoken | tail -n 1\n",
        "!pip install pypdf | tail -n 1\n",
        "!pip install cassio==0.1.10 | tail -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90d5355-66ce-44d7-b6b3-17e29d51c30e"
      },
      "source": [
        "Restart the kernel after performing the pip install if the below cell fails to import all the libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe434599-967f-4f87-98e0-00d84e50a966"
      },
      "outputs": [],
      "source": [
        "from elasticsearch import Elasticsearch, helpers\n",
        "from ibm_watsonx_ai import Credentials\n",
        "from ibm_watsonx_ai.foundation_models import Embeddings\n",
        "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
        "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai import APIClient\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import DirectoryLoader,PyPDFLoader,UnstructuredHTMLLoader\n",
        "\n",
        "import hashlib\n",
        "from bs4 import BeautifulSoup\n",
        "import multiprocessing\n",
        "import json\n",
        "import os\n",
        "import asyncio\n",
        "import re\n",
        "import zipfile\n",
        "from threading import Thread\n",
        "import shutil \n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from pymilvus import(IndexType,Status,connections,FieldSchema,DataType,Collection,CollectionSchema,utility)\n",
        "from langchain_milvus import Milvus, BM25BuiltInFunction\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a95f6b5b-427e-4a1b-93c0-24b98e6f513d"
      },
      "outputs": [],
      "source": [
        "project_id=os.environ['PROJECT_ID']\n",
        "# Environment and host url\n",
        "hostname = os.environ['RUNTIME_ENV_APSX_URL']\n",
        "\n",
        "if hostname.endswith(\"cloud.ibm.com\") == True:\n",
        "    environment = \"cloud\"\n",
        "    project_id = os.environ['PROJECT_ID']\n",
        "    runtime_region = os.environ[\"RUNTIME_ENV_REGION\"] \n",
        "else:\n",
        "    environment = \"on-prem\"\n",
        "    from ibm_watson_studio_lib import access_project_or_space\n",
        "    wslib = access_project_or_space()   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8cc005-454e-4646-901c-4cb3825d9bc5"
      },
      "source": [
        "<a id=\"import\"></a>\n",
        "### Import Parameter Sets, Credentials and Helper functions script.\n",
        "\n",
        "Below cells imports parameter sets values, sets the watsonx.ai credentials and imports the helper functions script. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89320d03-8254-4195-b127-cd5503e636bd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    filename = 'rag_helper_functions.py'\n",
        "    wslib.download_file(filename)\n",
        "    import rag_helper_functions\n",
        "    print(\"rag_helper_functions imported from the project assets\")\n",
        "except NameError as e:\n",
        "    print(str(e))\n",
        "    print(\"If running watsonx.ai aaS on IBM Cloud, check that the first cell in the notebook contains a project token. If not, select the vertical ellipsis button from the notebook toolbar and `insert project token`. Also check that you have specified your ibm_api_key in the second code cell of the notebook\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7937a1e9-2cd6-4e8d-9260-633f8bed4cc3"
      },
      "outputs": [],
      "source": [
        "parameter_sets = [\"RAG_parameter_set\",\"RAG_advanced_parameter_set\"]\n",
        "\n",
        "parameters=rag_helper_functions.get_parameter_sets(wslib, parameter_sets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2cd9353-c2a8-454a-b6eb-687e18ed79d4"
      },
      "outputs": [],
      "source": [
        "ibm_api_key=parameters['watsonx_ai_api_key']\n",
        "if environment == \"cloud\":\n",
        "    WML_SERVICE_URL = f\"https://{runtime_region}.ml.cloud.ibm.com\"\n",
        "    wml_credentials = Credentials(api_key=parameters['watsonx_ai_api_key'], url=WML_SERVICE_URL)\n",
        "else:\n",
        "    token = os.environ['USER_ACCESS_TOKEN']\n",
        "    wml_credentials=Credentials(token=os.environ['USER_ACCESS_TOKEN'],url=hostname,instance_id='openshift')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62a9b987-bf46-4e64-9cca-d6cdb8b7fc7d"
      },
      "source": [
        "### Set Watsonx.ai client\n",
        "Below cell uses the watson machine learning credentials to create an API client to interact with the project and deployment space. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72cd218c865b4e70a6da8f0406ed4398"
      },
      "outputs": [],
      "source": [
        "client = APIClient(wml_credentials)\n",
        "client.set.default_project(project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68f23d1cf1ed4357aa0557727b9e12e1"
      },
      "source": [
        "<a id=\"input\"></a>\n",
        "\n",
        "### Input File for extracting, loading, processing and indexing into a vector database. \n",
        "\n",
        "This can be updated in the **RAG parameter set** as required. Here are some of the options below on the formats that can be used:\n",
        "- The name of a zip file containing **.pdf / .docx / .pptx / .html/ .md/ .txt** files that exists in the project as a data asset. e.g. `ibm-docs-SSYOK8.zip`\n",
        "- The name of a **.pdf / .docx / .pptx / .html / .md/ .txt** file that exists in the project as a data asset. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd04febe735543118cc685b12e2d9b4f"
      },
      "outputs": [],
      "source": [
        "input_filename = parameters['ingestion_data_file']   \n",
        "    \n",
        "if any(input_filename.lower().endswith(ext) for ext in ['.zip', '.pdf', '.docx', '.pptx', '.html',\".md\", \".txt\"]):\n",
        "    wslib.download_file(input_filename)\n",
        "else:\n",
        "    raise ValueError(\"Input document data asset should have an extension (.zip/.pdf/.docx/.pptx/.html/.md/.txt) only!\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfa5d77cbdf74ba8bbe434fd8fdea290"
      },
      "source": [
        "if the format of the file is `.zip` then below cell extracts the files in the zip into a directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e59075a113784211bab7e22b36ed94f4"
      },
      "outputs": [],
      "source": [
        "directory =\"watson-docs\"\n",
        "\n",
        "if os.path.exists(directory):\n",
        "    shutil.rmtree(directory)\n",
        "os.makedirs(directory)\n",
        "if \".zip\" in input_filename:\n",
        "    with zipfile.ZipFile(input_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(path=directory)\n",
        "\n",
        "    print(\"Extraction completed!\")\n",
        "else:\n",
        "    shutil.move(input_filename, directory)\n",
        "    print(\"File copied\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0a3b078433244b3b41c0ba405a84043"
      },
      "source": [
        "<a id=\"split\"></a>\n",
        "### Load all the Documents \n",
        "\n",
        "The following cell generates a `DirectoryLoader` instance from the langchain library, configured to read files with extensions such as `.html, .pdf, .pptx, .md, .txt and .docx.` The loader traverses the specified folder, gathers all the documents, and appends them to a list. \n",
        "\n",
        "In the subsequent cell, these documents are processed to be inserted into a vector database. This involves splitting the documents using langchain's `RecursiveCharacterTextSplitter` and incorporating both content and metadata into the documents. The term \"recursive\" suggests that this division process happens in multiple stages or levels, breaking down the text into increasingly smaller segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e49ad90ad46d455da0e3e6e83583435c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "loaders = [\n",
        "    DirectoryLoader(directory, glob=\"**/*.html\",show_progress=True),\n",
        "    DirectoryLoader(directory, glob=\"**/*.pdf\",show_progress=True, loader_cls=PyPDFLoader),\n",
        "    DirectoryLoader(directory, glob=\"**/*.pptx\",show_progress=True),\n",
        "    DirectoryLoader(directory, glob=\"**/*.docx\",show_progress=True),\n",
        "    DirectoryLoader(directory, glob=\"**/*.md\",show_progress=True),\n",
        "    DirectoryLoader(directory, glob=\"**/*.txt\",show_progress=True)    \n",
        "]\n",
        "\n",
        "\n",
        "documents=[]\n",
        "for loader in loaders:\n",
        "    data =loader.load()\n",
        "    documents.extend(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83e3307d7e6b46cb8408550ee4b3fbb6"
      },
      "source": [
        "The cell below prepares documents for insertion into a vector database. It handles HTML files where document URLs are stored within the `canonical` tag. The cell reads the `canonical` tag and incorporates it into the HTML files metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "597bc5e52d4c4a678fcd061681c0e3e1"
      },
      "outputs": [],
      "source": [
        "def get_split_documents(documents):\n",
        "    content=[]\n",
        "    metadata = []\n",
        "    for doc in documents:\n",
        "        \n",
        "        document_url = \"\"\n",
        "        document_title = doc.metadata[\"title\"] if \"title\" in doc.metadata else doc.metadata['source'].split(\"/\")[-1].split(\".\")[0]\n",
        "    \n",
        "        if \".html\" in doc.metadata['source']:\n",
        "            with open(doc.metadata['source'], 'r', encoding='utf-8') as html_file:\n",
        "                html_content = html_file.read()\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "            canonical_tag = soup.find('link', {'rel': 'canonical'})\n",
        "            title_tag = soup.find('title')\n",
        "    \n",
        "            if canonical_tag:\n",
        "                document_url = canonical_tag.get('href')\n",
        "            \n",
        "            if title_tag:\n",
        "                document_title=title_tag.get_text()\n",
        "        \n",
        "        metadata.append({\n",
        "                \"title\":document_title ,\n",
        "                \"source\": doc.metadata['source'],\n",
        "                \"document_url\":document_url,\n",
        "                \"page_number\":str(doc.metadata['page']) if 'page' in doc.metadata else ''\n",
        "                \n",
        "            })\n",
        "        \n",
        "        content.append(doc.page_content)\n",
        "    \n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=parameters['ingestion_chunk_size'],\n",
        "        chunk_overlap=parameters['ingestion_chunk_overlap'],\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    \n",
        "    split_documents = text_splitter.create_documents(content, metadatas=metadata)\n",
        "    print(f\"{len(documents)} Documents are split into {len(split_documents)} documents with a chunk size\",parameters[\"ingestion_chunk_size\"])\n",
        "    \n",
        "    \n",
        "    \n",
        "    for chunk in split_documents:\n",
        "        chunk.metadata[\"title\"] = chunk.metadata.get(\"title\", \"Unknown Title\")\n",
        "        chunk.page_content = f\"Document Title: {chunk.metadata['title']}\\n Document Content: {chunk.page_content}\"\n",
        "    \n",
        "    split_docs = rag_helper_functions.remove_duplicate_records(split_documents)\n",
        "    print(f'After de-duplication, there are {len(split_docs)} documents present')\n",
        "\n",
        "    return split_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c819554d-992c-4275-ac9b-29e89e6b16ee"
      },
      "outputs": [],
      "source": [
        "split_docs = get_split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c3a18cdf0c34d0999ee68bf6577bcd9"
      },
      "source": [
        "<a id=\"connect\"></a>\n",
        "### Connecting to a vector database\n",
        "\n",
        "#### Connecting using Project Connection Asset (default)\n",
        "\n",
        "The notebook, by default, will look for a connection asset in the project named `milvus_connect` or `elasticsearch_connect` or `datastax_connect`.  You can set this up by following the instructions in the project readme. \n",
        "This code checks if a specified connection exists in the project. If found, it retrieves the connection details and identifies the connection type. Depending on the connection type, it establishes a connection to the appropriate database. If the connection is not found, it raises an error indicating the absence of the specified connection in the project.\n",
        "\n",
        "Additionally, \n",
        "If the connection type is **Elastic Search**, the below cell gets the status of the current Elasticsearch model (e.g. ELSER 2). For this ensure that you have the model downloaded and deployed on Elasticsearch. This can be done under **Machine Learning >> Trained Models** section on Elasticsearch.\n",
        "\n",
        "If the connection type is **DataStax**, the below cell gets the keyspace if not exists, it will create the keyspace with SimpleStrategy with single datacenter. For `Production` usecases, it is **recommended** to create your own `keyspace strategy` based on your network configuration which may need to apply custom changes in `datastax_ks_replication`. Also update cluster connection pooling as per official [doc](https://docs.datastax.com/en/datastax-drivers/connecting/connection-pool.html) for further customisations on its usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e14441e06d140ff87b3b687c80d606c"
      },
      "outputs": [],
      "source": [
        "connection_name=parameters[\"connection_asset\"]\n",
        "if(next((conn for conn in wslib.list_connections() if conn['name'] == connection_name), None)):\n",
        "    print(connection_name, \"Connection found in the project\")\n",
        "    db_connection = wslib.get_connection(connection_name)\n",
        "    \n",
        "    connection_datatypesource_id=client.connections.get_details(db_connection['.']['asset_id'])['entity']['datasource_type']\n",
        "    connection_type = client.connections.get_datasource_type_details_by_id(connection_datatypesource_id)['entity']['name']\n",
        "    \n",
        "    print(\"Successfully retrieved the connection details\")\n",
        "    print(\"Connection type is identified as:\",connection_type)\n",
        "\n",
        "    if connection_type==\"elasticsearch\":\n",
        "        es_client=rag_helper_functions.create_and_check_elastic_client(db_connection, parameters['elastic_search_model_id'])\n",
        "    elif connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "        milvus_credentials = rag_helper_functions.connect_to_milvus_database(db_connection, parameters)\n",
        "    elif connection_type==\"datastax\":\n",
        "        if environment == \"cloud\":\n",
        "            raise ValueError(f\"ERROR! we don't support officially datastax for Cloud as of now\")\n",
        "        datastax_session,datastax_cluster = rag_helper_functions.connect_to_datastax(db_connection, parameters)\n",
        "        try:\n",
        "            if datastax_session==None:\n",
        "                raise ValueError(f\"Failure in connecting named {connection_name} found in the project. Please check above\")\n",
        "            else:\n",
        "                if 'keyspace' not in db_connection:\n",
        "                    raise ValueError(f\"Failure in connecting named {connection_name} found in the project. Please add keyspace which is missing!\")\n",
        "                import cassio\n",
        "                check_datastax_ks_exists = rag_helper_functions.check_datastax_ks_exists(db_connection, datastax_session, client, parameters)\n",
        "                # Initialize cassio with the session and keyspace\n",
        "                cassio.init(session=datastax_session, keyspace=db_connection.get('keyspace'))\n",
        "        \n",
        "                if not check_datastax_ks_exists:\n",
        "                    # It is recommended to create your Keyspace by checking with Datastax Cluster administrator if you don't have already keyspace & use.\n",
        "                    raise ValueError(f\"ERROR: keyspace not found, Please create keyspace by checking with your administrator\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed in Datastax Connection {e}\")\n",
        "else:\n",
        "    db_connection=\"\"\n",
        "    raise ValueError(f\"No connection named {connection_name} found in the project.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6601de45-882e-4a7a-b765-5258e2c017e6"
      },
      "source": [
        "<a id=\"insert_documents\"></a>\n",
        "### Inserting Documents using Langchains Vector Store "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b92bba6-3690-493c-9974-3c5d452de674"
      },
      "source": [
        "Below code utilizes Langchains vector store extension to store documents. <br>\n",
        "The code sets up a vector store based on the specified connection type, either **\"elasticsearch\" or \"milvus\"**. <br>\n",
        "\n",
        "- If `connection_type` is `\"elasticsearch\"`, it imports `ElasticsearchStore` from the `langchain_elasticsearch` library and initializes it with an Elasticsearch client and specified parameters with the given model ID. The `Elastic Search`vector store is then created using the `model_id`, connection parameters and index settings <br>\n",
        "\n",
        "- If `connection_type` is `\"milvus\"`, the code imports `langchain_milvus` and configures credentials using an API key and service URL. It initializes the embedding model specified in the parameters to generate embeddings, and sets up index parameters with specific metrics and configurations. The `Milvus` vector store is then created using the embedding function, connection details, and index settings. For both cloud & SW environments, it uses the `ibm_watsonx_ai` library to initialize the embedding function with the required API key, URL, and project ID. On Cloud Pak for Data software, required encoded models needs to deployed which is mentioned in parameters[\"embedding_model_id\"] if they are not present on the cluster. Once loaded, those models are used to generate embeddings for the chunked documents. It initializes a **Milvus** vector store with either **dense embeddings** or **hybrid search** (dense + BM25 sparse embeddings) based on the `milvus_hybrid_search` parameter. If hybrid search is enabled, it creates a new collection with **IVF_FLAT for dense** and **BM25 for sparse** indexing. Otherwise, it only adds **dense embeddings**. \n",
        "\n",
        "* If `connection_type` is `\"datastax`\", it uses `langchain_community.vectorstores.Cassandra` to initialize a **Cassandra-based vector store**. The embedding model is initialized similarly using `get_embedding()`, and the store is configured using a specified keyspace and table name. This setup supports vector search on a DataStax Astra DB or Cassandra cluster.\n",
        "\n",
        "Firstly, it creates a connection to the vector database and defines how documents will be retrieved later. <br>\n",
        "Then, it defines a function to add these documents to the vector store. This function takes the documents and additional parameters for efficient processing, such as splitting the documents into smaller chunks and setting a timeout for requests. <br>\n",
        "\n",
        "Overall, this code efficiently adds a list of documents to a vector store, thereby making them searchable. <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "323028c9-1d09-4f45-aa88-9eb5a5c40920"
      },
      "source": [
        "Below code creates the `Elasticsearch` dense vector index for indexing of documents if dense embedding model like `E5 multilingual` is used and also creates elasticsearch pipeline for indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b3c5283-4ac0-4088-9580-4677346fef8b"
      },
      "outputs": [],
      "source": [
        "def create_es_dense_index(index_name):\n",
        "    try:\n",
        "        es_client.options(ignore_status=400).indices.create(\n",
        "                    index=index_name,\n",
        "                    mappings={\n",
        "                        'properties': {\n",
        "                            'vector.tokens': {\n",
        "                                'type': 'dense_vector',\n",
        "                            },\n",
        "                        }\n",
        "                    },\n",
        "                    settings={\n",
        "                        'index': {\n",
        "                            'default_pipeline': 'dense-ingest-pipeline',\n",
        "                        },\n",
        "                        \"number_of_shards\": parameters[\"es_number_of_shards\"],\n",
        "                    }\n",
        "        )\n",
        "        es_client.ingest.put_pipeline(\n",
        "                    id='dense-ingest-pipeline',\n",
        "                    processors=[\n",
        "                        {\n",
        "                            'inference': {\n",
        "                                'model_id': parameters['elastic_search_model_id'],\n",
        "                                'input_output': [\n",
        "                                    {\n",
        "                                        'input_field': 'text',\n",
        "                                        'output_field': 'vector.tokens',\n",
        "                                    }\n",
        "                                ]\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                )\n",
        "        print(f'Elastic search index created with {parameters[\"elastic_search_model_id\"]}!')\n",
        "    except Exception as e:\n",
        "        print('Error creating elastic index', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aaf52f8-147a-4074-87e5-4ac9dbd83e52"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_embedding(environment, parameters, project_id, wml_credentials, WML_SERVICE_URL):\n",
        "    if environment == \"cloud\":\n",
        "        credentials = Credentials(\n",
        "            api_key=parameters['watsonx_ai_api_key'],\n",
        "            url=WML_SERVICE_URL\n",
        "        )\n",
        "        embedding = Embeddings(\n",
        "            model_id=parameters['embedding_model_id'],\n",
        "            credentials=credentials,\n",
        "            project_id=project_id,\n",
        "            verify=True\n",
        "        )\n",
        "    elif environment == \"on-prem\":\n",
        "        try:\n",
        "            if client.foundation_models.EmbeddingModels.__members__:\n",
        "                if client.foundation_models.EmbeddingModels(parameters[\"embedding_model_id\"]).name:\n",
        "                    embedding = Embeddings(\n",
        "                        model_id=parameters['embedding_model_id'],\n",
        "                        credentials=wml_credentials,\n",
        "                        project_id=project_id,\n",
        "                        verify=True\n",
        "                    )\n",
        "                else:\n",
        "                    print(f\"Encoder model {parameters['embedding_model_id']} not found on the cluster. Please update embedding model_id param to a model which exists or deploy missing model on this cluster\")\n",
        "            else:\n",
        "                print(\"Local on-prem embedding models not found, using models from IBM Cloud API if required parameters are provided\")\n",
        "                credentials = Credentials(\n",
        "                    api_key=parameters['watsonx_ai_api_key'],\n",
        "                    url=parameters['watsonx_ai_url']\n",
        "                )\n",
        "                embedding = Embeddings(\n",
        "                    model_id=parameters['embedding_model_id'],\n",
        "                    credentials=credentials,\n",
        "                    space_id=parameters[\"wx_ai_inference_space_id\"],\n",
        "                    verify=True\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"Exception in loading Embedding Models: {str(e)}\")\n",
        "            raise\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid environment: {environment}. Must be 'cloud' or 'on-prem'.\")\n",
        "    \n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fa7d919-f219-4602-bcbc-f68f2d06bf93"
      },
      "outputs": [],
      "source": [
        "def create_vector_store(connection_type,index_name,parameters):\n",
        "    if connection_type==\"elasticsearch\":\n",
        "        from langchain_elasticsearch import ElasticsearchStore\n",
        "        if 'dense' in parameters['elastic_search_vector_type']:\n",
        "            create_es_dense_index(index_name)\n",
        "            vector_store=ElasticsearchStore(\n",
        "                            es_connection=es_client,\n",
        "                            index_name=index_name,\n",
        "                            strategy=ElasticsearchStore.ApproxRetrievalStrategy(query_model_id=parameters['elastic_search_model_id']),\n",
        "                            )\n",
        "        else:\n",
        "            vector_store=ElasticsearchStore(\n",
        "                            es_connection=es_client,\n",
        "                            index_name=index_name,\n",
        "                            strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(model_id=parameters['elastic_search_model_id']),\n",
        "                            custom_index_settings={\"number_of_shards\": parameters[\"es_number_of_shards\"]}\n",
        "                            )\n",
        "        print(\"Elastic Search Vector Store Created with\",parameters['elastic_search_model_id'])\n",
        "    elif connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "        from langchain_milvus import Milvus\n",
        "        print(\"using the model\",parameters['embedding_model_id'], \"to create embeddings\")\n",
        "        embedding = get_embedding(environment, parameters, project_id, wml_credentials, WML_SERVICE_URL) if environment == \"cloud\" else get_embedding(environment, parameters, project_id, wml_credentials, None)  \n",
        "        #milvus_index_params = {\"index_type\": \"HNSW\",\"metric_type\": \"L2\",  \"params\": { \"M\": 16,\"efConstruction\": 200,\"efSearch\": 16 }}\n",
        "        dense_index_param = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\",\"params\": {\"nlist\": 1024},}\n",
        "        sparse_index_param = {\"metric_type\": \"BM25\",\"index_type\": \"SPARSE_INVERTED_INDEX\", \"params\": {\"drop_ratio_build\": 0.2}}\n",
        "\n",
        "        hybrid_search = True if parameters['milvus_hybrid_search'].lower()==\"true\" else False\n",
        "        if hybrid_search:\n",
        "            print(\"Adding sparse and Dense embeddings\")\n",
        "            vector_store = Milvus(\n",
        "            embedding_function=embedding,\n",
        "            builtin_function=BM25BuiltInFunction(output_field_names=\"sparse\"), \n",
        "            index_params=[dense_index_param, sparse_index_param],\n",
        "            vector_field=[\"dense\", \"sparse\"],\n",
        "            connection_args=milvus_credentials,\n",
        "            primary_field='id',\n",
        "            consistency_level=\"Strong\",\n",
        "            collection_name=index_name\n",
        "            )\n",
        "        else:\n",
        "            print(\"Adding Dense embeddings\")\n",
        "            vector_store = Milvus(\n",
        "                embedding_function=embedding,\n",
        "                index_params=dense_index_param,\n",
        "                connection_args=milvus_credentials,\n",
        "                primary_field='id',\n",
        "                consistency_level=\"Strong\",\n",
        "                collection_name=index_name\n",
        "            )\n",
        "        print(\"Milvus Vector Store Created\")\n",
        "\n",
        "    elif connection_type == \"datastax\":\n",
        "        if environment == \"cloud\":\n",
        "            raise ValueError(f\"ERROR! we don't support datastax connection for Cloud as of now\")\n",
        "        print(\"using the model\",parameters['embedding_model_id'], \"to create embeddings\")\n",
        "        embedding = get_embedding(environment, parameters, project_id, wml_credentials, WML_SERVICE_URL) if environment == \"cloud\" else get_embedding(environment, parameters, project_id, wml_credentials, None)  \n",
        "        from langchain_community.vectorstores import Cassandra\n",
        "        vector_store = Cassandra(\n",
        "            embedding=embedding,\n",
        "            table_name=index_name\n",
        "        )\n",
        "        print(\"Datastax Vector Store Created\")\n",
        "\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d828495-d527-4aa5-bfda-3408a2180fc5"
      },
      "outputs": [],
      "source": [
        "def generate_hash(content):\n",
        "    return hashlib.sha256(content.encode()).hexdigest()\n",
        "\n",
        "\n",
        "def insert_docs_to_vector_store(vector_store,split_docs,insert_type=\"docs\" ):\n",
        "    with tqdm(total=len(split_docs), desc=\"Inserting Documents\", unit=\"docs\") as pbar:\n",
        "        try:\n",
        "            for i in range(0, len(split_docs), parameters['index_chunk_size']):\n",
        "                chunk = split_docs[i:i + parameters['index_chunk_size']]\n",
        "                if insert_type==\"docs\":\n",
        "                    id_chunk = [generate_hash(doc.page_content+'\\nTitle: '+doc.metadata['title']+'\\nUrl: '+doc.metadata['document_url']+'\\nPage: '+doc.metadata['page_number']) for doc in chunk]\n",
        "                elif insert_type==\"profiles\":\n",
        "                    id_chunk = [generate_hash(doc.page_content) for doc in chunk]\n",
        "                vector_store.add_documents(chunk, ids=id_chunk)\n",
        "                pbar.update(len(chunk))\n",
        "            print(\"Documents are inserted into vector database\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e58a206-14a3-4f8b-a91b-1319cfeb6aca"
      },
      "source": [
        "Below code defines a function to generates a list of unique IDs for each document by hashing their `page_content`. The code sets a chunk size for batch processing. It iterates over the documents in chunks, inserting each chunk into the vector store with corresponding IDs. The progress bar is updated to reflect the number of documents processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40112102-c7eb-40dd-929d-b5bcc6bee0a9"
      },
      "outputs": [],
      "source": [
        "vector_store=create_vector_store(connection_type,parameters['vector_store_index_name'], parameters)\n",
        "print(\"Inserting Documents\") \n",
        "insert_docs_to_vector_store(vector_store,split_docs,\"docs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6795cb26-7c52-40fb-ae7c-ccb4439aa24b"
      },
      "source": [
        "Above cell may take significant amount of time to complete based on the size of the documents. \n",
        "\n",
        "Optionally you can also proceed to : \n",
        "* **Test Queries for Vector Database** notebook to perform sample searches using various techniques and Analyze the search results to understand the effectiveness of different methods. \n",
        "* **Ingest Expert Profile data to vector DB** notebook to ingest expert profile data into the vector database. \n",
        "\n",
        "If you do not wish to **test sample search queries against the vector database** or **ingest expert profile data to the vector database**, you can proceed to **`Create and Deploy QnA AI Service`** notebook to create and deploy the RAG AI service without waiting for the next cell to complete.<br>\n",
        "\n",
        "Below cell can be uncommented to make the above insertion asynchronous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93f8aa00-a8e7-4f99-84c0-a51395d5dd7a"
      },
      "outputs": [],
      "source": [
        "#vector_store=create_vector_store(connection_type,parameters['vector_store_index_name'], parameters)\n",
        "#documents=asyncio.create_task(\n",
        "#    vector_store.add_documents(\n",
        "#    split_docs, \n",
        "#    bulk_kwargs={\n",
        "#                \"chunk_size\": parameters['index_chunk_size'],\n",
        "#                \"request_timeout\": 600\n",
        "#            },ids=[generate_hash(doc.page_content+'\\nTitle: '+doc.metadata['title']+'\\nUrl: '+doc.metadata['document_url']+'\\nPage: '+doc.metadata['page_number']) for doc in split_docs])\n",
        "#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d7fd078-86e1-4d5d-b77d-11d917fc884f"
      },
      "source": [
        "**Note** It's recommended to close the datastax session once you are done with ingestion in this notebook for optimal performance. once you execute this cell existing datastax connections are closed. if have to re run above code cells you have to create new connection for datastax by re running cells from `Connect to Vector Database`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af3c2c63-2b2e-4db6-921c-2b9be52f2ab6"
      },
      "outputs": [],
      "source": [
        "if connection_type==\"datastax\" and environment != \"cloud\":\n",
        "    if not datastax_session.is_shutdown:\n",
        "        datastax_session.shutdown()\n",
        "        print(f\"datastax_session got shutdown : {datastax_session.is_shutdown}\")\n",
        "    if not datastax_cluster.is_shutdown:\n",
        "        datastax_cluster.shutdown()\n",
        "        print(f\"datastax_cluster got shutdown : {datastax_cluster.is_shutdown}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "336cc73aa6c3495a85dc167312b99079"
      },
      "source": [
        "\n",
        "\n",
        "**Sample Materials, provided under license.</a> <br>\n",
        "Licensed Materials - Property of IBM. <br>\n",
        "© Copyright IBM Corp. 2024, 2025. All Rights Reserved. <br>\n",
        "US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. <br>**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ragvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
