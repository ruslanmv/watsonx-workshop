{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2c942354ffc4b7b97ba3c5039dcfce1"
      },
      "source": [
        "# Perform Sample Searches on Vector Database \n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook provides an optional exercise for exploring document search within a vector database. Experiment with different search techniques using **Elasticsearch or Milvus or Datastax** as your chosen database.\n",
        "\n",
        "This notebook demonstrates how to perform sample searches using various techniques and Analyze the search results to understand the effectiveness of different methods.\n",
        "While this notebook is optional, it can be valuable for gaining hands-on experience with vector databases and document search techniques. If you prefer, you can skip this exercise and proceed directly to **`Create and Deploy QnA AI Service`**\n",
        "\n",
        "In this notebook, we will cover the following actions:\n",
        "\n",
        "- Establishing a connection to the chosen vector database (Elasticsearch or Milvus or Datastax).\n",
        "- If your connection type is Elastic Search \n",
        "    - Using an Elastic Learned Sparse Encoder (ELSER)/ Dense model (like E5 multilingual) and LangChain to search and retrieve relevant documents based on specific queries in Elasticsearch.  \n",
        "- If your connection type is Milvus or Datastax  \n",
        "    - Employing an embedding model and LangChain to search and retrieve relevant documents based on specific queries in Milvus.\n",
        "- Additional functionality supported for search and retrieval of hybrid searching strategies with both Milvus and Elasticsearch Vector Database.\n",
        "\n",
        "**NOTE**: \n",
        "- Hybrid search is not supported for Bulk indexing in Milvus. Please disable `milvus_hybrid_search` param in `RAG_ADVANCED_PARAMETER_SET` in case data is bulk ingested in Milvus.\n",
        "- Datastax is not supported in this cloud version.\n",
        "\n",
        "## Contents\n",
        "\n",
        "This notebook contains the following parts:\n",
        "- [Setup](#setup)\n",
        "- [Connect to Vector Database](#connect)\n",
        "- [Q&A using Vectorstore and query templates](#QnATest)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e09609f5baf4507a57aca85bd321ff9"
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "### Pre-Requisite Libraries and Dependencies\n",
        "Download and import mandatory libraries and dependencies. \n",
        "\n",
        "Note : Some of the versions of the libraries may throw warnings after installation. These library versions are crucial for successful execution of the accelerator. Please ignore the warning/error and proceed with your execution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc36dbb7f1a446988816cff0db8aa113"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install elasticsearch==8.18.1 | tail -n 1\n",
        "!pip install langchain | tail -n 1\n",
        "!pip install ibm_watsonx_ai==1.3.26 | tail -n 1\n",
        "!pip install langchain_elasticsearch==0.3.2 | tail -n 1\n",
        "!pip install langchain_milvus==0.2.0 | tail -n 1\n",
        "!pip install pymilvus==2.5.11 | tail -n 1\n",
        "!pip install langchain_community | tail -n 1\n",
        "!pip install cassio==0.1.10 | tail -n 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d46b82d-1541-43be-b9b4-1113ebd001aa"
      },
      "source": [
        "Restart the kernel after performing the pip install if the below cell fails to import all the libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe434599-967f-4f87-98e0-00d84e50a966"
      },
      "outputs": [],
      "source": [
        "from elasticsearch import Elasticsearch, helpers\n",
        "from ibm_watsonx_ai import Credentials\n",
        "from ibm_watsonx_ai.foundation_models import Embeddings\n",
        "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai import APIClient\n",
        "import json\n",
        "import os\n",
        "import shutil \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from pymilvus import(IndexType,Status,connections,FieldSchema,DataType,Collection,CollectionSchema,utility)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a95f6b5b-427e-4a1b-93c0-24b98e6f513d"
      },
      "outputs": [],
      "source": [
        "project_id=os.environ['PROJECT_ID']\n",
        "# Environment and host url\n",
        "hostname = os.environ['RUNTIME_ENV_APSX_URL']\n",
        "\n",
        "if hostname.endswith(\"cloud.ibm.com\") == True:\n",
        "    environment = \"cloud\"\n",
        "    project_id = os.environ['PROJECT_ID']\n",
        "    runtime_region = os.environ[\"RUNTIME_ENV_REGION\"] \n",
        "else:\n",
        "    environment = \"on-prem\"\n",
        "    from ibm_watson_studio_lib import access_project_or_space\n",
        "    wslib = access_project_or_space()\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8cc005-454e-4646-901c-4cb3825d9bc5"
      },
      "source": [
        "\n",
        "<a id=\"parameterimport\"></a>\n",
        "### Import Parameter Sets, Credentials and Helper functions script.\n",
        "\n",
        "Below cells imports parameter sets values, the credentials and helper functions script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eecabd0e-f2c4-4900-bfad-36a3a3ac4dac"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    filename = 'rag_helper_functions.py'\n",
        "    wslib.download_file(filename)\n",
        "    import rag_helper_functions\n",
        "    print(\"rag_helper_functions imported from the project assets\")\n",
        "except NameError as e:\n",
        "    print(str(e))\n",
        "    print(\"If running watsonx.ai aaS on IBM Cloud, check that the first cell in the notebook contains a project token. If not, select the vertical ellipsis button from the notebook toolbar and `insert project token`. Also check that you have specified your ibm_api_key in the second code cell of the notebook\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30542d29-e1d7-4ba4-b467-d99a40dfb260"
      },
      "outputs": [],
      "source": [
        "parameter_sets = [\"RAG_parameter_set\",\"RAG_advanced_parameter_set\"]\n",
        "\n",
        "parameters=rag_helper_functions.get_parameter_sets(wslib, parameter_sets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ea4eee7-ebef-4d68-ba3a-cb3b03fb5c63"
      },
      "outputs": [],
      "source": [
        "ibm_api_key=parameters['watsonx_ai_api_key']\n",
        "if environment == \"cloud\":\n",
        "    WML_SERVICE_URL = f\"https://{runtime_region}.ml.cloud.ibm.com\"\n",
        "    wml_credentials = {\"apikey\": ibm_api_key, \"url\": WML_SERVICE_URL}\n",
        "else:\n",
        "    token = os.environ['USER_ACCESS_TOKEN']\n",
        "    wml_credentials = {\"token\": token,\"instance_id\" : \"openshift\",\"url\": hostname}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62a9b987-bf46-4e64-9cca-d6cdb8b7fc7d"
      },
      "source": [
        "### Set Watsonx.ai client\n",
        "Below cell uses the watson machine learning credentials to create an API client to interact with the project and deployment space. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72cd218c865b4e70a6da8f0406ed4398"
      },
      "outputs": [],
      "source": [
        "client = APIClient(wml_credentials)\n",
        "client.set.default_project(project_id=project_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c3a18cdf0c34d0999ee68bf6577bcd9"
      },
      "source": [
        "<a id=\"connect\"></a>\n",
        "### Connecting to a vector database\n",
        "\n",
        "#### Connecting using Project Connection Asset (default)\n",
        "The notebook, by default, will look for a connection asset in the project named `milvus_connect` or `elasticsearch_connect` or `datastax_connect`.  You can set this up by following the instructions in the project readme. \n",
        "This code checks if a specified connection exists in the project. If found, it retrieves the connection details and identifies the connection type. Depending on the connection type, it establishes a connection to the appropriate database. If the connection is not found, it raises an error indicating the absence of the specified connection in the project.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e14441e06d140ff87b3b687c80d606c"
      },
      "outputs": [],
      "source": [
        "connection_name=parameters[\"connection_asset\"]\n",
        "if(next((conn for conn in wslib.list_connections() if conn['name'] == connection_name), None)):\n",
        "    print(connection_name, \"Connection found in the project\")\n",
        "    db_connection = wslib.get_connection(connection_name)\n",
        "    \n",
        "    connection_datatypesource_id=client.connections.get_details(db_connection['.']['asset_id'])['entity']['datasource_type']\n",
        "    connection_type = client.connections.get_datasource_type_details_by_id(connection_datatypesource_id)['entity']['name']\n",
        "    \n",
        "    print(\"Successfully retrieved the connection details\")\n",
        "    print(\"Connection type is identified as:\",connection_type)\n",
        "\n",
        "    if connection_type==\"elasticsearch\":\n",
        "        es_client=rag_helper_functions.create_and_check_elastic_client(db_connection, parameters['elastic_search_model_id'])\n",
        "    elif connection_type==\"milvus\" or connection_type==\"milvuswxd\":\n",
        "        milvus_credentials = rag_helper_functions.connect_to_milvus_database(db_connection, parameters)\n",
        "\n",
        "    elif connection_type==\"datastax\" :\n",
        "        if environment == \"cloud\":\n",
        "            raise ValueError(f\"ERROR! we don't support datastax connection for Cloud as of now\")\n",
        "        import cassio\n",
        "        datastax_session,datastax_cluster = rag_helper_functions.connect_to_datastax(db_connection, parameters)\n",
        "        cassio.init(session=datastax_session, keyspace=db_connection.get('keyspace'))\n",
        "\n",
        "else:\n",
        "    db_connection=\"\"\n",
        "    raise ValueError(f\"No connection named {connection_name} found in the project.\")\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3498fdf-907b-4281-8e16-c7a416616878"
      },
      "source": [
        "<a id=\"QnATest\"></a>\n",
        "## Q&A on the Vector Database Index/Collection\n",
        "\n",
        "The following sections of the notebook are designed to test a sample Question and Answer (QnA) interaction on the vector store. The subsequent cell in the notebook executes this test and provides a response that includes several key pieces of information. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d81559d-4c7e-412c-b4dd-46e70751d767"
      },
      "outputs": [],
      "source": [
        "question =\"How can I create a project in watsonx.ai?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f082e9da3515446b8a81c102856f3817"
      },
      "source": [
        "### 1. Using Elastic Search Query Templates on Elastic Search Vector Database\n",
        "\n",
        "The following section of the notebook is designed to test a sample Question and Answer (QnA) interaction using sample template of ELSER model or multilingual model, assuming it is utilized.\n",
        "This response comprises of:\n",
        "\n",
        "* `Relevance Score`: A numerical value indicating the relevance or confidence level of the answer provided by the model.\n",
        "* `Title`: The title of the document from which the answer is derived.\n",
        "* `Document ID`: A unique identifier for the document within the database or index.\n",
        "* `Document URL`: The location where the original document can be accessed or referenced.\n",
        "* `Document Content`: The actual content or text from the document that is relevant to the queried question.\n",
        "* `Source`: The source of the content or text for the relevant document queried. \n",
        "* `Page Number`: Page Number of the document (if applicable).\n",
        "\n",
        "This setup allows for a practical demonstration of the model's capabilities in retrieving and presenting information in response to a specific query. \n",
        "There are 2 ways to perform this step, depending on the `elastic_search_template_file` parameter provided in the parameter set by the user. \n",
        "1. **ELSER**: An ELSER exclusive search query is invoked.\n",
        "2. **ELSER + BM25**: A hybrid search query that is a combination of a tradition vector search and ELSER is invoked.\n",
        "3. **Multilingual**: A dense vector search query is invoked.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c94b60a078a64e1f8db6005f310939ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "if connection_type==\"elasticsearch\":\n",
        "    wslib.download_file(parameters['elastic_search_template_file'])\n",
        "    with open(parameters['elastic_search_template_file']) as f:\n",
        "        es_query_json = json.load(f)\n",
        "\n",
        "    es_query_str = json.dumps(es_query_json)\n",
        "    if 'dense' in parameters['elastic_search_vector_type']:\n",
        "        from langchain_elasticsearch import ElasticsearchEmbeddings\n",
        "        embeddings = ElasticsearchEmbeddings.from_es_connection(\n",
        "                    model_id=parameters['elastic_search_model_id'],\n",
        "                    es_connection=es_client,\n",
        "                )\n",
        "        query_vector = embeddings.embed_documents([question])[0]\n",
        "        es_query_str = es_query_str.replace('\"{{query_vector}}\"', str(query_vector))\n",
        "    else:\n",
        "        es_query_str = es_query_str.replace(\"{{model_id}}\", parameters['elastic_search_model_id'])\n",
        "        es_query_str = es_query_str.replace(\"{{model_text}}\", question)\n",
        "    \n",
        "    # Convert back to dictionary\n",
        "    es_query_template = json.loads(es_query_str)\n",
        "    es_query=es_query_template.get(\"query\",es_query_template)\n",
        "    print(es_query)\n",
        "\n",
        "    query_temp_args = {'query': es_query}\n",
        "    if 'sub_searches' in es_query:\n",
        "        query_temp_args = {'body': es_query}\n",
        "\n",
        "    try:\n",
        "        response = es_client.search(\n",
        "            index=parameters[\"vector_store_index_name\"], \n",
        "            size=parameters['vectorsearch_top_n_results'],\n",
        "            **query_temp_args\n",
        "        )\n",
        "        print(\"\\nResponse:\")\n",
        "        for hit in response['hits']['hits']:\n",
        "\n",
        "            score = hit['_score']\n",
        "            title = hit['_source']['metadata']['title']\n",
        "            page_content=hit['_source']['text']\n",
        "            source = hit['_source']['metadata']['source']\n",
        "            url = hit['_source']['metadata']['document_url']\n",
        "            page_number = hit['_source']['metadata']['page_number']\n",
        "\n",
        "\n",
        "            print(f\"\\nRelevance Score  : {score}\\nTitle            : {title}\\nSource     : {source}\\nDocument Content : {page_content}\\nDocument URL : {url}\\nPage Number : {page_number}\")\n",
        "\n",
        "    except Exception as e:\n",
        "            print(\"\\nAn error occurred while querying elastic search, please retry after sometime:\", e)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73d038fd-70be-4a8e-a84e-38bd3c29d396"
      },
      "source": [
        "### 2. Using the Langchain Retrievers\n",
        "Below code utilizes Langchains vector store extension to retrieve documents. <br>\n",
        "\n",
        "The code sets up a vector store based on the specified connection type, either \"elasticsearch\" or \"milvus\". <br>\n",
        "\n",
        "* If `connection_type` is `\"elasticsearch\"`, it imports `ElasticsearchRetriever` from the `langchain_elasticsearch` library and initializes it with an Elasticsearch client and specified parameters with the given model ID.\n",
        "\n",
        "* If `connection_type` is `\"milvus\"`, the code imports `langchain_milvus` and the `Milvus` vector store is then created using the embedding function, connection parameters, and index settings. It initializes a **Milvus** vector store with either **dense embeddings** or **hybrid search** (dense + BM25 sparse embeddings) based on the `milvus_hybrid_search` parameter. If hybrid search is enabled, it performs a **weighted similarity search**. Otherwise, it only uses dense embeddings and retrieves the top `k` results. Finally, it prints the search results.\n",
        "\n",
        "- If `connection_type` is `\"datastax\", it Creates a `Cassandra` vector store with the specified embedding function and table name. then Performs a vector-based similarity search with the top `k` results and prints the results with scores.\n",
        "\n",
        "**NOTE**: Hybrid search is not supported for Bulk indexing in Milvus.\n",
        "\n",
        "Below cell should run successfully, regardless of which vector database is used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66ccfc89-3232-4474-9dd3-09cdfe01e308"
      },
      "outputs": [],
      "source": [
        "def get_embedding(environment, parameters, project_id, wml_credentials, WML_SERVICE_URL):\n",
        "    if environment == \"cloud\":\n",
        "        credentials = Credentials(\n",
        "            api_key=parameters['watsonx_ai_api_key'],\n",
        "            url=WML_SERVICE_URL\n",
        "        )\n",
        "        embedding = Embeddings(\n",
        "            model_id=parameters['embedding_model_id'],\n",
        "            credentials=credentials,\n",
        "            project_id=project_id,\n",
        "            verify=True\n",
        "        )\n",
        "    elif environment == \"on-prem\":\n",
        "        try:\n",
        "            if client.foundation_models.EmbeddingModels.__members__:\n",
        "                if client.foundation_models.EmbeddingModels(parameters[\"embedding_model_id\"]).name:\n",
        "                    embedding = Embeddings(\n",
        "                        model_id=parameters['embedding_model_id'],\n",
        "                        credentials=wml_credentials,\n",
        "                        project_id=project_id,\n",
        "                        verify=True\n",
        "                    )\n",
        "                else:\n",
        "                    print(\"Local on-prem embedding models not found, using models from IBM Cloud API\")\n",
        "                    credentials = Credentials(\n",
        "                        api_key=parameters['watsonx_ai_api_key'],\n",
        "                        url=parameters['watsonx_ai_url']\n",
        "                    )\n",
        "                    embedding = Embeddings(\n",
        "                        model_id=parameters['embedding_model_id'],\n",
        "                        credentials=credentials,\n",
        "                        space_id=parameters[\"wx_ai_inference_space_id\"],\n",
        "                        verify=True\n",
        "                    )\n",
        "        except Exception as e:\n",
        "            print(f\"Exception in loading Embedding Models: {str(e)}\")\n",
        "            raise\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid environment: {environment}. Must be 'cloud' or 'on-prem'.\")\n",
        "    \n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Search Query to obtain most relevant result using the Langchain retrievers\n",
        "\n",
        "Based on specific type of connection type (Elasticsearch/Milvus/Datastax) the below cell invokes the search against the vector index and provides the most relevant results for the above question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37380661770647e690843aac719ed208"
      },
      "outputs": [],
      "source": [
        "match connection_type:\n",
        "    case \"elasticsearch\":\n",
        "        search_kwargs = {\n",
        "        \"k\": parameters['vectorsearch_top_n_results'],\n",
        "        \"score_threshold\": float(parameters['rag_es_min_score']),\n",
        "        \"include_scores\": True,\n",
        "        \"verbose\": True\n",
        "        }\n",
        "\n",
        "        def custom_body_func(query: str) -> dict:\n",
        "            print(f\"Reading from the template {parameters['elastic_search_template_file']}\")\n",
        "            return es_query_template\n",
        "        \n",
        "        from langchain_elasticsearch import ElasticsearchRetriever\n",
        "        retriever = ElasticsearchRetriever(\n",
        "                        es_client=es_client,\n",
        "                        index_name=parameters[\"vector_store_index_name\"],\n",
        "                        body_func=custom_body_func,\n",
        "                        content_field=\"text\",\n",
        "                        # document_mapper = document_mapper,\n",
        "                        search_kwargs=search_kwargs\n",
        "                    )\n",
        "        \n",
        "        print(\"ElasticsearchRetriever Created with\",parameters['elastic_search_model_id'])\n",
        "        results = retriever.invoke(question)\n",
        "        print(f\"Question: {question}\")\n",
        "        print(\"Response: \")\n",
        "        print([{\"page_content\": doc.page_content, \"metadata\":doc.metadata['_source']['metadata'], \"score\": doc.metadata['_score'] or doc.metadata['_rank']} for doc in results])\n",
        "        \n",
        "    case \"milvus\" | \"milvuswxd\":\n",
        "        from langchain_milvus import Milvus, BM25BuiltInFunction\n",
        "        if environment==\"cloud\":\n",
        "            credentials=Credentials(\n",
        "                api_key = parameters['watsonx_ai_api_key'],\n",
        "                url =WML_SERVICE_URL)\n",
        "            embedding = Embeddings(\n",
        "            model_id=parameters['embedding_model_id'],\n",
        "            credentials=credentials,\n",
        "            project_id=project_id,\n",
        "            verify=True\n",
        "            )\n",
        "            \n",
        "        elif environment==\"on-prem\":\n",
        "            try:\n",
        "                if client.foundation_models.EmbeddingModels.__members__:\n",
        "                    if client.foundation_models.EmbeddingModels(parameters[\"embedding_model_id\"]).name:\n",
        "                        embedding = Embeddings(\n",
        "                            model_id=parameters['embedding_model_id'],\n",
        "                            credentials=wml_credentials,\n",
        "                            project_id=project_id,\n",
        "                            verify=True\n",
        "                        )\n",
        "                    else:\n",
        "                        raise Exception(parameters[\"embedding_model_id\"] + \"model is missing. Please check and update embedding_model_id adv param\")\n",
        "                else:\n",
        "                    print(\"local on prem embeddng models are not found, using models from IBM Cloud API\")\n",
        "                    credentials=Credentials(\n",
        "                        api_key = parameters['watsonx_ai_api_key'],\n",
        "                        url =parameters['watsonx_ai_url'])\n",
        "                    embedding = Embeddings(\n",
        "                        model_id=parameters['embedding_model_id'],\n",
        "                        credentials=credentials,\n",
        "                        space_id=parameters[\"wx_ai_inference_space_id\"],\n",
        "                        verify=True\n",
        "                    )\n",
        "            except Exception as e:\n",
        "                print(\"Exception in loading Embedding Models:\" + str(e))\n",
        "            \n",
        "        hybrid_search = True if parameters['milvus_hybrid_search'].lower()==\"true\" else False\n",
        "        dense_index_param = {\"metric_type\": \"L2\", \"index_type\": \"IVF_FLAT\",\"params\": {\"nlist\": 1024},}\n",
        "        print(f\"using the embedding model {parameters['embedding_model_id']} for dense embeddings.\")\n",
        "        if hybrid_search:\n",
        "            sparse_index_param = {\"metric_type\": \"BM25\",\"index_type\": \"SPARSE_INVERTED_INDEX\", \"params\": {\"drop_ratio_build\": 0.2}}\n",
        "            print(\"using BM25 sparse embeddings.\")\n",
        "            vector_store = Milvus(\n",
        "            embedding_function=embedding,\n",
        "            builtin_function=BM25BuiltInFunction(output_field_names=\"sparse\"), \n",
        "            index_params=[dense_index_param, sparse_index_param],\n",
        "            vector_field=[\"dense\", \"sparse\"],\n",
        "            connection_args=milvus_credentials,\n",
        "            primary_field='id',\n",
        "            consistency_level=\"Strong\",\n",
        "            collection_name=parameters[\"vector_store_index_name\"] \n",
        "            )\n",
        "            search_result = vector_store.similarity_search_with_score(question,  ranker_type=\"weighted\", ranker_params={\"weights\": [0.6, 0.4]})\n",
        "        else:\n",
        "            vector_store = Milvus(\n",
        "                embedding_function=embedding,\n",
        "                index_params=dense_index_param,\n",
        "                connection_args=milvus_credentials,\n",
        "                primary_field='id',\n",
        "                consistency_level=\"Strong\",\n",
        "                collection_name=parameters[\"vector_store_index_name\"] \n",
        "            )\n",
        "            search_result = vector_store.similarity_search_with_score_by_vector(embedding.embed_query(question), k=parameters['vectorsearch_top_n_results'])\n",
        "        print(search_result)\n",
        "\n",
        "    case \"datastax\":\n",
        "        if environment == \"cloud\":\n",
        "            raise ValueError(f\"ERROR! we don't support datastax connection for Cloud as of now\")\n",
        "        print(\"using the model\",parameters['embedding_model_id'], \"to create embeddings\")\n",
        "        embedding = get_embedding(environment, parameters, project_id, wml_credentials, WML_SERVICE_URL) if environment == \"cloud\" else get_embedding(environment, parameters, project_id, wml_credentials, None)  \n",
        "        from langchain_community.vectorstores import Cassandra\n",
        "        vector_store = Cassandra(\n",
        "            embedding=embedding,\n",
        "            table_name=parameters[\"vector_store_index_name\"] \n",
        "        )\n",
        "        print(\"Datastax vector store Created on the index\",parameters[\"vector_store_index_name\"] )\n",
        "        \n",
        "        search_result= vector_store.similarity_search_with_score_by_vector(embedding.embed_query(question), k=parameters['vectorsearch_top_n_results'])\n",
        "        print(\"\\nQuestion:\",question, \"\\nSearch Results:\", search_result)\n",
        "\n",
        "    case _:\n",
        "        raise ValueError(f\"Unsupported connection_type: {connection_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7089d29f-560d-46d7-a791-daaa9195b871"
      },
      "source": [
        "**Note** It's recommended to close the datastax session once you are done with ingestion in this notebook for optimal performance. once you execute this cell existing datastax connections are closed. if have to re run above code cells you have to create new connection for datastax by re running cells from `Connect to Vector Database`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1b3b534-7756-429a-9503-a8e3d332b955"
      },
      "outputs": [],
      "source": [
        "if connection_type==\"datastax\" and environment != \"cloud\":\n",
        "    if not datastax_session.is_shutdown:\n",
        "        datastax_session.shutdown()\n",
        "        print(f\"datastax_session got shutdown : {datastax_session.is_shutdown}\")\n",
        "    if not datastax_cluster.is_shutdown:\n",
        "        datastax_cluster.shutdown()\n",
        "        print(f\"datastax_cluster got shutdown : {datastax_cluster.is_shutdown}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4531ce13-1974-4b82-96dc-552f1cb982ce"
      },
      "source": [
        "Optionally, proceed to **Ingest Expert Profile data to vector DB** notebook to ingest expert profiles into vector database.<br>\n",
        "Otherwise, proceed to **Create and Deploy QnA AI Service** notebook to create and deploy the RAG AI Service python function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "336cc73aa6c3495a85dc167312b99079"
      },
      "source": [
        "\n",
        "\n",
        "**Sample Materials, provided under license.</a> <br>\n",
        "Licensed Materials - Property of IBM. <br>\n",
        "Â© Copyright IBM Corp. 2024, 2025. All Rights Reserved. <br>\n",
        "US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. <br>**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
